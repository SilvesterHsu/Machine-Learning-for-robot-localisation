{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:43:46.080691Z",
     "start_time": "2020-08-29T01:43:44.285361Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import roslib\n",
    "import rospy\n",
    "import tf as tf_ros\n",
    "from nav_msgs.msg import Odometry, Path\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "from geometry_msgs.msg import PoseStamped, PoseArray, Pose\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:43:46.507436Z",
     "start_time": "2020-08-29T01:43:46.082793Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "# S1: check GPU\n",
    "#list_device()\n",
    "\n",
    "# S2: default parameters\n",
    "set_device(1)\n",
    "np.set_printoptions(precision = 2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:43:46.518319Z",
     "start_time": "2020-08-29T01:43:46.509158Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=420, help='size of mini batch')\n",
    "#parser.add_argument('--batch_size', type=int, default=200, help='size of mini batch')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/gps_net_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--test_dataset', type=str, default=[# '/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_02_12',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_04_29',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_05_11',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_06_15',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_08_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "                                                         '/notebooks/michigan_nn_data/2012_10_28',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_11_16',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_12_01'\n",
    "                                                        ] )\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "#parser.add_argument('--map_dataset', type=str, default='/home/kevin/data/michigan_gt/training')\n",
    "parser.add_argument('--enable_ros', type=bool, default=False, help='put data into ros')\n",
    "parser.add_argument('--cuda_device', type=int, default=1, help='cuda device')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.enable_ros:\n",
    "    rospy.init_node('global_localization_tf_broadcaster_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:45:44.957254Z",
     "start_time": "2020-08-29T01:43:46.520033Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14301/14301 [00:18<00:00, 778.28it/s]\n",
      "100%|██████████| 7008/7008 [00:09<00:00, 768.69it/s]\n",
      "100%|██████████| 12852/12852 [00:16<00:00, 781.61it/s]\n",
      "100%|██████████| 9567/9567 [00:12<00:00, 777.05it/s]\n",
      "100%|██████████| 13580/13580 [00:17<00:00, 786.42it/s]\n",
      "100%|██████████| 14835/14835 [00:18<00:00, 781.55it/s]\n",
      "100%|██████████| 7114/7114 [00:09<00:00, 772.08it/s]\n",
      "100%|██████████| 12683/12683 [00:16<00:00, 767.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet, cnn_auxiliary\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize, denormalize_navie, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.test_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False, mode='evaluate', sampling_rate=2)\n",
    "\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:45:45.075701Z",
     "start_time": "2020-08-29T01:45:44.974171Z"
    },
    "code_folding": [
     3,
     24,
     40,
     69,
     88,
     118,
     155,
     159,
     178
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchlib.GPs import Backbone, NN, GPNode, BaseModule\n",
    "    \n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([output_dim]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([output_dim])),\n",
    "            batch_shape=torch.Size([output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class GPNode(nn.Module):\n",
    "    def __init__(self, inducing_points, seed=0, feat_dim = 128, sub_feat = True):\n",
    "        super().__init__()\n",
    "        output_dim = inducing_points.shape[0]\n",
    "        \n",
    "        if sub_feat:\n",
    "            sub_feat_dim = inducing_points.shape[-1]\n",
    "            torch.manual_seed(seed)\n",
    "            self.feat_index = torch.randperm(feat_dim)[:sub_feat_dim]\n",
    "        self.gp = GP(inducing_points,output_dim)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=output_dim) \n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        output = self.gp(input_data)\n",
    "        return output\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_gp = 20, sub_feat_rate = 0.6666, feat_dim = 128, output_dim = 3):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gps = nn.ModuleList()\n",
    "        \n",
    "        self.num_gp = num_gp\n",
    "        self.sub_feat_rate = sub_feat_rate\n",
    "        self.sub_feat_dim = int(feat_dim*self.sub_feat_rate)\n",
    "        \n",
    "        for i in range(self.num_gp):\n",
    "            inducing_points = torch.zeros(output_dim, 300, self.sub_feat_dim)\n",
    "            # use i as seed to fix sub features\n",
    "            gp = GPNode(inducing_points,seed=i)\n",
    "            self.gps.append(gp)\n",
    "        \n",
    "    def forward_nn(self, input_data):\n",
    "        dense_feat = self.backbone(input_data)\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention            \n",
    "        return feature_t, rot_pred\n",
    "    \n",
    "    def forward_gp(self,gp,trans_feat):\n",
    "        sub_trans_feat = trans_feat[:,gp.feat_index]\n",
    "        trans_pred = gp(sub_trans_feat)\n",
    "        return trans_pred\n",
    "    \n",
    "class PosePredictor(BaseModule):\n",
    "    def __init__(self, norm_mean, norm_std, args, num_gp=20,\n",
    "                 regressor_context_rate = [0.0,0.0],\n",
    "                 is_training=True, train_rot = True):\n",
    "        \n",
    "        super().__init__(norm_mean, norm_std, args)\n",
    "        self.model = Model(num_gp).to(self.device)\n",
    "        self.train_rot = train_rot\n",
    "        \n",
    "        # disable learning backbone\n",
    "        self.disable_requires_grad(self.model.backbone)\n",
    "        \n",
    "        if is_training:\n",
    "            # training tool\n",
    "            self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                             lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model)\n",
    "            \n",
    "    def _optimize(self, regressor_context_rate = [0.0,0.0]):\n",
    "        optimizer = [\n",
    "                {'params': self.model.gps.parameters(), \\\n",
    "                 'lr': self.args.learning_rate,'weight_decay':self.args.weight_decay}]\n",
    "            \n",
    "        # NN\n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor)\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "            self.train_rot = True\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_context)\n",
    "            \n",
    "        \n",
    "        if not self.train_rot and regressor_context_rate[1]==0.0:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc1_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc2_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc3_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.logits_r)\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        if self.args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert self.model.training == True\n",
    "        \n",
    "        trans_loss = torch.tensor(0.).to(self.device)\n",
    "        \n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        rot_loss = self._nn_loss(rot_pred,rot_target)\n",
    "        for i,gp in enumerate(self.model.gps):\n",
    "            #sampled_mask = torch.randint(high=args.batch_size, size=(self.model.sub_batch_size,))\n",
    "            sampled_mask = torch.randint(high=self.args.batch_size, size=(self.args.batch_size,))\n",
    "            sub_x = trans_feat[sampled_mask]\n",
    "            sub_y = trans_target[sampled_mask]\n",
    "            gp_loss = self._gp_loss(gp,sub_x,sub_y)\n",
    "            trans_loss += gp_loss\n",
    "        trans_loss = trans_loss/self.model.num_gp\n",
    "        \n",
    "        total_loss = trans_loss + self.args.lamda_weights * rot_loss\n",
    "        \n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        #Step 3: update\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(total_loss), batch_time    \n",
    "    \n",
    "    def _nn_loss(self, rot_pred, rot_target):\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        return rot_loss\n",
    "        \n",
    "    def _gp_loss(self, gp, trans_feat, trans_target):\n",
    "        # predict\n",
    "        trans_pred = self.model.forward_gp(gp,trans_feat)\n",
    "        \n",
    "        #num_data = int(min(len(dataloader)*args.batch_size,len(dataset))*self.model.sub_batch_rate)\n",
    "        num_data = min(len(dataloader)*self.args.batch_size,len(dataset))\n",
    "        mll = gpytorch.mlls.PredictiveLogLikelihood(gp.likelihood, gp.gp, num_data = num_data)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*mll(trans_pred, trans_target)\n",
    "        \n",
    "        return trans_loss\n",
    "    \n",
    "    def _eval_gp(self, gp, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = gp.likelihood(trans_pred).mean, gp.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, y_var\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = torch.distributions.Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "\n",
    "    def eval_forward(self, x, y, num_sample = 100, output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert self.model.training == False\n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        \n",
    "        trans_preds = 0\n",
    "        trans_means = 0\n",
    "        trans_vars = 0\n",
    "        for gp in self.model.gps:\n",
    "            trans_pred = self.model.forward_gp(gp,trans_feat)\n",
    "            trans_pred, trans_mean, trans_var = self._eval_gp(gp, trans_pred)\n",
    "            #trans_preds += trans_pred * 1/trans_var\n",
    "            #trans_means += trans_mean * 1/trans_var\n",
    "            #trans_vars += 1/trans_var\n",
    "            trans_preds += trans_pred\n",
    "            trans_means += trans_mean\n",
    "            trans_vars += trans_var\n",
    "         \n",
    "        #trans_vars = 1/trans_vars\n",
    "        #trans_preds *= trans_vars\n",
    "        #trans_means *= trans_vars\n",
    "        trans_preds /= self.model.num_gp\n",
    "        trans_means /= self.model.num_gp\n",
    "        trans_vars /= self.model.num_gp\n",
    "        \n",
    "        if self.args.is_normalization and output_denormalize:\n",
    "            trans_preds = denormalize_navie(trans_preds, self.norm_mean, self.norm_std)\n",
    "            trans_means = denormalize_navie(trans_means, self.norm_mean, self.norm_std)\n",
    "            trans_vars = trans_vars.mul(self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_means, trans_vars, num_sample)\n",
    "        \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_preds, rot_pred, trans_target, rot_target, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:45:50.880641Z",
     "start_time": "2020-08-29T01:45:45.082446Z"
    },
    "code_folding": [
     0,
     1
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model to TITAN Xp.\n"
     ]
    }
   ],
   "source": [
    "trainer = PosePredictor(args.norm_mean,args.norm_std,args,num_gp=10,is_training = False)\n",
    "trainer.load_model('pretrained_gp10_fix.pth')\n",
    "#trainer.load_model('pretrained_gp20_fix.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:45:50.902952Z",
     "start_time": "2020-08-29T01:45:50.883210Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "trans_errors = []\n",
    "rot_errors = []\n",
    "uncertainties = []\n",
    "pose_map = []\n",
    "\n",
    "total_trans_error = 0.\n",
    "total_rot_error = 0.\n",
    "\n",
    "count = 0.\n",
    "\n",
    "is_save_map = False\n",
    "is_read_map = False\n",
    "\n",
    "trans_preds = []\n",
    "trans_gts = []\n",
    "\n",
    "rot_preds = []\n",
    "rot_gts = []\n",
    "\n",
    "pred_uncertainties = []\n",
    "\n",
    "samples_mins = []\n",
    "\n",
    "pred_time = []\n",
    "\n",
    "br = tf_ros.TransformBroadcaster()\n",
    "\n",
    "GT_POSE_TOPIC = '/gt_pose'\n",
    "BIRDVIEW_TOPIC_PUB = '/bird_view'\n",
    "MAP_TOPIC_PUB = '/pose_map'\n",
    "PARTICLES_PUB = '/particles'\n",
    "NN_LOCALIZASION_PUB = '/nn_pose'\n",
    "gt_pose_pub = rospy.Publisher(GT_POSE_TOPIC, Odometry, queue_size=1)\n",
    "bird_view_pub = rospy.Publisher(BIRDVIEW_TOPIC_PUB, Image, queue_size=1)\n",
    "map_pub = rospy.Publisher(MAP_TOPIC_PUB, Path, queue_size=1)\n",
    "particles_pub = rospy.Publisher(PARTICLES_PUB, PoseArray, queue_size=1)\n",
    "nn_pose_pub = rospy.Publisher(NN_LOCALIZASION_PUB, Odometry, queue_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:51:00.877495Z",
     "start_time": "2020-08-29T01:45:50.904784Z"
    },
    "code_folding": [
     15
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/91980, translation error = 9.744, rotation error = 3.681, time/batch = 6.153\n",
      "840/91980, translation error = 7.000, rotation error = 3.187, time/batch = 1.240\n",
      "1260/91980, translation error = 7.019, rotation error = 3.434, time/batch = 1.236\n",
      "1680/91980, translation error = 5.925, rotation error = 3.575, time/batch = 1.159\n",
      "2100/91980, translation error = 5.400, rotation error = 3.641, time/batch = 1.154\n",
      "2520/91980, translation error = 4.914, rotation error = 3.654, time/batch = 1.162\n",
      "2940/91980, translation error = 4.621, rotation error = 3.648, time/batch = 1.159\n",
      "3360/91980, translation error = 4.436, rotation error = 3.688, time/batch = 1.166\n",
      "3780/91980, translation error = 4.159, rotation error = 3.587, time/batch = 1.156\n",
      "4200/91980, translation error = 3.849, rotation error = 3.508, time/batch = 1.173\n",
      "4620/91980, translation error = 3.620, rotation error = 3.484, time/batch = 1.162\n",
      "5040/91980, translation error = 3.415, rotation error = 3.478, time/batch = 1.171\n",
      "5460/91980, translation error = 3.347, rotation error = 3.442, time/batch = 1.163\n",
      "5880/91980, translation error = 3.284, rotation error = 3.403, time/batch = 1.166\n",
      "6300/91980, translation error = 3.195, rotation error = 3.325, time/batch = 1.157\n",
      "6720/91980, translation error = 5.327, rotation error = 3.713, time/batch = 1.162\n",
      "7140/91980, translation error = 5.403, rotation error = 4.032, time/batch = 1.169\n",
      "7560/91980, translation error = 5.253, rotation error = 4.082, time/batch = 1.164\n",
      "7980/91980, translation error = 5.315, rotation error = 4.130, time/batch = 1.165\n",
      "8400/91980, translation error = 5.890, rotation error = 4.229, time/batch = 1.168\n",
      "8820/91980, translation error = 5.713, rotation error = 4.190, time/batch = 1.163\n",
      "9240/91980, translation error = 5.591, rotation error = 4.170, time/batch = 1.164\n",
      "9660/91980, translation error = 5.457, rotation error = 4.101, time/batch = 1.167\n",
      "10080/91980, translation error = 5.371, rotation error = 4.089, time/batch = 1.164\n",
      "10500/91980, translation error = 5.224, rotation error = 4.176, time/batch = 1.158\n",
      "10920/91980, translation error = 5.115, rotation error = 4.171, time/batch = 1.164\n",
      "11340/91980, translation error = 5.003, rotation error = 4.108, time/batch = 1.164\n",
      "11760/91980, translation error = 4.872, rotation error = 4.043, time/batch = 1.168\n",
      "12180/91980, translation error = 4.756, rotation error = 3.991, time/batch = 1.163\n",
      "12600/91980, translation error = 4.662, rotation error = 3.955, time/batch = 1.167\n",
      "13020/91980, translation error = 4.587, rotation error = 3.964, time/batch = 1.166\n",
      "13440/91980, translation error = 4.596, rotation error = 3.975, time/batch = 1.175\n",
      "13860/91980, translation error = 4.525, rotation error = 3.943, time/batch = 1.166\n",
      "14280/91980, translation error = 4.464, rotation error = 3.956, time/batch = 1.179\n",
      "14700/91980, translation error = 4.419, rotation error = 3.931, time/batch = 1.166\n",
      "15120/91980, translation error = 4.381, rotation error = 3.930, time/batch = 1.174\n",
      "15540/91980, translation error = 4.337, rotation error = 3.887, time/batch = 1.164\n",
      "15960/91980, translation error = 5.341, rotation error = 4.097, time/batch = 1.169\n",
      "16380/91980, translation error = 5.279, rotation error = 4.089, time/batch = 1.166\n",
      "16800/91980, translation error = 5.208, rotation error = 4.134, time/batch = 1.170\n",
      "17220/91980, translation error = 5.136, rotation error = 4.133, time/batch = 1.167\n",
      "17640/91980, translation error = 5.050, rotation error = 4.096, time/batch = 1.177\n",
      "18060/91980, translation error = 5.000, rotation error = 4.117, time/batch = 1.164\n",
      "18480/91980, translation error = 4.943, rotation error = 4.121, time/batch = 1.168\n",
      "18900/91980, translation error = 4.874, rotation error = 4.135, time/batch = 1.174\n",
      "19320/91980, translation error = 4.821, rotation error = 4.102, time/batch = 1.191\n",
      "19740/91980, translation error = 4.787, rotation error = 4.108, time/batch = 1.173\n",
      "20160/91980, translation error = 4.730, rotation error = 4.098, time/batch = 1.184\n",
      "20580/91980, translation error = 4.664, rotation error = 4.076, time/batch = 1.177\n",
      "21000/91980, translation error = 4.607, rotation error = 4.063, time/batch = 1.171\n",
      "21420/91980, translation error = 4.552, rotation error = 4.058, time/batch = 1.182\n",
      "21840/91980, translation error = 6.086, rotation error = 4.229, time/batch = 1.168\n",
      "22260/91980, translation error = 10.083, rotation error = 5.284, time/batch = 1.177\n",
      "22680/91980, translation error = 10.042, rotation error = 5.249, time/batch = 1.167\n",
      "23100/91980, translation error = 9.928, rotation error = 5.240, time/batch = 1.170\n",
      "23520/91980, translation error = 9.791, rotation error = 5.207, time/batch = 1.190\n",
      "23940/91980, translation error = 9.661, rotation error = 5.199, time/batch = 1.167\n",
      "24360/91980, translation error = 9.587, rotation error = 5.205, time/batch = 1.180\n",
      "24780/91980, translation error = 9.504, rotation error = 5.221, time/batch = 1.164\n",
      "25200/91980, translation error = 9.376, rotation error = 5.198, time/batch = 1.167\n",
      "25620/91980, translation error = 9.253, rotation error = 5.175, time/batch = 1.173\n",
      "26040/91980, translation error = 9.136, rotation error = 5.166, time/batch = 1.172\n",
      "26460/91980, translation error = 9.015, rotation error = 5.139, time/batch = 1.175\n",
      "26880/91980, translation error = 8.983, rotation error = 5.126, time/batch = 1.168\n",
      "27300/91980, translation error = 8.879, rotation error = 5.084, time/batch = 1.174\n",
      "27720/91980, translation error = 8.789, rotation error = 5.042, time/batch = 1.171\n",
      "28140/91980, translation error = 8.693, rotation error = 5.012, time/batch = 1.165\n",
      "28560/91980, translation error = 8.614, rotation error = 4.990, time/batch = 1.172\n",
      "28980/91980, translation error = 8.558, rotation error = 5.052, time/batch = 1.170\n",
      "29400/91980, translation error = 8.669, rotation error = 5.092, time/batch = 1.171\n",
      "29820/91980, translation error = 8.578, rotation error = 5.055, time/batch = 1.195\n",
      "30240/91980, translation error = 8.530, rotation error = 5.042, time/batch = 1.173\n",
      "30660/91980, translation error = 8.452, rotation error = 5.025, time/batch = 1.172\n",
      "31080/91980, translation error = 8.365, rotation error = 5.030, time/batch = 1.170\n",
      "31500/91980, translation error = 8.287, rotation error = 5.009, time/batch = 1.168\n",
      "31920/91980, translation error = 8.211, rotation error = 4.977, time/batch = 1.195\n",
      "32340/91980, translation error = 8.135, rotation error = 4.965, time/batch = 1.168\n",
      "32760/91980, translation error = 8.060, rotation error = 4.954, time/batch = 1.172\n",
      "33180/91980, translation error = 7.985, rotation error = 4.930, time/batch = 1.178\n",
      "33600/91980, translation error = 7.927, rotation error = 4.914, time/batch = 1.171\n",
      "34020/91980, translation error = 7.867, rotation error = 4.899, time/batch = 1.174\n",
      "34440/91980, translation error = 9.232, rotation error = 5.169, time/batch = 1.177\n",
      "34860/91980, translation error = 9.165, rotation error = 5.137, time/batch = 1.173\n",
      "35280/91980, translation error = 9.096, rotation error = 5.117, time/batch = 1.170\n",
      "35700/91980, translation error = 10.341, rotation error = 5.325, time/batch = 1.174\n",
      "36120/91980, translation error = 10.259, rotation error = 5.306, time/batch = 1.179\n",
      "36540/91980, translation error = 10.198, rotation error = 5.303, time/batch = 1.172\n",
      "36960/91980, translation error = 10.110, rotation error = 5.295, time/batch = 1.176\n",
      "37380/91980, translation error = 10.022, rotation error = 5.273, time/batch = 1.174\n",
      "37800/91980, translation error = 9.934, rotation error = 5.254, time/batch = 1.170\n",
      "38220/91980, translation error = 9.850, rotation error = 5.258, time/batch = 1.177\n",
      "38640/91980, translation error = 9.761, rotation error = 5.239, time/batch = 1.181\n",
      "39060/91980, translation error = 9.689, rotation error = 5.230, time/batch = 1.182\n",
      "39480/91980, translation error = 9.615, rotation error = 5.224, time/batch = 1.168\n",
      "39900/91980, translation error = 9.543, rotation error = 5.212, time/batch = 1.177\n",
      "40320/91980, translation error = 9.463, rotation error = 5.194, time/batch = 1.174\n",
      "40740/91980, translation error = 9.384, rotation error = 5.182, time/batch = 1.196\n",
      "41160/91980, translation error = 9.311, rotation error = 5.182, time/batch = 1.172\n",
      "41580/91980, translation error = 9.240, rotation error = 5.173, time/batch = 1.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/91980, translation error = 9.170, rotation error = 5.168, time/batch = 1.167\n",
      "42420/91980, translation error = 9.187, rotation error = 5.160, time/batch = 1.176\n",
      "42840/91980, translation error = 9.129, rotation error = 5.131, time/batch = 1.170\n",
      "43260/91980, translation error = 9.060, rotation error = 5.110, time/batch = 1.170\n",
      "43680/91980, translation error = 9.060, rotation error = 5.126, time/batch = 1.176\n",
      "44100/91980, translation error = 9.015, rotation error = 5.116, time/batch = 1.174\n",
      "44520/91980, translation error = 8.994, rotation error = 5.126, time/batch = 1.178\n",
      "44940/91980, translation error = 8.941, rotation error = 5.122, time/batch = 1.176\n",
      "45360/91980, translation error = 8.881, rotation error = 5.121, time/batch = 1.179\n",
      "45780/91980, translation error = 8.830, rotation error = 5.112, time/batch = 1.183\n",
      "46200/91980, translation error = 8.775, rotation error = 5.112, time/batch = 1.175\n",
      "46620/91980, translation error = 8.727, rotation error = 5.100, time/batch = 1.177\n",
      "47040/91980, translation error = 8.681, rotation error = 5.090, time/batch = 1.176\n",
      "47460/91980, translation error = 8.650, rotation error = 5.077, time/batch = 1.179\n",
      "47880/91980, translation error = 8.589, rotation error = 5.064, time/batch = 1.178\n",
      "48300/91980, translation error = 8.533, rotation error = 5.056, time/batch = 1.179\n",
      "48720/91980, translation error = 8.486, rotation error = 5.038, time/batch = 1.179\n",
      "49140/91980, translation error = 8.436, rotation error = 5.017, time/batch = 1.180\n",
      "49560/91980, translation error = 8.381, rotation error = 5.008, time/batch = 1.170\n",
      "49980/91980, translation error = 8.343, rotation error = 4.994, time/batch = 1.178\n",
      "50400/91980, translation error = 8.298, rotation error = 4.979, time/batch = 1.174\n",
      "50820/91980, translation error = 9.288, rotation error = 5.121, time/batch = 1.179\n",
      "51240/91980, translation error = 9.250, rotation error = 5.138, time/batch = 1.174\n",
      "51660/91980, translation error = 9.195, rotation error = 5.124, time/batch = 1.174\n",
      "52080/91980, translation error = 9.136, rotation error = 5.129, time/batch = 1.169\n",
      "52500/91980, translation error = 9.097, rotation error = 5.130, time/batch = 1.171\n",
      "52920/91980, translation error = 9.045, rotation error = 5.109, time/batch = 1.178\n",
      "53340/91980, translation error = 8.997, rotation error = 5.125, time/batch = 1.174\n",
      "53760/91980, translation error = 8.945, rotation error = 5.122, time/batch = 1.171\n",
      "54180/91980, translation error = 8.966, rotation error = 5.127, time/batch = 1.172\n",
      "54600/91980, translation error = 8.950, rotation error = 5.131, time/batch = 1.176\n",
      "55020/91980, translation error = 8.900, rotation error = 5.126, time/batch = 1.170\n",
      "55440/91980, translation error = 8.854, rotation error = 5.125, time/batch = 1.171\n",
      "55860/91980, translation error = 9.106, rotation error = 5.159, time/batch = 1.175\n",
      "56280/91980, translation error = 9.267, rotation error = 5.247, time/batch = 1.173\n",
      "56700/91980, translation error = 9.230, rotation error = 5.253, time/batch = 1.177\n",
      "57120/91980, translation error = 9.189, rotation error = 5.254, time/batch = 1.170\n",
      "57540/91980, translation error = 10.011, rotation error = 5.287, time/batch = 1.176\n",
      "57960/91980, translation error = 9.965, rotation error = 5.287, time/batch = 1.171\n",
      "58380/91980, translation error = 9.929, rotation error = 5.286, time/batch = 1.176\n",
      "58800/91980, translation error = 9.898, rotation error = 5.284, time/batch = 1.174\n",
      "59220/91980, translation error = 9.841, rotation error = 5.280, time/batch = 1.177\n",
      "59640/91980, translation error = 9.790, rotation error = 5.269, time/batch = 1.173\n",
      "60060/91980, translation error = 9.736, rotation error = 5.259, time/batch = 1.199\n",
      "60480/91980, translation error = 9.690, rotation error = 5.251, time/batch = 1.175\n",
      "60900/91980, translation error = 9.640, rotation error = 5.240, time/batch = 1.173\n",
      "61320/91980, translation error = 9.600, rotation error = 5.234, time/batch = 1.174\n",
      "61740/91980, translation error = 9.921, rotation error = 5.395, time/batch = 1.177\n",
      "62160/91980, translation error = 9.875, rotation error = 5.394, time/batch = 1.177\n",
      "62580/91980, translation error = 9.822, rotation error = 5.378, time/batch = 1.173\n",
      "63000/91980, translation error = 9.773, rotation error = 5.377, time/batch = 1.176\n",
      "63420/91980, translation error = 9.761, rotation error = 5.393, time/batch = 1.169\n",
      "63840/91980, translation error = 9.711, rotation error = 5.377, time/batch = 1.172\n",
      "64260/91980, translation error = 10.314, rotation error = 5.559, time/batch = 1.196\n",
      "64680/91980, translation error = 10.311, rotation error = 5.623, time/batch = 1.197\n",
      "65100/91980, translation error = 10.276, rotation error = 5.621, time/batch = 1.175\n",
      "65520/91980, translation error = 10.225, rotation error = 5.604, time/batch = 1.178\n",
      "65940/91980, translation error = 10.190, rotation error = 5.613, time/batch = 1.175\n",
      "66360/91980, translation error = 10.146, rotation error = 5.603, time/batch = 1.197\n",
      "66780/91980, translation error = 10.142, rotation error = 5.618, time/batch = 1.171\n",
      "67200/91980, translation error = 10.105, rotation error = 5.619, time/batch = 1.171\n",
      "67620/91980, translation error = 10.068, rotation error = 5.640, time/batch = 1.171\n",
      "68040/91980, translation error = 10.018, rotation error = 5.620, time/batch = 1.176\n",
      "68460/91980, translation error = 9.975, rotation error = 5.622, time/batch = 1.181\n",
      "68880/91980, translation error = 9.929, rotation error = 5.606, time/batch = 1.175\n",
      "69300/91980, translation error = 9.923, rotation error = 5.618, time/batch = 1.194\n",
      "69720/91980, translation error = 9.900, rotation error = 5.612, time/batch = 1.174\n",
      "70140/91980, translation error = 9.901, rotation error = 5.604, time/batch = 1.178\n",
      "70560/91980, translation error = 9.852, rotation error = 5.591, time/batch = 1.171\n",
      "70980/91980, translation error = 9.808, rotation error = 5.581, time/batch = 1.177\n",
      "71400/91980, translation error = 9.759, rotation error = 5.567, time/batch = 1.215\n",
      "71820/91980, translation error = 9.716, rotation error = 5.552, time/batch = 1.177\n",
      "72240/91980, translation error = 9.677, rotation error = 5.541, time/batch = 1.177\n",
      "72660/91980, translation error = 9.634, rotation error = 5.528, time/batch = 1.173\n",
      "73080/91980, translation error = 9.594, rotation error = 5.529, time/batch = 1.178\n",
      "73500/91980, translation error = 9.554, rotation error = 5.520, time/batch = 1.174\n",
      "73920/91980, translation error = 9.547, rotation error = 5.523, time/batch = 1.174\n",
      "74340/91980, translation error = 9.505, rotation error = 5.513, time/batch = 1.178\n",
      "74760/91980, translation error = 9.760, rotation error = 5.697, time/batch = 1.195\n",
      "75180/91980, translation error = 9.783, rotation error = 5.724, time/batch = 1.179\n",
      "75600/91980, translation error = 9.923, rotation error = 5.753, time/batch = 1.175\n",
      "76020/91980, translation error = 9.887, rotation error = 5.739, time/batch = 1.174\n",
      "76440/91980, translation error = 9.849, rotation error = 5.729, time/batch = 1.179\n",
      "76860/91980, translation error = 9.834, rotation error = 5.722, time/batch = 1.183\n",
      "77280/91980, translation error = 9.858, rotation error = 5.724, time/batch = 1.173\n",
      "77700/91980, translation error = 10.356, rotation error = 5.842, time/batch = 1.178\n",
      "78120/91980, translation error = 10.493, rotation error = 5.923, time/batch = 1.172\n",
      "78540/91980, translation error = 10.570, rotation error = 5.946, time/batch = 1.176\n",
      "78960/91980, translation error = 10.637, rotation error = 5.981, time/batch = 1.176\n",
      "79380/91980, translation error = 10.816, rotation error = 6.204, time/batch = 1.194\n",
      "79800/91980, translation error = 11.316, rotation error = 6.222, time/batch = 1.172\n",
      "80220/91980, translation error = 11.278, rotation error = 6.235, time/batch = 1.179\n",
      "80640/91980, translation error = 11.347, rotation error = 6.288, time/batch = 1.177\n",
      "81060/91980, translation error = 11.319, rotation error = 6.297, time/batch = 1.175\n",
      "81480/91980, translation error = 11.385, rotation error = 6.358, time/batch = 1.172\n",
      "81900/91980, translation error = 11.361, rotation error = 6.368, time/batch = 1.178\n",
      "82320/91980, translation error = 11.335, rotation error = 6.358, time/batch = 1.180\n",
      "82740/91980, translation error = 11.323, rotation error = 6.364, time/batch = 1.174\n",
      "83160/91980, translation error = 11.291, rotation error = 6.389, time/batch = 1.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83580/91980, translation error = 11.286, rotation error = 6.378, time/batch = 1.196\n",
      "84000/91980, translation error = 11.288, rotation error = 6.381, time/batch = 1.175\n",
      "84420/91980, translation error = 11.274, rotation error = 6.411, time/batch = 1.173\n",
      "84840/91980, translation error = 11.232, rotation error = 6.399, time/batch = 1.177\n",
      "85260/91980, translation error = 11.194, rotation error = 6.389, time/batch = 1.179\n",
      "85680/91980, translation error = 11.153, rotation error = 6.372, time/batch = 1.176\n",
      "86100/91980, translation error = 11.275, rotation error = 6.443, time/batch = 1.178\n",
      "86520/91980, translation error = 11.238, rotation error = 6.428, time/batch = 1.173\n",
      "86940/91980, translation error = 11.199, rotation error = 6.412, time/batch = 1.172\n",
      "87360/91980, translation error = 11.230, rotation error = 6.424, time/batch = 1.180\n",
      "87780/91980, translation error = 11.193, rotation error = 6.432, time/batch = 1.176\n",
      "88200/91980, translation error = 11.301, rotation error = 6.538, time/batch = 1.197\n",
      "88620/91980, translation error = 11.257, rotation error = 6.530, time/batch = 1.178\n",
      "89040/91980, translation error = 11.458, rotation error = 6.549, time/batch = 1.175\n",
      "89460/91980, translation error = 11.420, rotation error = 6.552, time/batch = 1.176\n",
      "89880/91980, translation error = 11.435, rotation error = 6.556, time/batch = 1.177\n",
      "90300/91980, translation error = 11.432, rotation error = 6.540, time/batch = 1.180\n",
      "90720/91980, translation error = 11.399, rotation error = 6.534, time/batch = 1.179\n",
      "91140/91980, translation error = 11.361, rotation error = 6.526, time/batch = 1.173\n",
      "91560/91980, translation error = 11.339, rotation error = 6.637, time/batch = 1.172\n",
      "91980/91980, translation error = 11.305, rotation error = 6.630, time/batch = 4.586\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data.values()\n",
    "    trans_pred, rot_pred, trans_gt, rot_gt, samples = trainer.eval_forward(x,y)\n",
    "    \n",
    "    samples_distance = torch.sqrt(torch.sum((samples.cpu() - trans_gt)**2,dim=-1))\n",
    "    samples_min_distance = torch.min(samples_distance,dim=0)[0]\n",
    "    \n",
    "    # transform data\n",
    "    samples_min = samples_min_distance.cpu().numpy()\n",
    "    trans_pred = trans_pred.cpu().numpy()\n",
    "    rot_pred = rot_pred.cpu().numpy()\n",
    "    trans_gt = trans_gt.cpu().numpy()\n",
    "    rot_gt = rot_gt.cpu().numpy()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    if args.enable_ros:\n",
    "        particles = PoseArray()\n",
    "        particles.header.stamp = rospy.Time.now()\n",
    "        particles.header.frame_id = 'world'\n",
    "        for s in samples:\n",
    "            pose = Pose()\n",
    "            [pose.position.x, pose.position.y, pose.position.z] = s\n",
    "            [pose.orientation.x, pose.orientation.y, pose.orientation.z, pose.orientation.w] = rot_pred[0]\n",
    "            particles.poses.append(pose)\n",
    "        particles_pub.publish(particles)\n",
    "\n",
    "        [px_pred, py_pred, pz_pred] = trans_pred[0]\n",
    "        [qx_pred, qy_pred, qz_pred, qw_pred] = rot_pred[0]\n",
    "\n",
    "        br.sendTransform((px_pred, py_pred, pz_pred),\n",
    "                         (qx_pred, qy_pred, qz_pred, qw_pred), rospy.Time.now(),\n",
    "                         \"estimation\", \"world\")\n",
    "\n",
    "        [px_gt, py_gt, pz_gt] = trans_gt[0]\n",
    "        [qx_gt, qy_gt, qz_gt, qw_gt] = rot_gt[0]\n",
    "\n",
    "        br.sendTransform((px_gt, py_gt, pz_gt),\n",
    "                         (qx_gt, qy_gt, qz_gt, qw_gt),\n",
    "                         rospy.Time.now(), \"gt\", \"world\")\n",
    "\n",
    "        timestamp = rospy.Time.now()\n",
    "\n",
    "        nn_pose_msg = Odometry()\n",
    "        nn_pose_msg.header.frame_id = 'world'\n",
    "        nn_pose_msg.header.stamp = timestamp\n",
    "        nn_pose_msg.child_frame_id = 'base_link'\n",
    "        nn_pose_msg.pose.pose.position.x = px_pred\n",
    "        nn_pose_msg.pose.pose.position.y = py_pred\n",
    "        nn_pose_msg.pose.pose.position.z = pz_pred\n",
    "        [nn_pose_msg.pose.pose.orientation.x, nn_pose_msg.pose.pose.orientation.y, nn_pose_msg.pose.pose.orientation.z, nn_pose_msg.pose.pose.orientation.w] = [qx_pred, qy_pred, qz_pred, qw_pred]\n",
    "\n",
    "        conv = np.zeros((6,6), dtype=np.float32)\n",
    "        [conv[0][0], conv[1][1], conv[2][2]] = trans_cov[0]\n",
    "        nn_pose_msg.pose.covariance = conv.flatten().tolist()\n",
    "        nn_pose_pub.publish(nn_pose_msg)\n",
    "\n",
    "        bridge = CvBridge()\n",
    "\n",
    "        bird_view_img_msg = bridge.cv2_to_imgmsg(np.asarray(x[0].cpu(), dtype=np.float32), encoding=\"passthrough\")\n",
    "        stamp_now = rospy.Time.now()\n",
    "        bird_view_img_msg.header.stamp = stamp_now\n",
    "\n",
    "        bird_view_pub.publish(bird_view_img_msg)\n",
    "\n",
    "        rospy.sleep(.0)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        count += 1\n",
    "    else:\n",
    "        count += y.shape[0]\n",
    "    \n",
    "    trans_preds += [x for x in trans_pred]\n",
    "    rot_preds += [x for x in rot_pred]\n",
    "    trans_gts += [x for x in trans_gt]\n",
    "    rot_gts += [x for x in rot_gt]\n",
    "    samples_mins += [x for x in samples_min]\n",
    "\n",
    "    trans_error = np.sqrt(np.sum((trans_pred - trans_gt)**2,axis=1))\n",
    "    rot_error_1 = np.arccos(np.sum(np.multiply(rot_pred,rot_gt),axis=1))/math.pi*180\n",
    "    rot_error_2 = np.arccos(np.sum(np.multiply(rot_pred,-rot_gt),axis=1))/math.pi*180\n",
    "    rot_error = np.minimum(rot_error_1,rot_error_2)\n",
    "\n",
    "    trans_errors += [x for x in trans_error]\n",
    "    rot_errors += [x for x in rot_error]\n",
    "\n",
    "    total_trans_error += np.sum(trans_error)\n",
    "    total_rot_error += np.sum(rot_error)\n",
    "    \n",
    "    display = 1\n",
    "\n",
    "    if b % display == 0:\n",
    "        print(\n",
    "            \"{}/{}, translation error = {:.3f}, rotation error = {:.3f}, time/batch = {:.3f}\"\n",
    "            .format(\n",
    "             (b+1)*args.batch_size,\n",
    "            len(dataloader)*args.batch_size,\n",
    "            total_trans_error / count,\n",
    "            total_rot_error / count,\n",
    "            end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T01:51:18.669372Z",
     "start_time": "2020-08-29T01:51:00.884240Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median translation error = 2.306\n",
      "median rotation error = 3.165\n",
      "mean translation error = 11.305\n",
      "mean rotation error = 6.630\n",
      "median sample min error = 2.151\n",
      "mean sample min error = 11.149\n"
     ]
    }
   ],
   "source": [
    "sio.savemat('results.mat', {'trans_pred': np.array(trans_preds), 'trans_gt': np.array(trans_gts), 'uncertainty': np.array(pred_uncertainties)})\n",
    "\n",
    "if len(pose_map):\n",
    "    np.savetxt(os.path.join(args.map_dataset, 'map.txt'), np.asarray(pose_map, dtype=np.float32))\n",
    "    print(\"map is saved!\")\n",
    "\n",
    "plt.hist(trans_errors, bins='auto')\n",
    "plt.title(\"Translation errors\")\n",
    "plt.xlabel(\"translational error in meters\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('terror.png', bbox_inches='tight')\n",
    "\n",
    "plt.hist(rot_errors, bins='auto')\n",
    "plt.title(\"Rotation errors\")\n",
    "plt.xlabel(\"rotational error in degree\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('rerror.png', bbox_inches='tight')\n",
    "\n",
    "median_trans_errors = np.median(trans_errors)\n",
    "median_rot_errors = np.median(rot_errors)\n",
    "mean_trans_errors = np.mean(trans_errors)\n",
    "mean_rot_errors = np.mean(rot_errors)\n",
    "\n",
    "print(\"median translation error = {:.3f}\".format(median_trans_errors))\n",
    "print(\"median rotation error = {:.3f}\".format(median_rot_errors))\n",
    "print(\"mean translation error = {:.3f}\".format(mean_trans_errors))\n",
    "print(\"mean rotation error = {:.3f}\".format(mean_rot_errors))\n",
    "print(\"median sample min error = {:.3f}\".format(np.median(samples_mins)))\n",
    "print(\"mean sample min error = {:.3f}\".format(np.mean(samples_mins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T21:14:57.875539Z",
     "start_time": "2020-08-28T21:14:57.778684Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== median translation error ==================\n",
      "median translation error = 1.850\n",
      "median translation error = 1.886\n",
      "median translation error = 2.191\n",
      "median translation error = 2.153\n",
      "median translation error = 2.213\n",
      "median translation error = 2.329\n",
      "median translation error = 3.810\n",
      "median translation error = 3.268\n",
      "================== median rotation error ==================\n",
      "median rotation error = 2.597\n",
      "median rotation error = 2.820\n",
      "median rotation error = 3.000\n",
      "median rotation error = 2.950\n",
      "median rotation error = 3.195\n",
      "median rotation error = 3.325\n",
      "median rotation error = 4.464\n",
      "median rotation error = 4.200\n",
      "================== mean translation error ==================\n",
      "mean translation error = 4.472\n",
      "mean translation error = 4.742\n",
      "mean translation error = 13.297\n",
      "mean translation error = 13.422\n",
      "mean translation error = 9.508\n",
      "mean translation error = 11.675\n",
      "mean translation error = 22.271\n",
      "mean translation error = 14.357\n",
      "================== mean rotation error ==================\n",
      "mean rotation error = 3.973\n",
      "mean rotation error = 4.261\n",
      "mean rotation error = 6.256\n",
      "mean rotation error = 5.974\n",
      "mean rotation error = 5.671\n",
      "mean rotation error = 6.656\n",
      "mean rotation error = 12.778\n",
      "mean rotation error = 9.359\n"
     ]
    }
   ],
   "source": [
    "def evaluate(trans_errors,rot_errors):\n",
    "    t = dataset.last_indexes\n",
    "    trans_errors_month = list()\n",
    "    trans_errors_month.append(trans_errors[:t[0]])\n",
    "    trans_errors_month.append(trans_errors[t[0]:t[1]])\n",
    "    trans_errors_month.append(trans_errors[t[1]:t[2]])\n",
    "    trans_errors_month.append(trans_errors[t[2]:t[3]])\n",
    "    trans_errors_month.append(trans_errors[t[3]:t[4]])\n",
    "    trans_errors_month.append(trans_errors[t[4]:t[5]])\n",
    "    trans_errors_month.append(trans_errors[t[5]:t[6]])\n",
    "    trans_errors_month.append(trans_errors[t[6]:])\n",
    "\n",
    "    rot_errors_month = list()\n",
    "    rot_errors_month.append(rot_errors[:t[0]])\n",
    "    rot_errors_month.append(rot_errors[t[0]:t[1]])\n",
    "    rot_errors_month.append(rot_errors[t[1]:t[2]])\n",
    "    rot_errors_month.append(rot_errors[t[2]:t[3]])\n",
    "    rot_errors_month.append(rot_errors[t[3]:t[4]])\n",
    "    rot_errors_month.append(rot_errors[t[4]:t[5]])\n",
    "    rot_errors_month.append(rot_errors[t[5]:t[6]])\n",
    "    rot_errors_month.append(rot_errors[t[6]:])\n",
    "    \n",
    "    print('================== median translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"median translation error = {:.3f}\".format(np.median(trans_errors_i)))\n",
    "        \n",
    "    print('================== median rotation error ==================')\n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"median rotation error = {:.3f}\".format(np.median(rot_errors_i)))\n",
    "    \n",
    "    print('================== mean translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"mean translation error = {:.3f}\".format(np.mean(trans_errors_i)))\n",
    "        \n",
    "    print('================== mean rotation error ==================')  \n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"mean rotation error = {:.3f}\".format(np.mean(rot_errors_i)))\n",
    "        \n",
    "evaluate(trans_errors,rot_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
