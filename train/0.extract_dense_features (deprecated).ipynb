{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:27.393835Z",
     "start_time": "2020-07-18T09:56:26.173617Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ List Devices ------------\n",
      "Device 0 :\n",
      "GeForce RTX 2060\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n",
      "Device 1 :\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "list_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:27.398565Z",
     "start_time": "2020-07-18T09:56:27.395147Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "set_device(1)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:27.415015Z",
     "start_time": "2020-07-18T09:56:27.400138Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=450, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.000001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.9, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.01, help='weight of rotation error')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "#parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "#parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "#parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08'])\n",
    "\n",
    "\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=20, help='display frequency')\n",
    "parser.add_argument('--tensorboard', type=bool, default=False, help='open tensorboard')\n",
    "parser.add_argument('--train_validate_rate', type=float, default=0.7, help='split validation')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter('runs/nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:50.384194Z",
     "start_time": "2020-07-18T09:56:27.416788Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16446/16446 [00:22<00:00, 726.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\\ntraining_dataset, val_dataset = random_split(dataset, lengths)\\n\\ntraining_dataloader = DataLoader(training_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n\\nval_dataloader = DataLoader(val_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset,normalize\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False)\n",
    "if len(args.train_dataset)>7:\n",
    "    [args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "    torch.save([args.norm_mean, args.norm_std], *args.norm_tensor)\n",
    "    print('Save norm and std:',*args.norm_tensor)\n",
    "else:\n",
    "    [args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "    print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)\n",
    "'''\n",
    "lengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\n",
    "training_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:50.393270Z",
     "start_time": "2020-07-18T09:56:50.385526Z"
    },
    "code_folding": [
     0,
     8,
     19
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        outputs = []\n",
    "        for input_data in args:\n",
    "            dense_feat = self.backbone(input_data)\n",
    "            output, feature_t, feature_r = self.nn(dense_feat)\n",
    "            outputs += [output]\n",
    "        if len(args)>1:\n",
    "            return outputs\n",
    "        else:\n",
    "            return output, feature_t, feature_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:54.549074Z",
     "start_time": "2020-07-18T09:56:50.394765Z"
    },
    "code_folding": [
     1,
     15,
     28,
     33,
     54,
     73
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters layer: 346\n",
      "backbone.resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "backbone.resnet.bn1.weight torch.Size([64])\n",
      "backbone.resnet.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "backbone.resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "backbone.resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "backbone.resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "backbone.resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "backbone.resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "backbone.resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "backbone.resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "backbone.resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "backbone.resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "nn.global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "nn.global_context.context.squeeze.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_1.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_2.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_3.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_4.0.bias torch.Size([128])\n",
      "nn.global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "nn.global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "nn.global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "nn.global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "nn.global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "nn.global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "nn.global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "nn.global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "nn.global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "nn.global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "nn.global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "nn.global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "nn.global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "nn.global_regressor.regressor.logits_r.bias torch.Size([4])\n",
      "Parameters layer: 346\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, training = True):\n",
    "        # data\n",
    "        self.model = Model().cuda()\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # training tool\n",
    "        if training:\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), \n",
    "                                        lr=args.learning_rate, \n",
    "                                        weight_decay=args.weight_decay)\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                         lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth'):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, file_name))\n",
    "        print('Parameters layer:',len(state_dict.keys()))\n",
    "        # load file to model\n",
    "        self.model.load_state_dict(state_dict,strict = True)\n",
    "        # Display model structure\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Parameters layer:',len(self.model.state_dict().keys()))\n",
    "        # check load\n",
    "        assert len(state_dict.keys()) == len(self.model.state_dict().keys())\n",
    "         \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "            \n",
    "    def train(self,x0, x1, y0, y1):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x0,x1,y0,y1 = x0.cuda(),x1.cuda(),y0.cuda(),y1.cuda()\n",
    "        if args.is_normalization:\n",
    "            y0, y1 = [normalize(y,self.norm_mean, self.norm_std) for y in [y0,y1]]\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        single_loss = self._loss(x0, x1, y0, y1)\n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        #Step 3: update\n",
    "        single_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "            \n",
    "    def _loss(self,x0, x1, y0, y1):\n",
    "        # target relative\n",
    "        relative_target_normed = get_relative_pose(y0, y1)\n",
    "        # forward output\n",
    "        global_output0,global_output1 = self.model(x0, x1)\n",
    "        # output relative\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        \n",
    "        # target loss\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, gt=y1, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        # relative loss\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + geometry_consistent_loss        \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        output,_,_ = self.model(x)\n",
    "\n",
    "        if args.is_normalization and output_denormalize:\n",
    "            output = denormalize(output, self.norm_mean, self.norm_std)\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_prediction, rot_prediction = torch.split(output, [3, 4], dim=1)\n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target\n",
    "    \n",
    "    def dense_feature_forward(self,x):\n",
    "        # Step 1: get data\n",
    "        x = x.cuda()\n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        dense_feat = self.model.backbone(x)\n",
    "        return dense_feat\n",
    "\n",
    "trainer = Trainer(training=True)\n",
    "trainer.load_model('pretrained_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:56:57.776800Z",
     "start_time": "2020-07-18T09:56:54.550518Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "x_data = torch.zeros([len(dataset), 2048, 10, 10])\n",
    "y_data = torch.zeros([len(dataset), 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:58:04.767869Z",
     "start_time": "2020-07-18T09:56:57.888182Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/37, time/batch = 1.236\n",
      "10/37, time/batch = 1.238\n",
      "15/37, time/batch = 1.239\n",
      "20/37, time/batch = 1.243\n",
      "25/37, time/batch = 1.990\n",
      "30/37, time/batch = 1.245\n",
      "35/37, time/batch = 1.246\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data.values()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dense_feature = trainer.dense_feature_forward(x)\n",
    "        if b == len(dataloader)-1:\n",
    "            x_data[b*args.batch_size:] = dense_feature\n",
    "            y_data[b*args.batch_size:] = y\n",
    "        else:\n",
    "            x_data[b*args.batch_size:(b+1)*args.batch_size] = dense_feature\n",
    "            y_data[b*args.batch_size:(b+1)*args.batch_size] = y\n",
    "            \n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        if ((b+1)%5 == 0):\n",
    "            print(\"{}/{}, time/batch = {:.3f}\".format((b+1),len(dataloader),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T09:58:56.293453Z",
     "start_time": "2020-07-18T09:58:04.770955Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(x_data, args.train_dataset[0]+'/x.pt')\n",
    "torch.save(y_data, args.train_dataset[0]+'/y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
