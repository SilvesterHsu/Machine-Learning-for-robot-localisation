{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T19:58:43.105810Z",
     "start_time": "2020-08-22T19:58:41.288779Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ List Devices ------------\n",
      "Device 0 :\n",
      "GeForce RTX 2060\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n",
      "Device 1 :\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seel/.local/lib/python3.6/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "list_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T19:58:43.113244Z",
     "start_time": "2020-08-22T19:58:43.108081Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "set_device(1)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T19:58:43.134139Z",
     "start_time": "2020-08-22T19:58:43.115138Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=400, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.000001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.9, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.01, help='weight of rotation error')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "#parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "#parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "\n",
    "parser.add_argument('--test_dataset', type=str, default=[# '/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_02_12',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_04_29',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_05_11',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_06_15',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_08_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "                                                         '/notebooks/michigan_nn_data/2012_10_28',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_11_16',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_12_01'\n",
    "                                                        ] )\n",
    "'''\n",
    "#parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08'])\n",
    "'''\n",
    "\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=20, help='display frequency')\n",
    "parser.add_argument('--tensorboard', type=bool, default=False, help='open tensorboard')\n",
    "parser.add_argument('--train_validate_rate', type=float, default=0.7, help='split validation')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter('runs/nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:06:54.608067Z",
     "start_time": "2020-08-22T19:58:43.135685Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32892/32892 [00:42<00:00, 782.73it/s]\n",
      "100%|██████████| 45169/45169 [00:58<00:00, 771.16it/s]\n",
      "100%|██████████| 37310/37310 [00:54<00:00, 678.64it/s]\n",
      "100%|██████████| 34621/34621 [01:04<00:00, 538.28it/s]\n",
      "100%|██████████| 21533/21533 [00:42<00:00, 504.70it/s]\n",
      "100%|██████████| 29757/29757 [01:03<00:00, 465.24it/s]\n",
      "100%|██████████| 26904/26904 [01:28<00:00, 304.03it/s]\n",
      "100%|██████████| 28074/28074 [01:14<00:00, 378.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\\ntraining_dataset, val_dataset = random_split(dataset, lengths)\\n\\ntraining_dataloader = DataLoader(training_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n\\nval_dataloader = DataLoader(val_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset,normalize\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# testing\n",
    "'''\n",
    "dataset = LocalizationDataset(dataset_dirs = args.test_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False, mode='evaluate', sampling_rate=1)\n",
    "'''\n",
    "# training\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False, sampling_rate=1)\n",
    "'''\n",
    "if len(args.train_dataset)>7:\n",
    "    [args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "    torch.save([args.norm_mean, args.norm_std], *args.norm_tensor)\n",
    "    print('Save norm and std:',*args.norm_tensor)\n",
    "else:\n",
    "    [args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "    print('Load norm and std:',*args.norm_tensor)\n",
    "'''\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)\n",
    "'''\n",
    "lengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\n",
    "training_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:06:54.688394Z",
     "start_time": "2020-08-22T20:06:54.635053Z"
    },
    "code_folding": [
     0,
     8,
     19
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        outputs = []\n",
    "        for input_data in args:\n",
    "            dense_feat = self.backbone(input_data)\n",
    "            output, feature_t, feature_r = self.nn(dense_feat)\n",
    "            outputs += [output]\n",
    "        if len(args)>1:\n",
    "            return outputs\n",
    "        else:\n",
    "            return output, feature_t, feature_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:07:07.396560Z",
     "start_time": "2020-08-22T20:06:54.695927Z"
    },
    "code_folding": [
     1,
     16,
     29,
     39,
     44,
     65,
     84,
     103
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, training = True):\n",
    "        # data\n",
    "        self.model = Model().cuda()\n",
    "        self.args = args\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # training tool\n",
    "        if training:\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), \n",
    "                                        lr=args.learning_rate, \n",
    "                                        weight_decay=args.weight_decay)\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                         lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth', strict = True):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(self.args.model_dir, file_name))\n",
    "        if 'net.resnet.conv1.weight' in state_dict:\n",
    "            print('Transform from old model.')\n",
    "            state_dict = self._from_old_model(state_dict)\n",
    "            self.model.load_state_dict(state_dict,strict = strict)\n",
    "        else:\n",
    "            #print('Parameters layer:',len(state_dict.keys()))\n",
    "            # load file to model\n",
    "            self.model.load_state_dict(state_dict,strict = strict)\n",
    "        #print(\"Successfully loaded model to {}.\".format(self.device_name))\n",
    "    \n",
    "    def _from_old_model(self, state_dict, select = 'backbone'):\n",
    "        for key in list(state_dict):\n",
    "            if 'net.resnet.' in key:\n",
    "                state_dict[key.replace('net.resnet.','backbone.resnet.')] = state_dict.pop(key)\n",
    "            if 'net.global_regressor.' in key:\n",
    "                state_dict[key.replace('net.global_regressor.','nn.global_regressor.')] = state_dict.pop(key)\n",
    "            elif 'net.global_context.' in key:\n",
    "                state_dict[key.replace('net.global_context.','nn.global_context.')] = state_dict.pop(key)\n",
    "        return state_dict\n",
    "         \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "            \n",
    "    def train(self,x0, x1, y0, y1):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x0,x1,y0,y1 = x0.cuda(),x1.cuda(),y0.cuda(),y1.cuda()\n",
    "        if args.is_normalization:\n",
    "            y0, y1 = [normalize(y,self.norm_mean, self.norm_std) for y in [y0,y1]]\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        single_loss = self._loss(x0, x1, y0, y1)\n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        #Step 3: update\n",
    "        single_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "            \n",
    "    def _loss(self,x0, x1, y0, y1):\n",
    "        # target relative\n",
    "        relative_target_normed = get_relative_pose(y0, y1)\n",
    "        # forward output\n",
    "        global_output0,global_output1 = self.model(x0, x1)\n",
    "        # output relative\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        \n",
    "        # target loss\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, gt=y1, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        # relative loss\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + geometry_consistent_loss        \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        output,_,_ = self.model(x)\n",
    "\n",
    "        if args.is_normalization and output_denormalize:\n",
    "            output = denormalize(output, self.norm_mean, self.norm_std)\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_prediction, rot_prediction = torch.split(output, [3, 4], dim=1)\n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target\n",
    "    \n",
    "    def dense_feature_forward(self,x):\n",
    "        # Step 1: get data\n",
    "        x = x.cuda()\n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        dense_feat = self.model.backbone(x)\n",
    "        dense_feat = self.model.nn.global_context(dense_feat)\n",
    "        dense_feat = self.model.nn.global_regressor.regressor.flatten(dense_feat)\n",
    "        return dense_feat\n",
    "\n",
    "trainer = Trainer(args,training=True)\n",
    "#trainer.load_model('pretrained_feature.pth')\n",
    "#trainer.load_model('pretrained_old.pth',strict = False)\n",
    "trainer.load_model('pretrained_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:07:17.822795Z",
     "start_time": "2020-08-22T20:07:07.405105Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "x_data = torch.zeros([len(dataset), 6400])\n",
    "y_data = torch.zeros([len(dataset), 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:23:19.854132Z",
     "start_time": "2020-08-22T20:07:17.837447Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/641, time/batch = 0.998\n",
      "10/641, time/batch = 0.995\n",
      "15/641, time/batch = 0.997\n",
      "20/641, time/batch = 1.003\n",
      "25/641, time/batch = 1.003\n",
      "30/641, time/batch = 1.007\n",
      "35/641, time/batch = 1.003\n",
      "40/641, time/batch = 1.009\n",
      "45/641, time/batch = 1.005\n",
      "50/641, time/batch = 1.002\n",
      "55/641, time/batch = 1.003\n",
      "60/641, time/batch = 1.010\n",
      "65/641, time/batch = 1.011\n",
      "70/641, time/batch = 1.010\n",
      "75/641, time/batch = 1.010\n",
      "80/641, time/batch = 1.015\n",
      "85/641, time/batch = 1.009\n",
      "90/641, time/batch = 1.009\n",
      "95/641, time/batch = 1.006\n",
      "100/641, time/batch = 1.019\n",
      "105/641, time/batch = 1.011\n",
      "110/641, time/batch = 1.015\n",
      "115/641, time/batch = 1.013\n",
      "120/641, time/batch = 1.017\n",
      "125/641, time/batch = 1.015\n",
      "130/641, time/batch = 1.017\n",
      "135/641, time/batch = 1.014\n",
      "140/641, time/batch = 1.013\n",
      "145/641, time/batch = 1.012\n",
      "150/641, time/batch = 1.014\n",
      "155/641, time/batch = 1.015\n",
      "160/641, time/batch = 1.017\n",
      "165/641, time/batch = 1.010\n",
      "170/641, time/batch = 1.010\n",
      "175/641, time/batch = 1.011\n",
      "180/641, time/batch = 1.015\n",
      "185/641, time/batch = 1.008\n",
      "190/641, time/batch = 1.017\n",
      "195/641, time/batch = 1.008\n",
      "200/641, time/batch = 1.013\n",
      "205/641, time/batch = 1.019\n",
      "210/641, time/batch = 1.015\n",
      "215/641, time/batch = 1.237\n",
      "220/641, time/batch = 1.241\n",
      "225/641, time/batch = 1.229\n",
      "230/641, time/batch = 1.234\n",
      "235/641, time/batch = 1.235\n",
      "240/641, time/batch = 1.228\n",
      "245/641, time/batch = 1.226\n",
      "250/641, time/batch = 1.196\n",
      "255/641, time/batch = 1.244\n",
      "260/641, time/batch = 1.228\n",
      "265/641, time/batch = 1.238\n",
      "270/641, time/batch = 1.223\n",
      "275/641, time/batch = 1.231\n",
      "280/641, time/batch = 1.217\n",
      "285/641, time/batch = 1.231\n",
      "290/641, time/batch = 1.237\n",
      "295/641, time/batch = 1.239\n",
      "300/641, time/batch = 1.237\n",
      "305/641, time/batch = 1.239\n",
      "310/641, time/batch = 1.225\n",
      "315/641, time/batch = 1.227\n",
      "320/641, time/batch = 1.230\n",
      "325/641, time/batch = 1.247\n",
      "330/641, time/batch = 1.260\n",
      "335/641, time/batch = 1.238\n",
      "340/641, time/batch = 1.227\n",
      "345/641, time/batch = 1.238\n",
      "350/641, time/batch = 1.236\n",
      "355/641, time/batch = 1.238\n",
      "360/641, time/batch = 1.234\n",
      "365/641, time/batch = 1.232\n",
      "370/641, time/batch = 1.243\n",
      "375/641, time/batch = 1.248\n",
      "380/641, time/batch = 1.232\n",
      "385/641, time/batch = 1.234\n",
      "390/641, time/batch = 1.238\n",
      "395/641, time/batch = 1.234\n",
      "400/641, time/batch = 1.239\n",
      "405/641, time/batch = 1.230\n",
      "410/641, time/batch = 1.248\n",
      "415/641, time/batch = 1.214\n",
      "420/641, time/batch = 1.231\n",
      "425/641, time/batch = 1.237\n",
      "430/641, time/batch = 1.220\n",
      "435/641, time/batch = 1.270\n",
      "440/641, time/batch = 1.254\n",
      "445/641, time/batch = 1.239\n",
      "450/641, time/batch = 1.229\n",
      "455/641, time/batch = 1.233\n",
      "460/641, time/batch = 1.238\n",
      "465/641, time/batch = 1.226\n",
      "470/641, time/batch = 1.243\n",
      "475/641, time/batch = 1.228\n",
      "480/641, time/batch = 1.243\n",
      "485/641, time/batch = 1.244\n",
      "490/641, time/batch = 1.241\n",
      "495/641, time/batch = 1.235\n",
      "500/641, time/batch = 1.240\n",
      "505/641, time/batch = 1.227\n",
      "510/641, time/batch = 1.228\n",
      "515/641, time/batch = 1.235\n",
      "520/641, time/batch = 1.231\n",
      "525/641, time/batch = 1.239\n",
      "530/641, time/batch = 1.231\n",
      "535/641, time/batch = 1.235\n",
      "540/641, time/batch = 1.231\n",
      "545/641, time/batch = 1.234\n",
      "550/641, time/batch = 1.253\n",
      "555/641, time/batch = 1.255\n",
      "560/641, time/batch = 1.228\n",
      "565/641, time/batch = 1.227\n",
      "570/641, time/batch = 1.231\n",
      "575/641, time/batch = 1.234\n",
      "580/641, time/batch = 1.222\n",
      "585/641, time/batch = 1.239\n",
      "590/641, time/batch = 1.240\n",
      "595/641, time/batch = 1.218\n",
      "600/641, time/batch = 1.236\n",
      "605/641, time/batch = 1.253\n",
      "610/641, time/batch = 1.221\n",
      "615/641, time/batch = 1.242\n",
      "620/641, time/batch = 1.236\n",
      "625/641, time/batch = 1.244\n",
      "630/641, time/batch = 1.235\n",
      "635/641, time/batch = 1.222\n",
      "640/641, time/batch = 1.225\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data.values()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dense_feature = trainer.dense_feature_forward(x)\n",
    "        if b == len(dataloader)-1:\n",
    "            x_data[b*args.batch_size:] = dense_feature\n",
    "            y_data[b*args.batch_size:] = y\n",
    "        else:\n",
    "            x_data[b*args.batch_size:(b+1)*args.batch_size] = dense_feature\n",
    "            y_data[b*args.batch_size:(b+1)*args.batch_size] = y\n",
    "            \n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        if ((b+1)%5 == 0):\n",
    "            print(\"{}/{}, time/batch = {:.3f}\".format((b+1),len(dataloader),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T20:26:19.439727Z",
     "start_time": "2020-08-22T20:23:19.864689Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(x_data, 'x.pt')\n",
    "torch.save(y_data, 'y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
