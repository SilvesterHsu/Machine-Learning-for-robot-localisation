{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:05:17.315680Z",
     "start_time": "2020-08-21T13:05:15.505758Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ List Devices ------------\n",
      "Device 0 :\n",
      "GeForce RTX 2060\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n",
      "Device 1 :\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seel/.local/lib/python3.6/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "list_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:05:17.321178Z",
     "start_time": "2020-08-21T13:05:17.317189Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "set_device(1)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:05:17.339360Z",
     "start_time": "2020-08-21T13:05:17.322969Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=400, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.000001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.9, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.01, help='weight of rotation error')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "#parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "#parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "\n",
    "parser.add_argument('--test_dataset', type=str, default=[# '/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_02_12',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_04_29',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_05_11',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_06_15',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_08_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "                                                         '/notebooks/michigan_nn_data/2012_10_28',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_11_16',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_12_01'\n",
    "                                                        ] )\n",
    "'''\n",
    "#parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08'])\n",
    "'''\n",
    "\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=20, help='display frequency')\n",
    "parser.add_argument('--tensorboard', type=bool, default=False, help='open tensorboard')\n",
    "parser.add_argument('--train_validate_rate', type=float, default=0.7, help='split validation')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter('runs/nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:10:00.047090Z",
     "start_time": "2020-08-21T13:05:17.340881Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28602/28602 [00:36<00:00, 778.92it/s]\n",
      "100%|██████████| 14016/14016 [00:18<00:00, 772.93it/s]\n",
      "100%|██████████| 25704/25704 [00:32<00:00, 780.69it/s]\n",
      "100%|██████████| 19135/19135 [00:24<00:00, 774.78it/s]\n",
      "100%|██████████| 27160/27160 [00:37<00:00, 717.03it/s]\n",
      "100%|██████████| 29670/29670 [00:52<00:00, 568.51it/s]\n",
      "100%|██████████| 14229/14229 [00:27<00:00, 521.68it/s]\n",
      "100%|██████████| 25367/25367 [00:51<00:00, 494.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\\ntraining_dataset, val_dataset = random_split(dataset, lengths)\\n\\ntraining_dataloader = DataLoader(training_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n\\nval_dataloader = DataLoader(val_dataset, batch_size=args.batch_size,                     shuffle=True, num_workers=0,                     drop_last=False, pin_memory=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset,normalize\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.test_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False, mode='evaluate', sampling_rate=1)\n",
    "'''\n",
    "if len(args.train_dataset)>7:\n",
    "    [args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "    torch.save([args.norm_mean, args.norm_std], *args.norm_tensor)\n",
    "    print('Save norm and std:',*args.norm_tensor)\n",
    "else:\n",
    "    [args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "    print('Load norm and std:',*args.norm_tensor)\n",
    "'''\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)\n",
    "'''\n",
    "lengths = [round(len(dataset)*args.train_validate_rate), round(len(dataset)*(1-args.train_validate_rate))]\n",
    "training_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, \\\n",
    "                    shuffle=True, num_workers=0, \\\n",
    "                    drop_last=False, pin_memory=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:10:00.133942Z",
     "start_time": "2020-08-21T13:10:00.068143Z"
    },
    "code_folding": [
     0,
     8,
     19
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        outputs = []\n",
    "        for input_data in args:\n",
    "            dense_feat = self.backbone(input_data)\n",
    "            output, feature_t, feature_r = self.nn(dense_feat)\n",
    "            outputs += [output]\n",
    "        if len(args)>1:\n",
    "            return outputs\n",
    "        else:\n",
    "            return output, feature_t, feature_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:10:11.149065Z",
     "start_time": "2020-08-21T13:10:00.141453Z"
    },
    "code_folding": [
     1,
     39,
     44,
     65,
     84
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform from old model.\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, training = True):\n",
    "        # data\n",
    "        self.model = Model().cuda()\n",
    "        self.args = args\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # training tool\n",
    "        if training:\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), \n",
    "                                        lr=args.learning_rate, \n",
    "                                        weight_decay=args.weight_decay)\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                         lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth', strict = True):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(self.args.model_dir, file_name))\n",
    "        if 'net.resnet.conv1.weight' in state_dict:\n",
    "            print('Transform from old model.')\n",
    "            state_dict = self._from_old_model(state_dict)\n",
    "            self.model.load_state_dict(state_dict,strict = strict)\n",
    "        else:\n",
    "            #print('Parameters layer:',len(state_dict.keys()))\n",
    "            # load file to model\n",
    "            self.model.load_state_dict(state_dict,strict = strict)\n",
    "        #print(\"Successfully loaded model to {}.\".format(self.device_name))\n",
    "    \n",
    "    def _from_old_model(self, state_dict, select = 'backbone'):\n",
    "        for key in list(state_dict):\n",
    "            if 'net.resnet.' in key:\n",
    "                state_dict[key.replace('net.resnet.','backbone.resnet.')] = state_dict.pop(key)\n",
    "            if 'net.global_regressor.' in key:\n",
    "                state_dict[key.replace('net.global_regressor.','nn.global_regressor.')] = state_dict.pop(key)\n",
    "            elif 'net.global_context.' in key:\n",
    "                state_dict[key.replace('net.global_context.','nn.global_context.')] = state_dict.pop(key)\n",
    "        return state_dict\n",
    "         \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "            \n",
    "    def train(self,x0, x1, y0, y1):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x0,x1,y0,y1 = x0.cuda(),x1.cuda(),y0.cuda(),y1.cuda()\n",
    "        if args.is_normalization:\n",
    "            y0, y1 = [normalize(y,self.norm_mean, self.norm_std) for y in [y0,y1]]\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        single_loss = self._loss(x0, x1, y0, y1)\n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        #Step 3: update\n",
    "        single_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "            \n",
    "    def _loss(self,x0, x1, y0, y1):\n",
    "        # target relative\n",
    "        relative_target_normed = get_relative_pose(y0, y1)\n",
    "        # forward output\n",
    "        global_output0,global_output1 = self.model(x0, x1)\n",
    "        # output relative\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        \n",
    "        # target loss\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, gt=y1, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        # relative loss\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + geometry_consistent_loss        \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        output,_,_ = self.model(x)\n",
    "\n",
    "        if args.is_normalization and output_denormalize:\n",
    "            output = denormalize(output, self.norm_mean, self.norm_std)\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_prediction, rot_prediction = torch.split(output, [3, 4], dim=1)\n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target\n",
    "    \n",
    "    def dense_feature_forward(self,x):\n",
    "        # Step 1: get data\n",
    "        x = x.cuda()\n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        dense_feat = self.model.backbone(x)\n",
    "        dense_feat = self.model.nn.global_context(dense_feat)\n",
    "        dense_feat = self.model.nn.global_regressor.regressor.flatten(dense_feat)\n",
    "        return dense_feat\n",
    "\n",
    "trainer = Trainer(args,training=True)\n",
    "trainer.load_model('pretrained_old.pth', strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:10:19.752703Z",
     "start_time": "2020-08-21T13:10:11.151339Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "x_data = torch.zeros([len(dataset), 6400])\n",
    "y_data = torch.zeros([len(dataset), 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:20:43.914937Z",
     "start_time": "2020-08-21T13:10:19.764089Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/460, time/batch = 1.000\n",
      "10/460, time/batch = 0.998\n",
      "15/460, time/batch = 0.996\n",
      "20/460, time/batch = 1.006\n",
      "25/460, time/batch = 1.004\n",
      "30/460, time/batch = 1.001\n",
      "35/460, time/batch = 1.006\n",
      "40/460, time/batch = 1.007\n",
      "45/460, time/batch = 1.010\n",
      "50/460, time/batch = 1.007\n",
      "55/460, time/batch = 1.011\n",
      "60/460, time/batch = 1.007\n",
      "65/460, time/batch = 1.004\n",
      "70/460, time/batch = 1.011\n",
      "75/460, time/batch = 1.024\n",
      "80/460, time/batch = 1.019\n",
      "85/460, time/batch = 1.013\n",
      "90/460, time/batch = 1.017\n",
      "95/460, time/batch = 1.009\n",
      "100/460, time/batch = 1.019\n",
      "105/460, time/batch = 1.012\n",
      "110/460, time/batch = 1.026\n",
      "115/460, time/batch = 1.011\n",
      "120/460, time/batch = 1.011\n",
      "125/460, time/batch = 1.016\n",
      "130/460, time/batch = 1.022\n",
      "135/460, time/batch = 1.010\n",
      "140/460, time/batch = 1.011\n",
      "145/460, time/batch = 1.015\n",
      "150/460, time/batch = 1.013\n",
      "155/460, time/batch = 1.010\n",
      "160/460, time/batch = 1.021\n",
      "165/460, time/batch = 1.019\n",
      "170/460, time/batch = 1.015\n",
      "175/460, time/batch = 1.014\n",
      "180/460, time/batch = 1.016\n",
      "185/460, time/batch = 1.015\n",
      "190/460, time/batch = 1.017\n",
      "195/460, time/batch = 1.011\n",
      "200/460, time/batch = 1.019\n",
      "205/460, time/batch = 1.016\n",
      "210/460, time/batch = 1.014\n",
      "215/460, time/batch = 1.017\n",
      "220/460, time/batch = 1.019\n",
      "225/460, time/batch = 1.023\n",
      "230/460, time/batch = 1.026\n",
      "235/460, time/batch = 1.097\n",
      "240/460, time/batch = 1.115\n",
      "245/460, time/batch = 1.071\n",
      "250/460, time/batch = 1.068\n",
      "255/460, time/batch = 1.072\n",
      "260/460, time/batch = 1.069\n",
      "265/460, time/batch = 1.094\n",
      "270/460, time/batch = 1.081\n",
      "275/460, time/batch = 1.091\n",
      "280/460, time/batch = 1.063\n",
      "285/460, time/batch = 1.077\n",
      "290/460, time/batch = 1.079\n",
      "295/460, time/batch = 1.085\n",
      "300/460, time/batch = 1.087\n",
      "305/460, time/batch = 1.074\n",
      "310/460, time/batch = 1.093\n",
      "315/460, time/batch = 1.078\n",
      "320/460, time/batch = 1.071\n",
      "325/460, time/batch = 1.040\n",
      "330/460, time/batch = 1.072\n",
      "335/460, time/batch = 1.053\n",
      "340/460, time/batch = 1.103\n",
      "345/460, time/batch = 1.085\n",
      "350/460, time/batch = 1.102\n",
      "355/460, time/batch = 1.094\n",
      "360/460, time/batch = 1.072\n",
      "365/460, time/batch = 1.097\n",
      "370/460, time/batch = 1.080\n",
      "375/460, time/batch = 1.092\n",
      "380/460, time/batch = 1.059\n",
      "385/460, time/batch = 1.099\n",
      "390/460, time/batch = 1.096\n",
      "395/460, time/batch = 1.086\n",
      "400/460, time/batch = 1.069\n",
      "405/460, time/batch = 1.086\n",
      "410/460, time/batch = 1.094\n",
      "415/460, time/batch = 1.091\n",
      "420/460, time/batch = 1.097\n",
      "425/460, time/batch = 1.095\n",
      "430/460, time/batch = 1.082\n",
      "435/460, time/batch = 1.087\n",
      "440/460, time/batch = 1.081\n",
      "445/460, time/batch = 1.083\n",
      "450/460, time/batch = 1.096\n",
      "455/460, time/batch = 1.084\n",
      "460/460, time/batch = 3.990\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data.values()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dense_feature = trainer.dense_feature_forward(x)\n",
    "        if b == len(dataloader)-1:\n",
    "            x_data[b*args.batch_size:] = dense_feature\n",
    "            y_data[b*args.batch_size:] = y\n",
    "        else:\n",
    "            x_data[b*args.batch_size:(b+1)*args.batch_size] = dense_feature\n",
    "            y_data[b*args.batch_size:(b+1)*args.batch_size] = y\n",
    "            \n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        if ((b+1)%5 == 0):\n",
    "            print(\"{}/{}, time/batch = {:.3f}\".format((b+1),len(dataloader),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:21:31.877531Z",
     "start_time": "2020-08-21T13:20:43.951626Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(x_data, 'x.pt')\n",
    "torch.save(y_data, 'y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
