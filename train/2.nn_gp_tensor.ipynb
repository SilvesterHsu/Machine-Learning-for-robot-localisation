{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPUÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:11.694349Z",
     "start_time": "2020-08-19T19:42:10.481882Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "# S1: check GPU\n",
    "#list_device()\n",
    "\n",
    "# S2: default parameters\n",
    "set_device(1)\n",
    "np.set_printoptions(precision = 2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:11.710513Z",
     "start_time": "2020-08-19T19:42:11.696093Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=5000, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.007, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.8, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=.01, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/gp_net_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/train_dense_old'])\n",
    "\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "parser.add_argument('--tensorboard', type=bool, default=True, help='open tensorboard')\n",
    "parser.add_argument('--cuda_device', type=int, default=1, help='cuda device')\n",
    "\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:14.014825Z",
     "start_time": "2020-08-19T19:42:11.712150Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize_navie, denormalize, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "path = args.train_dataset[0]\n",
    "x_dense = torch.load(os.path.join(path,'x.pt'))\n",
    "y_dense = torch.load(os.path.join(path,'y.pt'))\n",
    "dataset = TensorDataset(x_dense, y_dense)\n",
    "\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:14.058992Z",
     "start_time": "2020-08-19T19:42:14.017557Z"
    },
    "code_folding": [
     3,
     15,
     37,
     54,
     55,
     77,
     110,
     137,
     152,
     180,
     185
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchlib.GPs import Backbone, NN, BaseModule\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        #context_feat = self.global_context(input_data)\n",
    "        context_feat = input_data\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([3]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([3])),\n",
    "            batch_shape=torch.Size([3]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        inducing_points = torch.zeros(3, 1000, 128)\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        #dense_feat = self.backbone(input_data)\n",
    "        dense_feat = input_data\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention \n",
    "        trans_pred = self.gp(feature_t)\n",
    "        return trans_pred, rot_pred\n",
    "\n",
    "class PosePredictor(BaseModule):\n",
    "    def __init__(self, norm_mean, norm_std, args,\n",
    "                 is_training=True, mixed_precision=True,\n",
    "                 regressor_context_rate = [0.0,0.0], train_rot = False):\n",
    "        super().__init__(norm_mean, norm_std, args)\n",
    "        self.model = Model().to(self.device)\n",
    "        self.train_rot = train_rot\n",
    "        self.mixed_precision = mixed_precision\n",
    "        if self.mixed_precision:\n",
    "            self.scaler = GradScaler()\n",
    "\n",
    "        self.disable_requires_grad(self.model.backbone)\n",
    "        \n",
    "        if is_training:\n",
    "            self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                             lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "            num_data = min(len(dataloader)*args.batch_size,len(dataset))\n",
    "            self.mll = gpytorch.mlls.PredictiveLogLikelihood(self.model.likelihood, \\\n",
    "                                                         self.model.gp, num_data = num_data)\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model)\n",
    "            \n",
    "    def _optimize(self,regressor_context_rate):\n",
    "        # GP\n",
    "        optimizer = [\n",
    "                {'params': self.model.gp.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "                {'params': self.model.likelihood.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay}]\n",
    "            \n",
    "        # NN\n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor)\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "            self.train_rot = True\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_context)\n",
    "            \n",
    "        \n",
    "        if not self.train_rot and regressor_context_rate[1]==0.0:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc1_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc2_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc3_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.logits_r)\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    def train(self,x, y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        if self.mixed_precision:\n",
    "            with autocast():\n",
    "                single_loss = self._loss(x, y)\n",
    "            self.scaler.scale(single_loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            single_loss = self._loss(x, y)\n",
    "            single_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "    \n",
    "    def _loss(self,x, y):\n",
    "        # target\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        # predict\n",
    "        trans_pred, rot_pred = self.model(x)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*self.mll(trans_pred, trans_target)\n",
    "        # rot loss\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        \n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss      \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,num_sample = 100,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        if self.mixed_precision:\n",
    "            with autocast():\n",
    "                trans_prediction, rot_prediction = self.model(x)\n",
    "            self.scaler.scale(trans_prediction)\n",
    "            self.scaler.scale(rot_prediction)\n",
    "        else:\n",
    "            trans_prediction, rot_prediction = self.model(x)\n",
    "            \n",
    "        trans_prediction, trans_mean, trans_var = self._eval_gp(trans_prediction)\n",
    "        \n",
    "        if args.is_normalization and output_denormalize:\n",
    "            trans_prediction = denormalize_navie(trans_prediction, self.norm_mean, self.norm_std)\n",
    "            trans_mean = denormalize_navie(trans_mean, self.norm_mean, self.norm_std)\n",
    "            trans_var = trans_var.mul(self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_mean, trans_var, num_sample)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target, samples\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = torch.distributions.Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "    \n",
    "    def _eval_gp(self, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = self.model.likelihood(trans_pred).mean, self.model.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:17.365559Z",
     "start_time": "2020-08-19T19:42:14.060403Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor learn rate: 0.05\n",
      "Successfully loaded model to TITAN Xp.\n"
     ]
    }
   ],
   "source": [
    "trainer = PosePredictor(args.norm_mean,args.norm_std,args,mixed_precision=False,\n",
    "                        regressor_context_rate = [0.05,0.], train_rot = False)\n",
    "#trainer.load_model('pretrained.pth',strict = False)\n",
    "#trainer.load_model('model-fast.pth')\n",
    "trainer.load_model('model-fast-zero-mean.pth')\n",
    "#trainer.show_require_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:17.375004Z",
     "start_time": "2020-08-19T19:42:17.367014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstate_dict = torch.load(os.path.join(args.model_dir, 'pretrained.pth'))\\nstate_dict = trainer._from_old_model(state_dict)\\nfor key in list(state_dict):\\n    if 'gp' in key or 'likelihood' in key:\\n        state_dict.pop(key)\\ntrainer.model.load_state_dict(state_dict, strict=False)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "state_dict = torch.load(os.path.join(args.model_dir, 'pretrained.pth'))\n",
    "state_dict = trainer._from_old_model(state_dict)\n",
    "for key in list(state_dict):\n",
    "    if 'gp' in key or 'likelihood' in key:\n",
    "        state_dict.pop(key)\n",
    "trainer.model.load_state_dict(state_dict, strict=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:42:20.578447Z",
     "start_time": "2020-08-19T19:42:17.376356Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    import os\n",
    "    os.system('rm -rf runs/gp')\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter('runs/gp')\n",
    "import time\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:05:25.955554Z",
     "start_time": "2020-08-19T19:58:29.431703Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.08s/it, ave=-9.17, loss=-9.17, lr=0.000752]\n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.08s/it, ave=-9.17, loss=-9.17, lr=0.000481]\n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.08s/it, ave=-9.17, loss=-9.17, lr=0.000308]\n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.09s/it, ave=-9.18, loss=-9.17, lr=0.000197]\n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.09s/it, ave=-9.18, loss=-9.18, lr=0.000126]\n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.08s/it, ave=-9.18, loss=-9.18, lr=8.07e-5] \n",
      "100%|ââââââââââ| 51/51 [00:55<00:00,  1.09s/it, ave=-9.18, loss=-9.18, lr=5.17e-5]\n",
      " 51%|âââââ     | 26/51 [00:29<00:28,  1.12s/it, ave=-9.18, loss=-9.17, lr=4.13e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-699dc6238e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a441bb70f97d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0msingle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0msingle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.model.train()\n",
    "for e in range(args.num_epochs):\n",
    "    minibatch_iter = tqdm(dataloader)\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(minibatch_iter):\n",
    "        x,y = data\n",
    "\n",
    "        loss, batch_time = trainer.train(x,y)\n",
    "        \n",
    "        train_loss += loss\n",
    "        ave_loss = train_loss/(b+1)\n",
    "        step = e*len(dataloader)+(b+1)\n",
    "        # display data\n",
    "        minibatch_iter.set_postfix(ave = ave_loss, loss=loss,lr=trainer.scheduler.get_last_lr()[0])\n",
    "        # tensorboard\n",
    "        trainer.data2tensorboard(writer,'training loss',{'item loss':loss,'batch loss':ave_loss},step)\n",
    "        # save model\n",
    "        trainer.save_model_step(e,step)\n",
    "        # step scheduler\n",
    "        trainer.schedule_step(step,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:05:30.568680Z",
     "start_time": "2020-08-19T20:05:29.171221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to model-fast-zero-mean.pth\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('model-fast-zero-mean.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:05:42.215415Z",
     "start_time": "2020-08-19T20:05:41.767865Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "trans_prediction, rot_prediction, trans_target, rot_target, samples = trainer.eval_forward(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:05:42.916607Z",
     "start_time": "2020-08-19T20:05:42.893713Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  66.3477,  242.2132,   -2.5149],\n",
       "        [-282.7759,  441.0593,  -12.2314],\n",
       "        [  77.3453,  225.7924,   -2.1146],\n",
       "        ...,\n",
       "        [-303.7842,  503.8400,  -12.1227],\n",
       "        [  47.1907,  181.5075,   -2.0454],\n",
       "        [-195.7864,  666.3806,  -12.7685]], device='cuda:1',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:05:43.620307Z",
     "start_time": "2020-08-19T20:05:43.612704Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  66.2164,  242.2283,   -2.5141],\n",
       "        [-282.5126,  441.7702,  -12.2235],\n",
       "        [  76.8958,  225.4434,   -2.0860],\n",
       "        ...,\n",
       "        [-302.6446,  503.5599,  -12.1453],\n",
       "        [  47.0254,  181.1609,   -2.0274],\n",
       "        [-195.4930,  666.0799,  -12.7509]], device='cuda:1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T18:20:47.023493Z",
     "start_time": "2020-08-19T18:20:45.762473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to model-fast.pth\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('model-fast.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
