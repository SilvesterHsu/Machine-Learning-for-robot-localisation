{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:14:31.389474Z",
     "start_time": "2020-07-27T04:14:29.634497Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ List Devices ------------\n",
      "Device 0 :\n",
      "GeForce RTX 2060\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n",
      "Device 1 :\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "list_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:14:31.394736Z",
     "start_time": "2020-07-27T04:14:31.390876Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "set_device(1)\n",
    "import numpy as np\n",
    "np.set_printoptions(precision = 2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:14:31.487258Z",
     "start_time": "2020-07-27T04:14:31.396446Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=300, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.75, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=.01, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/gp_gps_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "#parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08'])\n",
    "'''\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=500, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "parser.add_argument('--tensorboard', type=bool, default=True, help='open tensorboard')\n",
    "parser.add_argument('--num_gp', type=int, default=20, help='number of local gps')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.tensorboard:\n",
    "    import os\n",
    "    os.system('rm -rf runs/gp_gps')\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter('runs/gp_gps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:40.952335Z",
     "start_time": "2020-07-27T04:14:31.488946Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16446/16446 [00:22<00:00, 731.29it/s]\n",
      "100%|██████████| 22584/22584 [00:31<00:00, 722.37it/s]\n",
      "100%|██████████| 18655/18655 [00:25<00:00, 729.79it/s]\n",
      "100%|██████████| 17310/17310 [00:23<00:00, 728.96it/s]\n",
      "100%|██████████| 10766/10766 [00:14<00:00, 729.56it/s]\n",
      "100%|██████████| 14878/14878 [00:21<00:00, 676.41it/s]\n",
      "100%|██████████| 13452/13452 [00:20<00:00, 664.38it/s]\n",
      "100%|██████████| 14037/14037 [00:28<00:00, 493.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import gpflow.multioutput.kernels as mk\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize_navie, denormalize, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform, get_pair = False)\n",
    "\n",
    "if len(args.train_dataset)>7:\n",
    "    [args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "    torch.save([args.norm_mean, args.norm_std], *args.norm_tensor)\n",
    "    print('Save norm and std:',*args.norm_tensor)\n",
    "else:\n",
    "    [args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "    print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:44.743669Z",
     "start_time": "2020-07-27T04:17:40.975301Z"
    },
    "code_folding": [
     9
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "\n",
    "X = np.array(dataset.Targets)[:,:2]\n",
    "for i in range(20):\n",
    "    np.random.seed(args.seed+i)\n",
    "    mbk = MiniBatchKMeans(n_clusters = args.num_gp)\n",
    "    mbk.fit(X)\n",
    "    dis = Counter(mbk.labels_)\n",
    "    var = np.var(np.array(list(dis.values())))/len(X)\n",
    "    if var<21:\n",
    "        break\n",
    "if i != 20:\n",
    "    plt.hist(mbk.labels_, bins = range(args.num_gp+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:45.126704Z",
     "start_time": "2020-07-27T04:17:44.745607Z"
    },
    "code_folding": [
     11
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1512x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "import random\n",
    "r = lambda: random.randint(0,255)\n",
    "colors = ['#%02X%02X%02X' % (r(),r(),r()) for _ in range(args.num_gp)]\n",
    "fig = plt.figure(figsize=(21, 8))\n",
    "#colors = ['#4EACC5', '#FF9C34', '#4E9A06']\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\n",
    "np.save(os.path.join(args.model_dir,'k-means.npy'),mbk_means_cluster_centers)\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "for k, col in zip(range(args.num_gp), colors):\n",
    "    my_members = mbk_means_labels == k\n",
    "    cluster_center = mbk_means_cluster_centers[k]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "            markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=6)\n",
    "ax.set_title('MiniBatchKMeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:45.144748Z",
     "start_time": "2020-07-27T04:17:45.128616Z"
    },
    "code_folding": [
     0,
     8,
     19,
     40,
     51,
     55
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([output_dim]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([output_dim])),\n",
    "            batch_shape=torch.Size([output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class GPNode(nn.Module):\n",
    "    def __init__(self,inducing_points):\n",
    "        super().__init__()\n",
    "        output_dim = inducing_points.shape[0]\n",
    "        feat_dim = inducing_points.shape[-1]\n",
    "        assert output_dim == args.output_dim\n",
    "        assert feat_dim == args.feat_dim\n",
    "        \n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=output_dim) \n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        output = self.gp(input_data)\n",
    "        return output\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gps = nn.ModuleList()\n",
    "        \n",
    "        self.num_gp = args.num_gp\n",
    "        \n",
    "        for i in range(self.num_gp):\n",
    "            inducing_points = torch.zeros(args.output_dim, args.batch_size, args.feat_dim)\n",
    "            gp = GPNode(inducing_points)\n",
    "            self.gps.append(gp)\n",
    "        \n",
    "    def forward_nn(self, input_data):\n",
    "        dense_feat = self.backbone(input_data)\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention            \n",
    "        return feature_t, rot_pred\n",
    "    \n",
    "    def forward_gp(self,gp,trans_feat):\n",
    "        trans_pred = gp(trans_feat)\n",
    "        return trans_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:52.622158Z",
     "start_time": "2020-07-27T04:17:45.148834Z"
    },
    "code_folding": [
     1,
     20,
     43,
     60,
     65,
     88,
     130,
     134,
     144,
     150
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform from old model.\n",
      "Backbone parameters layer: 318\n",
      "NN parameters layer: 28\n",
      "Model Structure:\n",
      "backbone.resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "backbone.resnet.bn1.weight torch.Size([64])\n",
      "backbone.resnet.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "backbone.resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "backbone.resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "backbone.resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "backbone.resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "backbone.resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "backbone.resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "backbone.resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "backbone.resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "backbone.resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "backbone.resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "backbone.resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "backbone.resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "backbone.resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "backbone.resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "backbone.resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "backbone.resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "backbone.resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "backbone.resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "backbone.resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "backbone.resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "backbone.resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "backbone.resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "backbone.resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "backbone.resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "backbone.resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "backbone.resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "backbone.resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "backbone.resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "backbone.resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "backbone.resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "backbone.resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "backbone.resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "backbone.resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "nn.global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "nn.global_context.context.squeeze.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_1.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_2.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_3.0.bias torch.Size([128])\n",
      "nn.global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "nn.global_context.context.context5_4.0.bias torch.Size([128])\n",
      "nn.global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "nn.global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "nn.global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "nn.global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "nn.global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "nn.global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "nn.global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "nn.global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "nn.global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "nn.global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "nn.global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "nn.global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "nn.global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "nn.global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "nn.global_regressor.regressor.logits_r.bias torch.Size([4])\n",
      "gps.0.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.0.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.0.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.0.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.0.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.0.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.0.likelihood.raw_noise torch.Size([1])\n",
      "gps.0.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.1.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.1.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.1.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.1.likelihood.raw_noise torch.Size([1])\n",
      "gps.1.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.2.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.2.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.2.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.2.likelihood.raw_noise torch.Size([1])\n",
      "gps.2.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.3.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.3.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.3.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.3.likelihood.raw_noise torch.Size([1])\n",
      "gps.3.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.4.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.4.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.4.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.4.likelihood.raw_noise torch.Size([1])\n",
      "gps.4.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.5.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.5.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.5.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.5.likelihood.raw_noise torch.Size([1])\n",
      "gps.5.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.6.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.6.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.6.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.6.likelihood.raw_noise torch.Size([1])\n",
      "gps.6.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.7.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.7.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.7.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.7.likelihood.raw_noise torch.Size([1])\n",
      "gps.7.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.8.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.8.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.8.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.8.likelihood.raw_noise torch.Size([1])\n",
      "gps.8.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.9.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.9.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.9.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.9.likelihood.raw_noise torch.Size([1])\n",
      "gps.9.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.10.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.10.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.10.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.10.likelihood.raw_noise torch.Size([1])\n",
      "gps.10.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.11.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.11.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.11.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.11.likelihood.raw_noise torch.Size([1])\n",
      "gps.11.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.12.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.12.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.12.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.12.likelihood.raw_noise torch.Size([1])\n",
      "gps.12.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.13.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.13.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.13.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.13.likelihood.raw_noise torch.Size([1])\n",
      "gps.13.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.14.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.14.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.14.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.14.likelihood.raw_noise torch.Size([1])\n",
      "gps.14.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.15.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.15.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.15.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.15.likelihood.raw_noise torch.Size([1])\n",
      "gps.15.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.16.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.16.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.16.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.16.likelihood.raw_noise torch.Size([1])\n",
      "gps.16.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.17.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.17.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.17.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.17.likelihood.raw_noise torch.Size([1])\n",
      "gps.17.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.18.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.18.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.18.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.18.likelihood.raw_noise torch.Size([1])\n",
      "gps.18.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.19.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.19.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.19.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.19.likelihood.raw_noise torch.Size([1])\n",
      "gps.19.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "Parameters layer: 546\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,is_training=True,regressor_context_rate = [0.0,0.0]):\n",
    "        self.model = Model().cuda()\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # disable learning backbone\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if is_training:\n",
    "            # training tool\n",
    "            self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                             lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        else:\n",
    "            # disable all learning\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth'):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, file_name))\n",
    "        if 'net.resnet.conv1.weight' in state_dict:\n",
    "            print('Transform from old model.')\n",
    "            # Part 1: backbone\n",
    "            backbone_state_dict = self._from_old_model(state_dict,'backbone')\n",
    "            print('Backbone parameters layer:',len(backbone_state_dict.keys()))\n",
    "            self.model.backbone.load_state_dict(backbone_state_dict,strict = True)\n",
    "            # Part 2: nn\n",
    "            nn_state_dict = self._from_old_model(torch.load(os.path.join(args.model_dir, file_name)),'nn')\n",
    "            print('NN parameters layer:',len(nn_state_dict.keys()))\n",
    "            self.model.nn.load_state_dict(nn_state_dict,strict = True)\n",
    "        else:\n",
    "            print('Parameters layer:',len(state_dict.keys()))\n",
    "            # load file to model\n",
    "            self.model.load_state_dict(state_dict,strict = True)\n",
    "        print('Model Structure:')\n",
    "        # Display model structure\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Parameters layer:',len(self.model.state_dict().keys()))\n",
    "    \n",
    "    def _from_old_model(self, state_dict, select = 'backbone'):\n",
    "        if select == 'backbone':\n",
    "            for key in list(state_dict):\n",
    "                if 'net.resnet.' in key:\n",
    "                    state_dict[key.replace('net.resnet.','resnet.')] = state_dict.pop(key)\n",
    "                else:\n",
    "                    state_dict.pop(key)\n",
    "        elif select == 'nn':\n",
    "            for key in list(state_dict):\n",
    "                if 'net.global_regressor.' in key:\n",
    "                    state_dict[key.replace('net.global_regressor.','global_regressor.')] = state_dict.pop(key)\n",
    "                elif 'net.global_context.' in key:\n",
    "                    state_dict[key.replace('net.global_context.','global_context.')] = state_dict.pop(key)\n",
    "                else:\n",
    "                    state_dict.pop(key)\n",
    "        return state_dict\n",
    "    \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "        \n",
    "    def _optimize(self,regressor_context_rate = [0.0,0.0]):\n",
    "        optimizer = [\n",
    "                {'params': self.model.gps.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay}]\n",
    "            \n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            for param in self.model.nn.global_regressor.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "        else:\n",
    "            for param in self.model.nn.global_context.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        return optimizer\n",
    "            \n",
    "    def train(self,x,y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        #labels = torch.from_numpy(pairwise_distances_argmin(y[:,:2].numpy(), mbk_means_cluster_centers)).cuda()\n",
    "        labels = pairwise_distances_argmin(y[:,:2].numpy(), mbk_means_cluster_centers)\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        \n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert self.model.training == True\n",
    "        \n",
    "        trans_loss = torch.tensor(0.).cuda()\n",
    "        \n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        rot_loss = self._nn_loss(rot_pred,rot_target)\n",
    "        \n",
    "        gps_loss = np.zeros(args.num_gp)\n",
    "        for i,gp in enumerate(self.model.gps):\n",
    "            num_data = dis[i]\n",
    "            label_mask = labels == i\n",
    "            sub_x = trans_feat[label_mask]\n",
    "            sub_y = trans_target[label_mask]\n",
    "            if sub_y.shape[0]>0:\n",
    "                gp_loss = self._gp_loss(gp,num_data,sub_x,sub_y)\n",
    "                gps_loss[i] = float(gp_loss)\n",
    "                trans_loss += gp_loss\n",
    "        trans_loss = trans_loss/self.model.num_gp\n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss\n",
    "        \n",
    "        #Step 3: update\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - start\n",
    "        return float(total_loss), batch_time, gps_loss\n",
    "    \n",
    "    def _nn_loss(self,rot_pred,rot_target):\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        return rot_loss\n",
    "        \n",
    "    def _gp_loss(self,gp,num_data,trans_feat,trans_target):\n",
    "        # predict\n",
    "        trans_pred = self.model.forward_gp(gp,trans_feat)\n",
    "        mll = gpytorch.mlls.PredictiveLogLikelihood(gp.likelihood, gp.gp, num_data = num_data)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*mll(trans_pred, trans_target)\n",
    "        \n",
    "        return trans_loss\n",
    "    \n",
    "    def _eval_gp(self, gp, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = gp.likelihood(trans_pred).mean, gp.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "\n",
    "    def eval_forward(self,x,y,num_sample = 100,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        labels = pairwise_distances_argmin(y[:,:2].numpy(), mbk_means_cluster_centers)\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert self.model.training == False\n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        \n",
    "        trans_pred = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "        trans_mean = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "        trans_var = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "\n",
    "        for i,gp in enumerate(trainer.model.gps):\n",
    "            label_mask = labels == i\n",
    "            sub_x = trans_feat[label_mask]\n",
    "            if sub_x.shape[0]>0:\n",
    "                sub_trans_pred = self.model.forward_gp(gp,sub_x)\n",
    "                sub_trans_pred, sub_trans_mean, sub_trans_var = self._eval_gp(gp, sub_trans_pred)\n",
    "                trans_pred[label_mask] = sub_trans_pred\n",
    "                trans_mean[label_mask] = sub_trans_mean\n",
    "                trans_var[label_mask] = sub_trans_var\n",
    "        \n",
    "        if args.is_normalization and output_denormalize:\n",
    "            trans_pred = denormalize_navie(trans_pred, self.norm_mean, self.norm_std)\n",
    "            trans_mean = denormalize_navie(trans_mean, self.norm_mean, self.norm_std)\n",
    "            trans_var = trans_var.mul(self.norm_std)\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_mean, trans_var, num_sample)\n",
    "        \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_pred, rot_pred, trans_target, rot_target, samples\n",
    "\n",
    "trainer = Trainer(regressor_context_rate = [0.,0.])\n",
    "#trainer = Trainer(regressor_context_rate = [0.1,0.01])\n",
    "\n",
    "trainer.load_model('pretrained_old.pth')\n",
    "#trainer.load_model('pretrained_gp30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T04:17:52.654172Z",
     "start_time": "2020-07-27T04:17:52.625322Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gps.0.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.0.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.0.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.0.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.0.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.0.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.0.likelihood.raw_noise torch.Size([1])\n",
      "gps.0.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.1.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.1.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.1.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.1.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.1.likelihood.raw_noise torch.Size([1])\n",
      "gps.1.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.2.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.2.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.2.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.2.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.2.likelihood.raw_noise torch.Size([1])\n",
      "gps.2.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.3.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.3.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.3.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.3.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.3.likelihood.raw_noise torch.Size([1])\n",
      "gps.3.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.4.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.4.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.4.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.4.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.4.likelihood.raw_noise torch.Size([1])\n",
      "gps.4.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.5.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.5.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.5.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.5.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.5.likelihood.raw_noise torch.Size([1])\n",
      "gps.5.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.6.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.6.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.6.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.6.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.6.likelihood.raw_noise torch.Size([1])\n",
      "gps.6.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.7.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.7.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.7.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.7.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.7.likelihood.raw_noise torch.Size([1])\n",
      "gps.7.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.8.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.8.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.8.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.8.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.8.likelihood.raw_noise torch.Size([1])\n",
      "gps.8.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.9.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.9.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.9.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.9.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.9.likelihood.raw_noise torch.Size([1])\n",
      "gps.9.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.10.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.10.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.10.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.10.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.10.likelihood.raw_noise torch.Size([1])\n",
      "gps.10.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.11.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.11.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.11.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.11.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.11.likelihood.raw_noise torch.Size([1])\n",
      "gps.11.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.12.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.12.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.12.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.12.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.12.likelihood.raw_noise torch.Size([1])\n",
      "gps.12.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.13.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.13.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.13.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.13.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.13.likelihood.raw_noise torch.Size([1])\n",
      "gps.13.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.14.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.14.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.14.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.14.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.14.likelihood.raw_noise torch.Size([1])\n",
      "gps.14.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.15.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.15.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.15.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.15.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.15.likelihood.raw_noise torch.Size([1])\n",
      "gps.15.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.16.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.16.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.16.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.16.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.16.likelihood.raw_noise torch.Size([1])\n",
      "gps.16.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.17.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.17.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.17.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.17.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.17.likelihood.raw_noise torch.Size([1])\n",
      "gps.17.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.18.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.18.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.18.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.18.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.18.likelihood.raw_noise torch.Size([1])\n",
      "gps.18.likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gps.19.gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gps.19.gp.mean_module.constant torch.Size([3, 1])\n",
      "gps.19.gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gps.19.gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "gps.19.likelihood.raw_noise torch.Size([1])\n",
      "gps.19.likelihood.noise_covar.raw_noise torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:10:22.234959Z",
     "start_time": "2020-07-27T04:17:52.655422Z"
    },
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/85400 (epoch 0), train_loss = 3.60290873, time/batch = 1.292, learning rate = 0.10000000\n",
      "20/85400 (epoch 0), train_loss = 2.74672922, time/batch = 1.296, learning rate = 0.10000000\n",
      "30/85400 (epoch 0), train_loss = 1.92104980, time/batch = 1.295, learning rate = 0.10000000\n",
      "40/85400 (epoch 0), train_loss = 1.20780340, time/batch = 1.304, learning rate = 0.10000000\n",
      "50/85400 (epoch 0), train_loss = 0.64259346, time/batch = 1.304, learning rate = 0.10000000\n",
      "60/85400 (epoch 0), train_loss = 0.23849093, time/batch = 1.302, learning rate = 0.10000000\n",
      "70/85400 (epoch 0), train_loss = -0.03224901, time/batch = 1.313, learning rate = 0.10000000\n",
      "80/85400 (epoch 0), train_loss = -0.24615546, time/batch = 1.306, learning rate = 0.10000000\n",
      "90/85400 (epoch 0), train_loss = -0.42737917, time/batch = 1.310, learning rate = 0.10000000\n",
      "100/85400 (epoch 0), train_loss = -0.58110338, time/batch = 1.304, learning rate = 0.10000000\n",
      "110/85400 (epoch 0), train_loss = -0.70538337, time/batch = 1.302, learning rate = 0.10000000\n",
      "120/85400 (epoch 0), train_loss = -0.81734264, time/batch = 1.305, learning rate = 0.10000000\n",
      "130/85400 (epoch 0), train_loss = -0.91622599, time/batch = 1.306, learning rate = 0.10000000\n",
      "140/85400 (epoch 0), train_loss = -0.99225459, time/batch = 1.308, learning rate = 0.10000000\n",
      "150/85400 (epoch 0), train_loss = -1.05092409, time/batch = 1.321, learning rate = 0.10000000\n",
      "160/85400 (epoch 0), train_loss = -1.11000476, time/batch = 1.308, learning rate = 0.10000000\n",
      "170/85400 (epoch 0), train_loss = -1.17553491, time/batch = 1.312, learning rate = 0.10000000\n",
      "180/85400 (epoch 0), train_loss = -1.22507663, time/batch = 1.325, learning rate = 0.10000000\n",
      "190/85400 (epoch 0), train_loss = -1.27449038, time/batch = 1.307, learning rate = 0.10000000\n",
      "200/85400 (epoch 0), train_loss = -1.31358467, time/batch = 1.309, learning rate = 0.10000000\n",
      "210/85400 (epoch 0), train_loss = -1.35672722, time/batch = 1.315, learning rate = 0.07500000\n",
      "220/85400 (epoch 0), train_loss = -1.40315022, time/batch = 1.313, learning rate = 0.07500000\n",
      "230/85400 (epoch 0), train_loss = -1.44920545, time/batch = 1.305, learning rate = 0.07500000\n",
      "240/85400 (epoch 0), train_loss = -1.49861790, time/batch = 1.306, learning rate = 0.07500000\n",
      "250/85400 (epoch 0), train_loss = -1.53917677, time/batch = 1.317, learning rate = 0.07500000\n",
      "260/85400 (epoch 0), train_loss = -1.57916726, time/batch = 1.323, learning rate = 0.07500000\n",
      "270/85400 (epoch 0), train_loss = -1.61250454, time/batch = 1.301, learning rate = 0.07500000\n",
      "280/85400 (epoch 0), train_loss = -1.64349069, time/batch = 1.305, learning rate = 0.07500000\n",
      "290/85400 (epoch 0), train_loss = -1.67118459, time/batch = 1.313, learning rate = 0.07500000\n",
      "300/85400 (epoch 0), train_loss = -1.69607405, time/batch = 1.307, learning rate = 0.07500000\n",
      "310/85400 (epoch 0), train_loss = -1.72576593, time/batch = 1.303, learning rate = 0.07500000\n",
      "320/85400 (epoch 0), train_loss = -1.75803260, time/batch = 1.322, learning rate = 0.07500000\n",
      "330/85400 (epoch 0), train_loss = -1.78442648, time/batch = 1.329, learning rate = 0.07500000\n",
      "340/85400 (epoch 0), train_loss = -1.80913648, time/batch = 1.323, learning rate = 0.07500000\n",
      "350/85400 (epoch 0), train_loss = -1.83004956, time/batch = 1.304, learning rate = 0.07500000\n",
      "360/85400 (epoch 0), train_loss = -1.84590963, time/batch = 1.307, learning rate = 0.07500000\n",
      "370/85400 (epoch 0), train_loss = -1.86803553, time/batch = 1.319, learning rate = 0.07500000\n",
      "380/85400 (epoch 0), train_loss = -1.88424364, time/batch = 1.323, learning rate = 0.07500000\n",
      "390/85400 (epoch 0), train_loss = -1.90138242, time/batch = 1.317, learning rate = 0.07500000\n",
      "400/85400 (epoch 0), train_loss = -1.91913277, time/batch = 1.319, learning rate = 0.07500000\n",
      "410/85400 (epoch 0), train_loss = -1.94297529, time/batch = 1.324, learning rate = 0.05625000\n",
      "420/85400 (epoch 0), train_loss = -1.97304580, time/batch = 1.311, learning rate = 0.05625000\n",
      "437/85400 (epoch 1), train_loss = -3.20657029, time/batch = 1.316, learning rate = 0.05625000\n",
      "447/85400 (epoch 1), train_loss = -3.19215078, time/batch = 1.324, learning rate = 0.05625000\n",
      "457/85400 (epoch 1), train_loss = -3.20521413, time/batch = 1.314, learning rate = 0.05625000\n",
      "467/85400 (epoch 1), train_loss = -3.19277101, time/batch = 1.324, learning rate = 0.05625000\n",
      "477/85400 (epoch 1), train_loss = -3.19219109, time/batch = 1.309, learning rate = 0.05625000\n",
      "487/85400 (epoch 1), train_loss = -3.20474413, time/batch = 1.320, learning rate = 0.05625000\n",
      "497/85400 (epoch 1), train_loss = -3.20409125, time/batch = 1.316, learning rate = 0.05625000\n",
      "Saving model to model-1-500.pth\n",
      "507/85400 (epoch 1), train_loss = -3.19770092, time/batch = 1.336, learning rate = 0.05625000\n",
      "517/85400 (epoch 1), train_loss = -3.18490788, time/batch = 1.326, learning rate = 0.05625000\n",
      "527/85400 (epoch 1), train_loss = -3.18937672, time/batch = 1.323, learning rate = 0.05625000\n",
      "537/85400 (epoch 1), train_loss = -3.21260723, time/batch = 1.309, learning rate = 0.05625000\n",
      "547/85400 (epoch 1), train_loss = -3.22377767, time/batch = 1.325, learning rate = 0.05625000\n",
      "557/85400 (epoch 1), train_loss = -3.21478450, time/batch = 1.315, learning rate = 0.05625000\n",
      "567/85400 (epoch 1), train_loss = -3.21192229, time/batch = 1.307, learning rate = 0.05625000\n",
      "577/85400 (epoch 1), train_loss = -3.20926058, time/batch = 1.329, learning rate = 0.05625000\n",
      "587/85400 (epoch 1), train_loss = -3.20227124, time/batch = 1.309, learning rate = 0.05625000\n",
      "597/85400 (epoch 1), train_loss = -3.20483188, time/batch = 1.319, learning rate = 0.05625000\n",
      "607/85400 (epoch 1), train_loss = -3.21199200, time/batch = 1.312, learning rate = 0.04218750\n",
      "617/85400 (epoch 1), train_loss = -3.23657289, time/batch = 1.328, learning rate = 0.04218750\n",
      "627/85400 (epoch 1), train_loss = -3.26655161, time/batch = 1.315, learning rate = 0.04218750\n",
      "637/85400 (epoch 1), train_loss = -3.29659440, time/batch = 1.318, learning rate = 0.04218750\n",
      "647/85400 (epoch 1), train_loss = -3.32544596, time/batch = 1.320, learning rate = 0.04218750\n",
      "657/85400 (epoch 1), train_loss = -3.35426951, time/batch = 1.321, learning rate = 0.04218750\n",
      "667/85400 (epoch 1), train_loss = -3.38859471, time/batch = 1.315, learning rate = 0.04218750\n",
      "677/85400 (epoch 1), train_loss = -3.42090926, time/batch = 1.309, learning rate = 0.04218750\n",
      "687/85400 (epoch 1), train_loss = -3.44871159, time/batch = 1.332, learning rate = 0.04218750\n",
      "697/85400 (epoch 1), train_loss = -3.47223179, time/batch = 1.327, learning rate = 0.04218750\n",
      "707/85400 (epoch 1), train_loss = -3.48679421, time/batch = 1.305, learning rate = 0.04218750\n",
      "717/85400 (epoch 1), train_loss = -3.50165815, time/batch = 1.321, learning rate = 0.04218750\n",
      "727/85400 (epoch 1), train_loss = -3.51694582, time/batch = 1.332, learning rate = 0.04218750\n",
      "737/85400 (epoch 1), train_loss = -3.53341318, time/batch = 1.316, learning rate = 0.04218750\n",
      "747/85400 (epoch 1), train_loss = -3.54839136, time/batch = 1.319, learning rate = 0.04218750\n",
      "757/85400 (epoch 1), train_loss = -3.56578133, time/batch = 1.311, learning rate = 0.04218750\n",
      "767/85400 (epoch 1), train_loss = -3.57973393, time/batch = 1.320, learning rate = 0.04218750\n",
      "777/85400 (epoch 1), train_loss = -3.59855147, time/batch = 1.319, learning rate = 0.04218750\n",
      "787/85400 (epoch 1), train_loss = -3.61648845, time/batch = 1.321, learning rate = 0.04218750\n",
      "797/85400 (epoch 1), train_loss = -3.63022908, time/batch = 1.319, learning rate = 0.04218750\n",
      "807/85400 (epoch 1), train_loss = -3.64617860, time/batch = 1.324, learning rate = 0.03164062\n",
      "817/85400 (epoch 1), train_loss = -3.67330871, time/batch = 1.317, learning rate = 0.03164062\n",
      "827/85400 (epoch 1), train_loss = -3.70064898, time/batch = 1.324, learning rate = 0.03164062\n",
      "837/85400 (epoch 1), train_loss = -3.72533962, time/batch = 1.322, learning rate = 0.03164062\n",
      "847/85400 (epoch 1), train_loss = -3.74936010, time/batch = 1.326, learning rate = 0.03164062\n",
      "864/85400 (epoch 2), train_loss = -4.76698098, time/batch = 1.327, learning rate = 0.03164062\n",
      "874/85400 (epoch 2), train_loss = -4.81436849, time/batch = 1.314, learning rate = 0.03164062\n",
      "884/85400 (epoch 2), train_loss = -4.84077039, time/batch = 1.332, learning rate = 0.03164062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894/85400 (epoch 2), train_loss = -4.86956195, time/batch = 1.323, learning rate = 0.03164062\n",
      "904/85400 (epoch 2), train_loss = -4.89494643, time/batch = 1.322, learning rate = 0.03164062\n",
      "914/85400 (epoch 2), train_loss = -4.90810072, time/batch = 1.321, learning rate = 0.03164062\n",
      "924/85400 (epoch 2), train_loss = -4.91878445, time/batch = 1.313, learning rate = 0.03164062\n",
      "934/85400 (epoch 2), train_loss = -4.93406439, time/batch = 1.326, learning rate = 0.03164062\n",
      "944/85400 (epoch 2), train_loss = -4.96477698, time/batch = 1.323, learning rate = 0.03164062\n",
      "954/85400 (epoch 2), train_loss = -4.96751006, time/batch = 1.325, learning rate = 0.03164062\n",
      "964/85400 (epoch 2), train_loss = -4.97445404, time/batch = 1.325, learning rate = 0.03164062\n",
      "974/85400 (epoch 2), train_loss = -4.98904994, time/batch = 1.329, learning rate = 0.03164062\n",
      "984/85400 (epoch 2), train_loss = -5.00102344, time/batch = 1.320, learning rate = 0.03164062\n",
      "994/85400 (epoch 2), train_loss = -5.00962415, time/batch = 1.317, learning rate = 0.03164062\n",
      "Saving model to model-2-1000.pth\n",
      "1004/85400 (epoch 2), train_loss = -5.02630818, time/batch = 1.323, learning rate = 0.02373047\n",
      "1014/85400 (epoch 2), train_loss = -5.07557769, time/batch = 1.326, learning rate = 0.02373047\n",
      "1024/85400 (epoch 2), train_loss = -5.13231110, time/batch = 1.332, learning rate = 0.02373047\n",
      "1034/85400 (epoch 2), train_loss = -5.17641764, time/batch = 1.313, learning rate = 0.02373047\n",
      "1044/85400 (epoch 2), train_loss = -5.21732669, time/batch = 1.330, learning rate = 0.02373047\n",
      "1054/85400 (epoch 2), train_loss = -5.25170607, time/batch = 1.317, learning rate = 0.02373047\n",
      "1064/85400 (epoch 2), train_loss = -5.28669990, time/batch = 1.324, learning rate = 0.02373047\n",
      "1074/85400 (epoch 2), train_loss = -5.32500433, time/batch = 1.317, learning rate = 0.02373047\n",
      "1084/85400 (epoch 2), train_loss = -5.36444081, time/batch = 1.320, learning rate = 0.02373047\n",
      "1094/85400 (epoch 2), train_loss = -5.40380938, time/batch = 1.318, learning rate = 0.02373047\n",
      "1104/85400 (epoch 2), train_loss = -5.43447825, time/batch = 1.324, learning rate = 0.02373047\n",
      "1114/85400 (epoch 2), train_loss = -5.46474901, time/batch = 1.321, learning rate = 0.02373047\n",
      "1124/85400 (epoch 2), train_loss = -5.49399101, time/batch = 1.329, learning rate = 0.02373047\n",
      "1134/85400 (epoch 2), train_loss = -5.52071963, time/batch = 1.318, learning rate = 0.02373047\n",
      "1144/85400 (epoch 2), train_loss = -5.54568124, time/batch = 1.317, learning rate = 0.02373047\n",
      "1154/85400 (epoch 2), train_loss = -5.56715156, time/batch = 1.327, learning rate = 0.02373047\n",
      "1164/85400 (epoch 2), train_loss = -5.58784065, time/batch = 1.324, learning rate = 0.02373047\n",
      "1174/85400 (epoch 2), train_loss = -5.61064480, time/batch = 1.320, learning rate = 0.02373047\n",
      "1184/85400 (epoch 2), train_loss = -5.63769873, time/batch = 1.320, learning rate = 0.02373047\n",
      "1194/85400 (epoch 2), train_loss = -5.65725341, time/batch = 1.322, learning rate = 0.02373047\n",
      "1204/85400 (epoch 2), train_loss = -5.67870750, time/batch = 1.321, learning rate = 0.01779785\n",
      "1214/85400 (epoch 2), train_loss = -5.70713019, time/batch = 1.317, learning rate = 0.01779785\n",
      "1224/85400 (epoch 2), train_loss = -5.73533402, time/batch = 1.315, learning rate = 0.01779785\n",
      "1234/85400 (epoch 2), train_loss = -5.76520192, time/batch = 1.316, learning rate = 0.01779785\n",
      "1244/85400 (epoch 2), train_loss = -5.79680090, time/batch = 1.330, learning rate = 0.01779785\n",
      "1254/85400 (epoch 2), train_loss = -5.82808632, time/batch = 1.319, learning rate = 0.01779785\n",
      "1264/85400 (epoch 2), train_loss = -5.85979457, time/batch = 1.316, learning rate = 0.01779785\n",
      "1274/85400 (epoch 2), train_loss = -5.88798008, time/batch = 1.313, learning rate = 0.01779785\n",
      "1291/85400 (epoch 3), train_loss = -7.08816609, time/batch = 1.325, learning rate = 0.01779785\n",
      "1301/85400 (epoch 3), train_loss = -7.06546817, time/batch = 1.334, learning rate = 0.01779785\n",
      "1311/85400 (epoch 3), train_loss = -7.08400323, time/batch = 1.338, learning rate = 0.01779785\n",
      "1321/85400 (epoch 3), train_loss = -7.10728251, time/batch = 1.315, learning rate = 0.01779785\n",
      "1331/85400 (epoch 3), train_loss = -7.12830580, time/batch = 1.322, learning rate = 0.01779785\n",
      "1341/85400 (epoch 3), train_loss = -7.10615745, time/batch = 1.329, learning rate = 0.01779785\n",
      "1351/85400 (epoch 3), train_loss = -7.09689117, time/batch = 1.328, learning rate = 0.01779785\n",
      "1361/85400 (epoch 3), train_loss = -7.09984149, time/batch = 1.319, learning rate = 0.01779785\n",
      "1371/85400 (epoch 3), train_loss = -7.11922163, time/batch = 1.330, learning rate = 0.01779785\n",
      "1381/85400 (epoch 3), train_loss = -7.12974794, time/batch = 1.323, learning rate = 0.01779785\n",
      "1391/85400 (epoch 3), train_loss = -7.13459591, time/batch = 1.320, learning rate = 0.01779785\n",
      "1401/85400 (epoch 3), train_loss = -7.13732568, time/batch = 1.314, learning rate = 0.01334839\n",
      "1411/85400 (epoch 3), train_loss = -7.15888962, time/batch = 1.318, learning rate = 0.01334839\n",
      "1421/85400 (epoch 3), train_loss = -7.18585144, time/batch = 1.315, learning rate = 0.01334839\n",
      "1431/85400 (epoch 3), train_loss = -7.21296727, time/batch = 1.322, learning rate = 0.01334839\n",
      "1441/85400 (epoch 3), train_loss = -7.23908127, time/batch = 1.312, learning rate = 0.01334839\n",
      "1451/85400 (epoch 3), train_loss = -7.26211071, time/batch = 1.325, learning rate = 0.01334839\n",
      "1461/85400 (epoch 3), train_loss = -7.28340863, time/batch = 1.324, learning rate = 0.01334839\n",
      "1471/85400 (epoch 3), train_loss = -7.30198268, time/batch = 1.333, learning rate = 0.01334839\n",
      "1481/85400 (epoch 3), train_loss = -7.32014385, time/batch = 1.323, learning rate = 0.01334839\n",
      "1491/85400 (epoch 3), train_loss = -7.33444736, time/batch = 1.327, learning rate = 0.01334839\n",
      "Saving model to model-3-1500.pth\n",
      "1501/85400 (epoch 3), train_loss = -7.34878301, time/batch = 1.306, learning rate = 0.01334839\n",
      "1511/85400 (epoch 3), train_loss = -7.36001625, time/batch = 1.316, learning rate = 0.01334839\n",
      "1521/85400 (epoch 3), train_loss = -7.36998631, time/batch = 1.323, learning rate = 0.01334839\n",
      "1531/85400 (epoch 3), train_loss = -7.38033824, time/batch = 1.326, learning rate = 0.01334839\n",
      "1541/85400 (epoch 3), train_loss = -7.38975633, time/batch = 1.318, learning rate = 0.01334839\n",
      "1551/85400 (epoch 3), train_loss = -7.40100270, time/batch = 1.317, learning rate = 0.01334839\n",
      "1561/85400 (epoch 3), train_loss = -7.41263658, time/batch = 1.312, learning rate = 0.01334839\n",
      "1571/85400 (epoch 3), train_loss = -7.42456080, time/batch = 1.312, learning rate = 0.01334839\n",
      "1581/85400 (epoch 3), train_loss = -7.43569884, time/batch = 1.320, learning rate = 0.01334839\n",
      "1591/85400 (epoch 3), train_loss = -7.44513035, time/batch = 1.319, learning rate = 0.01334839\n",
      "1601/85400 (epoch 3), train_loss = -7.45545087, time/batch = 1.323, learning rate = 0.01001129\n",
      "1611/85400 (epoch 3), train_loss = -7.46942976, time/batch = 1.316, learning rate = 0.01001129\n",
      "1621/85400 (epoch 3), train_loss = -7.48331186, time/batch = 1.330, learning rate = 0.01001129\n",
      "1631/85400 (epoch 3), train_loss = -7.49668043, time/batch = 1.314, learning rate = 0.01001129\n",
      "1641/85400 (epoch 3), train_loss = -7.51034343, time/batch = 1.322, learning rate = 0.01001129\n",
      "1651/85400 (epoch 3), train_loss = -7.52210829, time/batch = 1.319, learning rate = 0.01001129\n",
      "1661/85400 (epoch 3), train_loss = -7.53493958, time/batch = 1.329, learning rate = 0.01001129\n",
      "1671/85400 (epoch 3), train_loss = -7.54733335, time/batch = 1.322, learning rate = 0.01001129\n",
      "1681/85400 (epoch 3), train_loss = -7.55551764, time/batch = 1.329, learning rate = 0.01001129\n",
      "1691/85400 (epoch 3), train_loss = -7.56677291, time/batch = 1.323, learning rate = 0.01001129\n",
      "1701/85400 (epoch 3), train_loss = -7.57754275, time/batch = 1.326, learning rate = 0.01001129\n",
      "1718/85400 (epoch 4), train_loss = -8.05623236, time/batch = 1.318, learning rate = 0.01001129\n",
      "1728/85400 (epoch 4), train_loss = -8.04301777, time/batch = 1.324, learning rate = 0.01001129\n",
      "1738/85400 (epoch 4), train_loss = -8.05491095, time/batch = 1.314, learning rate = 0.01001129\n",
      "1748/85400 (epoch 4), train_loss = -8.04564084, time/batch = 1.329, learning rate = 0.01001129\n",
      "1758/85400 (epoch 4), train_loss = -8.03609578, time/batch = 1.327, learning rate = 0.01001129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1768/85400 (epoch 4), train_loss = -8.04281803, time/batch = 1.316, learning rate = 0.01001129\n",
      "1778/85400 (epoch 4), train_loss = -8.04538845, time/batch = 1.325, learning rate = 0.01001129\n",
      "1788/85400 (epoch 4), train_loss = -8.05592225, time/batch = 1.327, learning rate = 0.01001129\n",
      "1798/85400 (epoch 4), train_loss = -8.06393593, time/batch = 1.317, learning rate = 0.01001129\n",
      "1808/85400 (epoch 4), train_loss = -8.06528467, time/batch = 1.324, learning rate = 0.00750847\n",
      "1818/85400 (epoch 4), train_loss = -8.07119630, time/batch = 1.329, learning rate = 0.00750847\n",
      "1828/85400 (epoch 4), train_loss = -8.08150137, time/batch = 1.321, learning rate = 0.00750847\n",
      "1838/85400 (epoch 4), train_loss = -8.09038810, time/batch = 1.314, learning rate = 0.00750847\n",
      "1848/85400 (epoch 4), train_loss = -8.09579264, time/batch = 1.332, learning rate = 0.00750847\n",
      "1858/85400 (epoch 4), train_loss = -8.09614812, time/batch = 1.328, learning rate = 0.00750847\n",
      "1868/85400 (epoch 4), train_loss = -8.10090401, time/batch = 1.310, learning rate = 0.00750847\n",
      "1878/85400 (epoch 4), train_loss = -8.10627986, time/batch = 1.328, learning rate = 0.00750847\n",
      "1888/85400 (epoch 4), train_loss = -8.11031771, time/batch = 1.322, learning rate = 0.00750847\n",
      "1898/85400 (epoch 4), train_loss = -8.11525733, time/batch = 1.312, learning rate = 0.00750847\n",
      "1908/85400 (epoch 4), train_loss = -8.11826019, time/batch = 1.316, learning rate = 0.00750847\n",
      "1918/85400 (epoch 4), train_loss = -8.12507475, time/batch = 1.329, learning rate = 0.00750847\n",
      "1928/85400 (epoch 4), train_loss = -8.13319972, time/batch = 1.311, learning rate = 0.00750847\n",
      "1938/85400 (epoch 4), train_loss = -8.13960941, time/batch = 1.321, learning rate = 0.00750847\n",
      "1948/85400 (epoch 4), train_loss = -8.14570642, time/batch = 1.322, learning rate = 0.00750847\n",
      "1958/85400 (epoch 4), train_loss = -8.15050603, time/batch = 1.321, learning rate = 0.00750847\n",
      "1968/85400 (epoch 4), train_loss = -8.15520112, time/batch = 1.317, learning rate = 0.00750847\n",
      "1978/85400 (epoch 4), train_loss = -8.15383441, time/batch = 1.323, learning rate = 0.00750847\n",
      "1988/85400 (epoch 4), train_loss = -8.15508938, time/batch = 1.331, learning rate = 0.00750847\n",
      "1998/85400 (epoch 4), train_loss = -8.15812764, time/batch = 1.319, learning rate = 0.00750847\n",
      "Saving model to model-4-2000.pth\n",
      "2008/85400 (epoch 4), train_loss = -8.16310139, time/batch = 1.324, learning rate = 0.00563135\n",
      "2018/85400 (epoch 4), train_loss = -8.16829070, time/batch = 1.321, learning rate = 0.00563135\n",
      "2028/85400 (epoch 4), train_loss = -8.17392049, time/batch = 1.330, learning rate = 0.00563135\n",
      "2038/85400 (epoch 4), train_loss = -8.17882115, time/batch = 1.319, learning rate = 0.00563135\n",
      "2048/85400 (epoch 4), train_loss = -8.18233203, time/batch = 1.322, learning rate = 0.00563135\n",
      "2058/85400 (epoch 4), train_loss = -8.18692212, time/batch = 1.330, learning rate = 0.00563135\n",
      "2068/85400 (epoch 4), train_loss = -8.18988555, time/batch = 1.317, learning rate = 0.00563135\n",
      "2078/85400 (epoch 4), train_loss = -8.19340082, time/batch = 1.327, learning rate = 0.00563135\n",
      "2088/85400 (epoch 4), train_loss = -8.19655014, time/batch = 1.320, learning rate = 0.00563135\n",
      "2098/85400 (epoch 4), train_loss = -8.19970253, time/batch = 1.311, learning rate = 0.00563135\n",
      "2108/85400 (epoch 4), train_loss = -8.20382121, time/batch = 1.317, learning rate = 0.00563135\n",
      "2118/85400 (epoch 4), train_loss = -8.20800416, time/batch = 1.311, learning rate = 0.00563135\n",
      "2128/85400 (epoch 4), train_loss = -8.21200942, time/batch = 1.317, learning rate = 0.00563135\n",
      "2145/85400 (epoch 5), train_loss = -8.33674498, time/batch = 1.323, learning rate = 0.00563135\n",
      "2155/85400 (epoch 5), train_loss = -8.34204545, time/batch = 1.324, learning rate = 0.00563135\n",
      "2165/85400 (epoch 5), train_loss = -8.34367202, time/batch = 1.322, learning rate = 0.00563135\n",
      "2175/85400 (epoch 5), train_loss = -8.34152706, time/batch = 1.322, learning rate = 0.00563135\n",
      "2185/85400 (epoch 5), train_loss = -8.34231222, time/batch = 1.326, learning rate = 0.00563135\n",
      "2195/85400 (epoch 5), train_loss = -8.35004317, time/batch = 1.320, learning rate = 0.00563135\n",
      "2205/85400 (epoch 5), train_loss = -8.35739083, time/batch = 1.313, learning rate = 0.00422351\n",
      "2215/85400 (epoch 5), train_loss = -8.36537465, time/batch = 1.322, learning rate = 0.00422351\n",
      "2225/85400 (epoch 5), train_loss = -8.36398819, time/batch = 1.342, learning rate = 0.00422351\n",
      "2235/85400 (epoch 5), train_loss = -8.36607429, time/batch = 1.325, learning rate = 0.00422351\n",
      "2245/85400 (epoch 5), train_loss = -8.36728146, time/batch = 1.325, learning rate = 0.00422351\n",
      "2255/85400 (epoch 5), train_loss = -8.37101560, time/batch = 1.323, learning rate = 0.00422351\n",
      "2265/85400 (epoch 5), train_loss = -8.37339428, time/batch = 1.313, learning rate = 0.00422351\n",
      "2275/85400 (epoch 5), train_loss = -8.37646387, time/batch = 1.322, learning rate = 0.00422351\n",
      "2285/85400 (epoch 5), train_loss = -8.38003327, time/batch = 1.327, learning rate = 0.00422351\n",
      "2295/85400 (epoch 5), train_loss = -8.38261869, time/batch = 1.323, learning rate = 0.00422351\n",
      "2305/85400 (epoch 5), train_loss = -8.38503160, time/batch = 1.323, learning rate = 0.00422351\n",
      "2315/85400 (epoch 5), train_loss = -8.38830508, time/batch = 1.324, learning rate = 0.00422351\n",
      "2325/85400 (epoch 5), train_loss = -8.39149497, time/batch = 1.322, learning rate = 0.00422351\n",
      "2335/85400 (epoch 5), train_loss = -8.39290774, time/batch = 1.324, learning rate = 0.00422351\n",
      "2345/85400 (epoch 5), train_loss = -8.39227645, time/batch = 1.314, learning rate = 0.00422351\n",
      "2355/85400 (epoch 5), train_loss = -8.39260758, time/batch = 1.326, learning rate = 0.00422351\n",
      "2365/85400 (epoch 5), train_loss = -8.39387319, time/batch = 1.319, learning rate = 0.00422351\n",
      "2375/85400 (epoch 5), train_loss = -8.39308485, time/batch = 1.316, learning rate = 0.00422351\n",
      "2385/85400 (epoch 5), train_loss = -8.39284365, time/batch = 1.308, learning rate = 0.00422351\n",
      "2395/85400 (epoch 5), train_loss = -8.39340906, time/batch = 1.317, learning rate = 0.00422351\n",
      "2405/85400 (epoch 5), train_loss = -8.39481378, time/batch = 1.319, learning rate = 0.00316764\n",
      "2415/85400 (epoch 5), train_loss = -8.39691270, time/batch = 1.315, learning rate = 0.00316764\n",
      "2425/85400 (epoch 5), train_loss = -8.40017222, time/batch = 1.325, learning rate = 0.00316764\n",
      "2435/85400 (epoch 5), train_loss = -8.40166924, time/batch = 1.319, learning rate = 0.00316764\n",
      "2445/85400 (epoch 5), train_loss = -8.40278653, time/batch = 1.322, learning rate = 0.00316764\n",
      "2455/85400 (epoch 5), train_loss = -8.40489995, time/batch = 1.333, learning rate = 0.00316764\n",
      "2465/85400 (epoch 5), train_loss = -8.40740352, time/batch = 1.313, learning rate = 0.00316764\n",
      "2475/85400 (epoch 5), train_loss = -8.40979339, time/batch = 1.324, learning rate = 0.00316764\n",
      "2485/85400 (epoch 5), train_loss = -8.41161424, time/batch = 1.327, learning rate = 0.00316764\n",
      "2495/85400 (epoch 5), train_loss = -8.41287662, time/batch = 1.326, learning rate = 0.00316764\n",
      "Saving model to model-5-2500.pth\n",
      "2505/85400 (epoch 5), train_loss = -8.41430008, time/batch = 1.311, learning rate = 0.00316764\n",
      "2515/85400 (epoch 5), train_loss = -8.41585012, time/batch = 1.322, learning rate = 0.00316764\n",
      "2525/85400 (epoch 5), train_loss = -8.41719101, time/batch = 1.323, learning rate = 0.00316764\n",
      "2535/85400 (epoch 5), train_loss = -8.41771751, time/batch = 1.318, learning rate = 0.00316764\n",
      "2545/85400 (epoch 5), train_loss = -8.41874814, time/batch = 1.319, learning rate = 0.00316764\n",
      "2555/85400 (epoch 5), train_loss = -8.41973025, time/batch = 1.311, learning rate = 0.00316764\n",
      "2572/85400 (epoch 6), train_loss = -8.44365263, time/batch = 1.292, learning rate = 0.00316764\n",
      "2582/85400 (epoch 6), train_loss = -8.48044314, time/batch = 1.315, learning rate = 0.00316764\n",
      "2592/85400 (epoch 6), train_loss = -8.47962901, time/batch = 1.320, learning rate = 0.00316764\n",
      "2602/85400 (epoch 6), train_loss = -8.47968485, time/batch = 1.326, learning rate = 0.00237573\n",
      "2612/85400 (epoch 6), train_loss = -8.47652599, time/batch = 1.323, learning rate = 0.00237573\n",
      "2622/85400 (epoch 6), train_loss = -8.47818427, time/batch = 1.323, learning rate = 0.00237573\n",
      "2632/85400 (epoch 6), train_loss = -8.48409229, time/batch = 1.316, learning rate = 0.00237573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642/85400 (epoch 6), train_loss = -8.48666468, time/batch = 1.323, learning rate = 0.00237573\n",
      "2652/85400 (epoch 6), train_loss = -8.48901798, time/batch = 1.324, learning rate = 0.00237573\n",
      "2662/85400 (epoch 6), train_loss = -8.48996737, time/batch = 1.326, learning rate = 0.00237573\n",
      "2672/85400 (epoch 6), train_loss = -8.49291107, time/batch = 1.322, learning rate = 0.00237573\n",
      "2682/85400 (epoch 6), train_loss = -8.49556034, time/batch = 1.321, learning rate = 0.00237573\n",
      "2692/85400 (epoch 6), train_loss = -8.49561583, time/batch = 1.319, learning rate = 0.00237573\n",
      "2702/85400 (epoch 6), train_loss = -8.49461420, time/batch = 1.310, learning rate = 0.00237573\n",
      "2712/85400 (epoch 6), train_loss = -8.49462419, time/batch = 1.325, learning rate = 0.00237573\n",
      "2722/85400 (epoch 6), train_loss = -8.49661918, time/batch = 1.315, learning rate = 0.00237573\n",
      "2732/85400 (epoch 6), train_loss = -8.49816058, time/batch = 1.311, learning rate = 0.00237573\n",
      "2742/85400 (epoch 6), train_loss = -8.49985436, time/batch = 1.319, learning rate = 0.00237573\n",
      "2752/85400 (epoch 6), train_loss = -8.50001673, time/batch = 1.312, learning rate = 0.00237573\n",
      "2762/85400 (epoch 6), train_loss = -8.50003660, time/batch = 1.322, learning rate = 0.00237573\n",
      "2772/85400 (epoch 6), train_loss = -8.50127743, time/batch = 1.322, learning rate = 0.00237573\n",
      "2782/85400 (epoch 6), train_loss = -8.50236256, time/batch = 1.326, learning rate = 0.00237573\n",
      "2792/85400 (epoch 6), train_loss = -8.50156830, time/batch = 1.324, learning rate = 0.00237573\n",
      "2802/85400 (epoch 6), train_loss = -8.50222221, time/batch = 1.327, learning rate = 0.00178179\n",
      "2812/85400 (epoch 6), train_loss = -8.50366455, time/batch = 1.323, learning rate = 0.00178179\n",
      "2822/85400 (epoch 6), train_loss = -8.50521299, time/batch = 1.317, learning rate = 0.00178179\n",
      "2832/85400 (epoch 6), train_loss = -8.50654076, time/batch = 1.328, learning rate = 0.00178179\n",
      "2842/85400 (epoch 6), train_loss = -8.50783765, time/batch = 1.320, learning rate = 0.00178179\n",
      "2852/85400 (epoch 6), train_loss = -8.50941730, time/batch = 1.325, learning rate = 0.00178179\n",
      "2862/85400 (epoch 6), train_loss = -8.50909125, time/batch = 1.330, learning rate = 0.00178179\n",
      "2872/85400 (epoch 6), train_loss = -8.50970223, time/batch = 1.326, learning rate = 0.00178179\n",
      "2882/85400 (epoch 6), train_loss = -8.51058252, time/batch = 1.324, learning rate = 0.00178179\n",
      "2892/85400 (epoch 6), train_loss = -8.51079927, time/batch = 1.320, learning rate = 0.00178179\n",
      "2902/85400 (epoch 6), train_loss = -8.51012730, time/batch = 1.327, learning rate = 0.00178179\n",
      "2912/85400 (epoch 6), train_loss = -8.51132542, time/batch = 1.320, learning rate = 0.00178179\n",
      "2922/85400 (epoch 6), train_loss = -8.51229563, time/batch = 1.319, learning rate = 0.00178179\n",
      "2932/85400 (epoch 6), train_loss = -8.51300179, time/batch = 1.319, learning rate = 0.00178179\n",
      "2942/85400 (epoch 6), train_loss = -8.51389099, time/batch = 1.318, learning rate = 0.00178179\n",
      "2952/85400 (epoch 6), train_loss = -8.51369977, time/batch = 1.323, learning rate = 0.00178179\n",
      "2962/85400 (epoch 6), train_loss = -8.51360993, time/batch = 1.324, learning rate = 0.00178179\n",
      "2972/85400 (epoch 6), train_loss = -8.51352792, time/batch = 1.316, learning rate = 0.00178179\n",
      "2982/85400 (epoch 6), train_loss = -8.51376760, time/batch = 1.320, learning rate = 0.00178179\n",
      "2999/85400 (epoch 7), train_loss = -8.52267475, time/batch = 1.324, learning rate = 0.00178179\n",
      "Saving model to model-7-3000.pth\n",
      "3009/85400 (epoch 7), train_loss = -8.52869973, time/batch = 1.324, learning rate = 0.00133635\n",
      "3019/85400 (epoch 7), train_loss = -8.53776150, time/batch = 1.321, learning rate = 0.00133635\n",
      "3029/85400 (epoch 7), train_loss = -8.54527647, time/batch = 1.321, learning rate = 0.00133635\n",
      "3039/85400 (epoch 7), train_loss = -8.54647732, time/batch = 1.320, learning rate = 0.00133635\n",
      "3049/85400 (epoch 7), train_loss = -8.54463477, time/batch = 1.324, learning rate = 0.00133635\n",
      "3059/85400 (epoch 7), train_loss = -8.54637359, time/batch = 1.311, learning rate = 0.00133635\n",
      "3069/85400 (epoch 7), train_loss = -8.54619447, time/batch = 1.316, learning rate = 0.00133635\n",
      "3079/85400 (epoch 7), train_loss = -8.54750920, time/batch = 1.314, learning rate = 0.00133635\n",
      "3089/85400 (epoch 7), train_loss = -8.54903764, time/batch = 1.312, learning rate = 0.00133635\n",
      "3099/85400 (epoch 7), train_loss = -8.54940114, time/batch = 1.329, learning rate = 0.00133635\n",
      "3109/85400 (epoch 7), train_loss = -8.54971602, time/batch = 1.321, learning rate = 0.00133635\n",
      "3119/85400 (epoch 7), train_loss = -8.55003859, time/batch = 1.322, learning rate = 0.00133635\n",
      "3129/85400 (epoch 7), train_loss = -8.54829694, time/batch = 1.320, learning rate = 0.00133635\n",
      "3139/85400 (epoch 7), train_loss = -8.54731028, time/batch = 1.313, learning rate = 0.00133635\n",
      "3149/85400 (epoch 7), train_loss = -8.54654565, time/batch = 1.324, learning rate = 0.00133635\n",
      "3159/85400 (epoch 7), train_loss = -8.54604667, time/batch = 1.320, learning rate = 0.00133635\n",
      "3169/85400 (epoch 7), train_loss = -8.54594289, time/batch = 1.322, learning rate = 0.00133635\n",
      "3179/85400 (epoch 7), train_loss = -8.54601599, time/batch = 1.320, learning rate = 0.00133635\n",
      "3189/85400 (epoch 7), train_loss = -8.54753030, time/batch = 1.314, learning rate = 0.00133635\n",
      "3199/85400 (epoch 7), train_loss = -8.54839280, time/batch = 1.331, learning rate = 0.00133635\n",
      "3209/85400 (epoch 7), train_loss = -8.54931522, time/batch = 1.316, learning rate = 0.00100226\n",
      "3219/85400 (epoch 7), train_loss = -8.55122187, time/batch = 1.321, learning rate = 0.00100226\n",
      "3229/85400 (epoch 7), train_loss = -8.55202299, time/batch = 1.323, learning rate = 0.00100226\n",
      "3239/85400 (epoch 7), train_loss = -8.55327053, time/batch = 1.315, learning rate = 0.00100226\n",
      "3249/85400 (epoch 7), train_loss = -8.55366684, time/batch = 1.318, learning rate = 0.00100226\n",
      "3259/85400 (epoch 7), train_loss = -8.55450116, time/batch = 1.322, learning rate = 0.00100226\n",
      "3269/85400 (epoch 7), train_loss = -8.55500042, time/batch = 1.320, learning rate = 0.00100226\n",
      "3279/85400 (epoch 7), train_loss = -8.55521427, time/batch = 1.324, learning rate = 0.00100226\n",
      "3289/85400 (epoch 7), train_loss = -8.55439587, time/batch = 1.326, learning rate = 0.00100226\n",
      "3299/85400 (epoch 7), train_loss = -8.55428839, time/batch = 1.324, learning rate = 0.00100226\n",
      "3309/85400 (epoch 7), train_loss = -8.55522333, time/batch = 1.336, learning rate = 0.00100226\n",
      "3319/85400 (epoch 7), train_loss = -8.55633622, time/batch = 1.316, learning rate = 0.00100226\n",
      "3329/85400 (epoch 7), train_loss = -8.55598410, time/batch = 1.315, learning rate = 0.00100226\n",
      "3339/85400 (epoch 7), train_loss = -8.55534225, time/batch = 1.332, learning rate = 0.00100226\n",
      "3349/85400 (epoch 7), train_loss = -8.55605327, time/batch = 1.320, learning rate = 0.00100226\n",
      "3359/85400 (epoch 7), train_loss = -8.55619257, time/batch = 1.329, learning rate = 0.00100226\n",
      "3369/85400 (epoch 7), train_loss = -8.55703029, time/batch = 1.319, learning rate = 0.00100226\n",
      "3379/85400 (epoch 7), train_loss = -8.55767146, time/batch = 1.322, learning rate = 0.00100226\n",
      "3389/85400 (epoch 7), train_loss = -8.55745804, time/batch = 1.315, learning rate = 0.00100226\n",
      "3399/85400 (epoch 7), train_loss = -8.55638180, time/batch = 1.324, learning rate = 0.00100226\n",
      "3409/85400 (epoch 7), train_loss = -8.55673172, time/batch = 1.309, learning rate = 0.00075169\n",
      "3426/85400 (epoch 8), train_loss = -8.56072130, time/batch = 1.316, learning rate = 0.00075169\n",
      "3436/85400 (epoch 8), train_loss = -8.54604187, time/batch = 1.323, learning rate = 0.00075169\n",
      "3446/85400 (epoch 8), train_loss = -8.55002518, time/batch = 1.324, learning rate = 0.00075169\n",
      "3456/85400 (epoch 8), train_loss = -8.55647748, time/batch = 1.320, learning rate = 0.00075169\n",
      "3466/85400 (epoch 8), train_loss = -8.56490368, time/batch = 1.325, learning rate = 0.00075169\n",
      "3476/85400 (epoch 8), train_loss = -8.56954985, time/batch = 1.320, learning rate = 0.00075169\n",
      "3486/85400 (epoch 8), train_loss = -8.57246771, time/batch = 1.332, learning rate = 0.00075169\n",
      "3496/85400 (epoch 8), train_loss = -8.57510234, time/batch = 1.327, learning rate = 0.00075169\n",
      "Saving model to model-8-3500.pth\n",
      "3506/85400 (epoch 8), train_loss = -8.57655468, time/batch = 1.312, learning rate = 0.00075169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516/85400 (epoch 8), train_loss = -8.57663486, time/batch = 1.323, learning rate = 0.00075169\n",
      "3526/85400 (epoch 8), train_loss = -8.57602290, time/batch = 1.318, learning rate = 0.00075169\n",
      "3536/85400 (epoch 8), train_loss = -8.57521274, time/batch = 1.313, learning rate = 0.00075169\n",
      "3546/85400 (epoch 8), train_loss = -8.57151403, time/batch = 1.318, learning rate = 0.00075169\n",
      "3556/85400 (epoch 8), train_loss = -8.57126171, time/batch = 1.327, learning rate = 0.00075169\n",
      "3566/85400 (epoch 8), train_loss = -8.57228198, time/batch = 1.312, learning rate = 0.00075169\n",
      "3576/85400 (epoch 8), train_loss = -8.57276850, time/batch = 1.313, learning rate = 0.00075169\n",
      "3586/85400 (epoch 8), train_loss = -8.57267193, time/batch = 1.325, learning rate = 0.00075169\n",
      "3596/85400 (epoch 8), train_loss = -8.57239628, time/batch = 1.334, learning rate = 0.00075169\n",
      "3606/85400 (epoch 8), train_loss = -8.57158631, time/batch = 1.330, learning rate = 0.00056377\n",
      "3616/85400 (epoch 8), train_loss = -8.57200223, time/batch = 1.316, learning rate = 0.00056377\n",
      "3626/85400 (epoch 8), train_loss = -8.57125800, time/batch = 1.327, learning rate = 0.00056377\n",
      "3636/85400 (epoch 8), train_loss = -8.57287146, time/batch = 1.313, learning rate = 0.00056377\n",
      "3646/85400 (epoch 8), train_loss = -8.57394074, time/batch = 1.325, learning rate = 0.00056377\n",
      "3656/85400 (epoch 8), train_loss = -8.57476641, time/batch = 1.327, learning rate = 0.00056377\n",
      "3666/85400 (epoch 8), train_loss = -8.57640025, time/batch = 1.324, learning rate = 0.00056377\n",
      "3676/85400 (epoch 8), train_loss = -8.57758278, time/batch = 1.319, learning rate = 0.00056377\n",
      "3686/85400 (epoch 8), train_loss = -8.57775645, time/batch = 1.325, learning rate = 0.00056377\n",
      "3696/85400 (epoch 8), train_loss = -8.57860985, time/batch = 1.321, learning rate = 0.00056377\n",
      "3706/85400 (epoch 8), train_loss = -8.57902016, time/batch = 1.318, learning rate = 0.00056377\n",
      "3716/85400 (epoch 8), train_loss = -8.57850349, time/batch = 1.320, learning rate = 0.00056377\n",
      "3726/85400 (epoch 8), train_loss = -8.57931320, time/batch = 1.319, learning rate = 0.00056377\n",
      "3736/85400 (epoch 8), train_loss = -8.57936723, time/batch = 1.308, learning rate = 0.00056377\n",
      "3746/85400 (epoch 8), train_loss = -8.57994409, time/batch = 1.338, learning rate = 0.00056377\n",
      "3756/85400 (epoch 8), train_loss = -8.58041798, time/batch = 1.326, learning rate = 0.00056377\n",
      "3766/85400 (epoch 8), train_loss = -8.57973351, time/batch = 1.320, learning rate = 0.00056377\n",
      "3776/85400 (epoch 8), train_loss = -8.57952244, time/batch = 1.315, learning rate = 0.00056377\n",
      "3786/85400 (epoch 8), train_loss = -8.57937192, time/batch = 1.320, learning rate = 0.00056377\n",
      "3796/85400 (epoch 8), train_loss = -8.57858220, time/batch = 1.327, learning rate = 0.00056377\n",
      "3806/85400 (epoch 8), train_loss = -8.57874413, time/batch = 1.315, learning rate = 0.00042283\n",
      "3816/85400 (epoch 8), train_loss = -8.57854950, time/batch = 1.327, learning rate = 0.00042283\n",
      "3826/85400 (epoch 8), train_loss = -8.57906687, time/batch = 1.315, learning rate = 0.00042283\n",
      "3836/85400 (epoch 8), train_loss = -8.57951225, time/batch = 1.319, learning rate = 0.00042283\n",
      "3853/85400 (epoch 9), train_loss = -8.61123362, time/batch = 1.311, learning rate = 0.00042283\n",
      "3863/85400 (epoch 9), train_loss = -8.60738983, time/batch = 1.324, learning rate = 0.00042283\n",
      "3873/85400 (epoch 9), train_loss = -8.60228561, time/batch = 1.320, learning rate = 0.00042283\n",
      "3883/85400 (epoch 9), train_loss = -8.60221465, time/batch = 1.308, learning rate = 0.00042283\n",
      "3893/85400 (epoch 9), train_loss = -8.59932526, time/batch = 1.321, learning rate = 0.00042283\n",
      "3903/85400 (epoch 9), train_loss = -8.59820046, time/batch = 1.330, learning rate = 0.00042283\n",
      "3913/85400 (epoch 9), train_loss = -8.59849491, time/batch = 1.314, learning rate = 0.00042283\n",
      "3923/85400 (epoch 9), train_loss = -8.59937226, time/batch = 1.313, learning rate = 0.00042283\n",
      "3933/85400 (epoch 9), train_loss = -8.59905647, time/batch = 1.316, learning rate = 0.00042283\n",
      "3943/85400 (epoch 9), train_loss = -8.59890308, time/batch = 1.321, learning rate = 0.00042283\n",
      "3953/85400 (epoch 9), train_loss = -8.59826751, time/batch = 1.320, learning rate = 0.00042283\n",
      "3963/85400 (epoch 9), train_loss = -8.59418418, time/batch = 1.325, learning rate = 0.00042283\n",
      "3973/85400 (epoch 9), train_loss = -8.59191266, time/batch = 1.324, learning rate = 0.00042283\n",
      "3983/85400 (epoch 9), train_loss = -8.59141033, time/batch = 1.324, learning rate = 0.00042283\n",
      "3993/85400 (epoch 9), train_loss = -8.59188240, time/batch = 1.319, learning rate = 0.00042283\n",
      "Saving model to model-9-4000.pth\n",
      "4003/85400 (epoch 9), train_loss = -8.59138694, time/batch = 1.306, learning rate = 0.00031712\n",
      "4013/85400 (epoch 9), train_loss = -8.59124194, time/batch = 1.319, learning rate = 0.00031712\n",
      "4023/85400 (epoch 9), train_loss = -8.59223034, time/batch = 1.319, learning rate = 0.00031712\n",
      "4033/85400 (epoch 9), train_loss = -8.59099056, time/batch = 1.311, learning rate = 0.00031712\n",
      "4043/85400 (epoch 9), train_loss = -8.59227515, time/batch = 1.324, learning rate = 0.00031712\n",
      "4053/85400 (epoch 9), train_loss = -8.59321166, time/batch = 1.320, learning rate = 0.00031712\n",
      "4063/85400 (epoch 9), train_loss = -8.59424859, time/batch = 1.325, learning rate = 0.00031712\n",
      "4073/85400 (epoch 9), train_loss = -8.59434654, time/batch = 1.321, learning rate = 0.00031712\n",
      "4083/85400 (epoch 9), train_loss = -8.59471678, time/batch = 1.327, learning rate = 0.00031712\n",
      "4093/85400 (epoch 9), train_loss = -8.59434698, time/batch = 1.316, learning rate = 0.00031712\n",
      "4103/85400 (epoch 9), train_loss = -8.59510349, time/batch = 1.323, learning rate = 0.00031712\n",
      "4113/85400 (epoch 9), train_loss = -8.59426381, time/batch = 1.318, learning rate = 0.00031712\n",
      "4123/85400 (epoch 9), train_loss = -8.59501171, time/batch = 1.309, learning rate = 0.00031712\n",
      "4133/85400 (epoch 9), train_loss = -8.59538121, time/batch = 1.316, learning rate = 0.00031712\n",
      "4143/85400 (epoch 9), train_loss = -8.59506900, time/batch = 1.333, learning rate = 0.00031712\n",
      "4153/85400 (epoch 9), train_loss = -8.59549901, time/batch = 1.318, learning rate = 0.00031712\n",
      "4163/85400 (epoch 9), train_loss = -8.59587893, time/batch = 1.317, learning rate = 0.00031712\n",
      "4173/85400 (epoch 9), train_loss = -8.59641947, time/batch = 1.318, learning rate = 0.00031712\n",
      "4183/85400 (epoch 9), train_loss = -8.59618860, time/batch = 1.318, learning rate = 0.00031712\n",
      "4193/85400 (epoch 9), train_loss = -8.59596326, time/batch = 1.318, learning rate = 0.00031712\n",
      "4203/85400 (epoch 9), train_loss = -8.59538637, time/batch = 1.310, learning rate = 0.00023784\n",
      "4213/85400 (epoch 9), train_loss = -8.59543196, time/batch = 1.321, learning rate = 0.00023784\n",
      "4223/85400 (epoch 9), train_loss = -8.59553650, time/batch = 1.326, learning rate = 0.00023784\n",
      "4233/85400 (epoch 9), train_loss = -8.59562430, time/batch = 1.323, learning rate = 0.00023784\n",
      "4243/85400 (epoch 9), train_loss = -8.59572583, time/batch = 1.323, learning rate = 0.00023784\n",
      "4253/85400 (epoch 9), train_loss = -8.59611747, time/batch = 1.317, learning rate = 0.00023784\n",
      "4263/85400 (epoch 9), train_loss = -8.59648602, time/batch = 1.317, learning rate = 0.00023784\n",
      "4280/85400 (epoch 10), train_loss = -8.59065247, time/batch = 1.321, learning rate = 0.00023784\n",
      "4290/85400 (epoch 10), train_loss = -8.59208803, time/batch = 1.322, learning rate = 0.00023784\n",
      "4300/85400 (epoch 10), train_loss = -8.58454021, time/batch = 1.317, learning rate = 0.00023784\n",
      "4310/85400 (epoch 10), train_loss = -8.58926406, time/batch = 1.320, learning rate = 0.00023784\n",
      "4320/85400 (epoch 10), train_loss = -8.58806252, time/batch = 1.319, learning rate = 0.00023784\n",
      "4330/85400 (epoch 10), train_loss = -8.59305414, time/batch = 1.328, learning rate = 0.00023784\n",
      "4340/85400 (epoch 10), train_loss = -8.59129238, time/batch = 1.310, learning rate = 0.00023784\n",
      "4350/85400 (epoch 10), train_loss = -8.59215505, time/batch = 1.329, learning rate = 0.00023784\n",
      "4360/85400 (epoch 10), train_loss = -8.59415438, time/batch = 1.325, learning rate = 0.00023784\n",
      "4370/85400 (epoch 10), train_loss = -8.59641527, time/batch = 1.321, learning rate = 0.00023784\n",
      "4380/85400 (epoch 10), train_loss = -8.59696462, time/batch = 1.323, learning rate = 0.00023784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4390/85400 (epoch 10), train_loss = -8.59860463, time/batch = 1.328, learning rate = 0.00023784\n",
      "4400/85400 (epoch 10), train_loss = -8.60071818, time/batch = 1.321, learning rate = 0.00023784\n",
      "4410/85400 (epoch 10), train_loss = -8.60089812, time/batch = 1.312, learning rate = 0.00017838\n",
      "4420/85400 (epoch 10), train_loss = -8.60118895, time/batch = 1.318, learning rate = 0.00017838\n",
      "4430/85400 (epoch 10), train_loss = -8.60109065, time/batch = 1.326, learning rate = 0.00017838\n",
      "4440/85400 (epoch 10), train_loss = -8.60043074, time/batch = 1.322, learning rate = 0.00017838\n",
      "4450/85400 (epoch 10), train_loss = -8.60063888, time/batch = 1.321, learning rate = 0.00017838\n",
      "4460/85400 (epoch 10), train_loss = -8.60116185, time/batch = 1.317, learning rate = 0.00017838\n",
      "4470/85400 (epoch 10), train_loss = -8.59925728, time/batch = 1.319, learning rate = 0.00017838\n",
      "4480/85400 (epoch 10), train_loss = -8.59997816, time/batch = 1.328, learning rate = 0.00017838\n",
      "4490/85400 (epoch 10), train_loss = -8.60095250, time/batch = 1.321, learning rate = 0.00017838\n",
      "4500/85400 (epoch 10), train_loss = -8.60226903, time/batch = 1.323, learning rate = 0.00017838\n",
      "Saving model to model-10-4500.pth\n",
      "4510/85400 (epoch 10), train_loss = -8.60159550, time/batch = 1.324, learning rate = 0.00017838\n",
      "4520/85400 (epoch 10), train_loss = -8.60170883, time/batch = 1.323, learning rate = 0.00017838\n",
      "4530/85400 (epoch 10), train_loss = -8.60281786, time/batch = 1.316, learning rate = 0.00017838\n",
      "4540/85400 (epoch 10), train_loss = -8.60326888, time/batch = 1.321, learning rate = 0.00017838\n",
      "4550/85400 (epoch 10), train_loss = -8.60335228, time/batch = 1.326, learning rate = 0.00017838\n",
      "4560/85400 (epoch 10), train_loss = -8.60195475, time/batch = 1.318, learning rate = 0.00017838\n",
      "4570/85400 (epoch 10), train_loss = -8.60210437, time/batch = 1.318, learning rate = 0.00017838\n",
      "4580/85400 (epoch 10), train_loss = -8.60170780, time/batch = 1.310, learning rate = 0.00017838\n",
      "4590/85400 (epoch 10), train_loss = -8.60203599, time/batch = 1.320, learning rate = 0.00017838\n",
      "4600/85400 (epoch 10), train_loss = -8.60248493, time/batch = 1.313, learning rate = 0.00017838\n",
      "4610/85400 (epoch 10), train_loss = -8.60301070, time/batch = 1.325, learning rate = 0.00013379\n",
      "4620/85400 (epoch 10), train_loss = -8.60281298, time/batch = 1.325, learning rate = 0.00013379\n",
      "4630/85400 (epoch 10), train_loss = -8.60366027, time/batch = 1.322, learning rate = 0.00013379\n",
      "4640/85400 (epoch 10), train_loss = -8.60438105, time/batch = 1.309, learning rate = 0.00013379\n",
      "4650/85400 (epoch 10), train_loss = -8.60415001, time/batch = 1.325, learning rate = 0.00013379\n",
      "4660/85400 (epoch 10), train_loss = -8.60392596, time/batch = 1.323, learning rate = 0.00013379\n",
      "4670/85400 (epoch 10), train_loss = -8.60453858, time/batch = 1.326, learning rate = 0.00013379\n",
      "4680/85400 (epoch 10), train_loss = -8.60518574, time/batch = 1.320, learning rate = 0.00013379\n",
      "4690/85400 (epoch 10), train_loss = -8.60519919, time/batch = 1.318, learning rate = 0.00013379\n",
      "4707/85400 (epoch 11), train_loss = -8.60046911, time/batch = 1.320, learning rate = 0.00013379\n",
      "4717/85400 (epoch 11), train_loss = -8.61450462, time/batch = 1.314, learning rate = 0.00013379\n",
      "4727/85400 (epoch 11), train_loss = -8.60935488, time/batch = 1.316, learning rate = 0.00013379\n",
      "4737/85400 (epoch 11), train_loss = -8.60698860, time/batch = 1.321, learning rate = 0.00013379\n",
      "4747/85400 (epoch 11), train_loss = -8.60545778, time/batch = 1.314, learning rate = 0.00013379\n",
      "4757/85400 (epoch 11), train_loss = -8.60731843, time/batch = 1.320, learning rate = 0.00013379\n",
      "4767/85400 (epoch 11), train_loss = -8.60982424, time/batch = 1.319, learning rate = 0.00013379\n",
      "4777/85400 (epoch 11), train_loss = -8.60820754, time/batch = 1.321, learning rate = 0.00013379\n",
      "4787/85400 (epoch 11), train_loss = -8.60677243, time/batch = 1.319, learning rate = 0.00013379\n",
      "4797/85400 (epoch 11), train_loss = -8.60599843, time/batch = 1.321, learning rate = 0.00013379\n",
      "4807/85400 (epoch 11), train_loss = -8.60702659, time/batch = 1.324, learning rate = 0.00010034\n",
      "4817/85400 (epoch 11), train_loss = -8.60865484, time/batch = 1.311, learning rate = 0.00010034\n",
      "4827/85400 (epoch 11), train_loss = -8.60686885, time/batch = 1.320, learning rate = 0.00010034\n",
      "4837/85400 (epoch 11), train_loss = -8.60582361, time/batch = 1.326, learning rate = 0.00010034\n",
      "4847/85400 (epoch 11), train_loss = -8.60737212, time/batch = 1.319, learning rate = 0.00010034\n",
      "4857/85400 (epoch 11), train_loss = -8.60856443, time/batch = 1.320, learning rate = 0.00010034\n",
      "4867/85400 (epoch 11), train_loss = -8.60776261, time/batch = 1.311, learning rate = 0.00010034\n",
      "4877/85400 (epoch 11), train_loss = -8.60772636, time/batch = 1.326, learning rate = 0.00010034\n",
      "4887/85400 (epoch 11), train_loss = -8.60931021, time/batch = 1.323, learning rate = 0.00010034\n",
      "4897/85400 (epoch 11), train_loss = -8.61000037, time/batch = 1.329, learning rate = 0.00010034\n",
      "4907/85400 (epoch 11), train_loss = -8.60999353, time/batch = 1.319, learning rate = 0.00010034\n",
      "4917/85400 (epoch 11), train_loss = -8.60941305, time/batch = 1.324, learning rate = 0.00010034\n",
      "4927/85400 (epoch 11), train_loss = -8.60975849, time/batch = 1.320, learning rate = 0.00010034\n",
      "4937/85400 (epoch 11), train_loss = -8.60983135, time/batch = 1.313, learning rate = 0.00010034\n",
      "4947/85400 (epoch 11), train_loss = -8.61014521, time/batch = 1.323, learning rate = 0.00010034\n",
      "4957/85400 (epoch 11), train_loss = -8.61032611, time/batch = 1.319, learning rate = 0.00010034\n",
      "4967/85400 (epoch 11), train_loss = -8.61035422, time/batch = 1.310, learning rate = 0.00010034\n",
      "4977/85400 (epoch 11), train_loss = -8.60971868, time/batch = 1.314, learning rate = 0.00010034\n",
      "4987/85400 (epoch 11), train_loss = -8.60960474, time/batch = 1.326, learning rate = 0.00010034\n",
      "4997/85400 (epoch 11), train_loss = -8.60991414, time/batch = 1.322, learning rate = 0.00010034\n",
      "Saving model to model-11-5000.pth\n",
      "5007/85400 (epoch 11), train_loss = -8.61071999, time/batch = 1.320, learning rate = 0.00007525\n",
      "5017/85400 (epoch 11), train_loss = -8.60977358, time/batch = 1.317, learning rate = 0.00007525\n",
      "5027/85400 (epoch 11), train_loss = -8.61032697, time/batch = 1.317, learning rate = 0.00007525\n",
      "5037/85400 (epoch 11), train_loss = -8.60950743, time/batch = 1.318, learning rate = 0.00007525\n",
      "5047/85400 (epoch 11), train_loss = -8.60975122, time/batch = 1.321, learning rate = 0.00007525\n",
      "5057/85400 (epoch 11), train_loss = -8.60974483, time/batch = 1.316, learning rate = 0.00007525\n",
      "5067/85400 (epoch 11), train_loss = -8.60968504, time/batch = 1.321, learning rate = 0.00007525\n",
      "5077/85400 (epoch 11), train_loss = -8.60998790, time/batch = 1.317, learning rate = 0.00007525\n",
      "5087/85400 (epoch 11), train_loss = -8.61037582, time/batch = 1.326, learning rate = 0.00007525\n",
      "5097/85400 (epoch 11), train_loss = -8.61028714, time/batch = 1.319, learning rate = 0.00007525\n",
      "5107/85400 (epoch 11), train_loss = -8.61035775, time/batch = 1.319, learning rate = 0.00007525\n",
      "5117/85400 (epoch 11), train_loss = -8.61074033, time/batch = 1.314, learning rate = 0.00007525\n",
      "5134/85400 (epoch 12), train_loss = -8.61118011, time/batch = 1.316, learning rate = 0.00007525\n",
      "5144/85400 (epoch 12), train_loss = -8.60741506, time/batch = 1.311, learning rate = 0.00007525\n",
      "5154/85400 (epoch 12), train_loss = -8.61187766, time/batch = 1.311, learning rate = 0.00007525\n",
      "5164/85400 (epoch 12), train_loss = -8.60978637, time/batch = 1.317, learning rate = 0.00007525\n",
      "5174/85400 (epoch 12), train_loss = -8.61302387, time/batch = 1.320, learning rate = 0.00007525\n",
      "5184/85400 (epoch 12), train_loss = -8.61543825, time/batch = 1.314, learning rate = 0.00007525\n",
      "5194/85400 (epoch 12), train_loss = -8.61688747, time/batch = 1.327, learning rate = 0.00007525\n",
      "5204/85400 (epoch 12), train_loss = -8.61436526, time/batch = 1.310, learning rate = 0.00005644\n",
      "5214/85400 (epoch 12), train_loss = -8.61535948, time/batch = 1.323, learning rate = 0.00005644\n",
      "5224/85400 (epoch 12), train_loss = -8.61193049, time/batch = 1.316, learning rate = 0.00005644\n",
      "5234/85400 (epoch 12), train_loss = -8.61275812, time/batch = 1.319, learning rate = 0.00005644\n",
      "5244/85400 (epoch 12), train_loss = -8.61110212, time/batch = 1.325, learning rate = 0.00005644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5254/85400 (epoch 12), train_loss = -8.61077643, time/batch = 1.325, learning rate = 0.00005644\n",
      "5264/85400 (epoch 12), train_loss = -8.61137226, time/batch = 1.321, learning rate = 0.00005644\n",
      "5274/85400 (epoch 12), train_loss = -8.61169401, time/batch = 1.320, learning rate = 0.00005644\n",
      "5284/85400 (epoch 12), train_loss = -8.61129606, time/batch = 1.316, learning rate = 0.00005644\n",
      "5294/85400 (epoch 12), train_loss = -8.61138228, time/batch = 1.316, learning rate = 0.00005644\n",
      "5304/85400 (epoch 12), train_loss = -8.61057242, time/batch = 1.314, learning rate = 0.00005644\n",
      "5314/85400 (epoch 12), train_loss = -8.61215465, time/batch = 1.325, learning rate = 0.00005644\n",
      "5324/85400 (epoch 12), train_loss = -8.61292380, time/batch = 1.312, learning rate = 0.00005644\n",
      "5334/85400 (epoch 12), train_loss = -8.61243608, time/batch = 1.323, learning rate = 0.00005644\n",
      "5344/85400 (epoch 12), train_loss = -8.61307936, time/batch = 1.316, learning rate = 0.00005644\n",
      "5354/85400 (epoch 12), train_loss = -8.61433178, time/batch = 1.325, learning rate = 0.00005644\n",
      "5364/85400 (epoch 12), train_loss = -8.61557066, time/batch = 1.317, learning rate = 0.00005644\n",
      "5374/85400 (epoch 12), train_loss = -8.61573489, time/batch = 1.320, learning rate = 0.00005644\n",
      "5384/85400 (epoch 12), train_loss = -8.61623842, time/batch = 1.323, learning rate = 0.00005644\n",
      "5394/85400 (epoch 12), train_loss = -8.61540632, time/batch = 1.324, learning rate = 0.00005644\n",
      "5404/85400 (epoch 12), train_loss = -8.61601267, time/batch = 1.319, learning rate = 0.00004233\n",
      "5414/85400 (epoch 12), train_loss = -8.61492828, time/batch = 1.321, learning rate = 0.00004233\n",
      "5424/85400 (epoch 12), train_loss = -8.61549276, time/batch = 1.314, learning rate = 0.00004233\n",
      "5434/85400 (epoch 12), train_loss = -8.61440980, time/batch = 1.327, learning rate = 0.00004233\n",
      "5444/85400 (epoch 12), train_loss = -8.61467960, time/batch = 1.324, learning rate = 0.00004233\n",
      "5454/85400 (epoch 12), train_loss = -8.61393214, time/batch = 1.317, learning rate = 0.00004233\n",
      "5464/85400 (epoch 12), train_loss = -8.61436221, time/batch = 1.319, learning rate = 0.00004233\n",
      "5474/85400 (epoch 12), train_loss = -8.61394588, time/batch = 1.319, learning rate = 0.00004233\n",
      "5484/85400 (epoch 12), train_loss = -8.61445920, time/batch = 1.314, learning rate = 0.00004233\n",
      "5494/85400 (epoch 12), train_loss = -8.61384383, time/batch = 1.321, learning rate = 0.00004233\n",
      "Saving model to model-12-5500.pth\n",
      "5504/85400 (epoch 12), train_loss = -8.61379048, time/batch = 1.316, learning rate = 0.00004233\n",
      "5514/85400 (epoch 12), train_loss = -8.61344035, time/batch = 1.331, learning rate = 0.00004233\n",
      "5524/85400 (epoch 12), train_loss = -8.61369057, time/batch = 1.325, learning rate = 0.00004233\n",
      "5534/85400 (epoch 12), train_loss = -8.61432173, time/batch = 1.322, learning rate = 0.00004233\n",
      "5544/85400 (epoch 12), train_loss = -8.61453242, time/batch = 1.317, learning rate = 0.00004233\n",
      "5561/85400 (epoch 13), train_loss = -8.58628016, time/batch = 1.325, learning rate = 0.00004233\n",
      "5571/85400 (epoch 13), train_loss = -8.60756679, time/batch = 1.324, learning rate = 0.00004233\n",
      "5581/85400 (epoch 13), train_loss = -8.61360175, time/batch = 1.321, learning rate = 0.00004233\n",
      "5591/85400 (epoch 13), train_loss = -8.61247602, time/batch = 1.316, learning rate = 0.00004233\n",
      "5601/85400 (epoch 13), train_loss = -8.60887848, time/batch = 1.318, learning rate = 0.00003175\n",
      "5611/85400 (epoch 13), train_loss = -8.61298806, time/batch = 1.321, learning rate = 0.00003175\n",
      "5621/85400 (epoch 13), train_loss = -8.61230974, time/batch = 1.328, learning rate = 0.00003175\n",
      "5631/85400 (epoch 13), train_loss = -8.61415592, time/batch = 1.315, learning rate = 0.00003175\n",
      "5641/85400 (epoch 13), train_loss = -8.61269717, time/batch = 1.316, learning rate = 0.00003175\n",
      "5651/85400 (epoch 13), train_loss = -8.61296866, time/batch = 1.325, learning rate = 0.00003175\n",
      "5661/85400 (epoch 13), train_loss = -8.61402401, time/batch = 1.331, learning rate = 0.00003175\n",
      "5671/85400 (epoch 13), train_loss = -8.61011717, time/batch = 1.322, learning rate = 0.00003175\n",
      "5681/85400 (epoch 13), train_loss = -8.61015174, time/batch = 1.319, learning rate = 0.00003175\n",
      "5691/85400 (epoch 13), train_loss = -8.61129837, time/batch = 1.311, learning rate = 0.00003175\n",
      "5701/85400 (epoch 13), train_loss = -8.61088977, time/batch = 1.325, learning rate = 0.00003175\n",
      "5711/85400 (epoch 13), train_loss = -8.61028724, time/batch = 1.319, learning rate = 0.00003175\n",
      "5721/85400 (epoch 13), train_loss = -8.61042459, time/batch = 1.326, learning rate = 0.00003175\n",
      "5731/85400 (epoch 13), train_loss = -8.61189590, time/batch = 1.322, learning rate = 0.00003175\n",
      "5741/85400 (epoch 13), train_loss = -8.61231928, time/batch = 1.321, learning rate = 0.00003175\n",
      "5751/85400 (epoch 13), train_loss = -8.61328059, time/batch = 1.320, learning rate = 0.00003175\n",
      "5761/85400 (epoch 13), train_loss = -8.61375645, time/batch = 1.313, learning rate = 0.00003175\n",
      "5771/85400 (epoch 13), train_loss = -8.61369869, time/batch = 1.324, learning rate = 0.00003175\n",
      "5781/85400 (epoch 13), train_loss = -8.61313955, time/batch = 1.330, learning rate = 0.00003175\n",
      "5791/85400 (epoch 13), train_loss = -8.61414457, time/batch = 1.324, learning rate = 0.00003175\n",
      "5801/85400 (epoch 13), train_loss = -8.61435692, time/batch = 1.313, learning rate = 0.00002381\n",
      "5811/85400 (epoch 13), train_loss = -8.61446652, time/batch = 1.322, learning rate = 0.00002381\n",
      "5821/85400 (epoch 13), train_loss = -8.61472160, time/batch = 1.319, learning rate = 0.00002381\n",
      "5831/85400 (epoch 13), train_loss = -8.61466269, time/batch = 1.323, learning rate = 0.00002381\n",
      "5841/85400 (epoch 13), train_loss = -8.61439372, time/batch = 1.326, learning rate = 0.00002381\n",
      "5851/85400 (epoch 13), train_loss = -8.61491040, time/batch = 1.323, learning rate = 0.00002381\n",
      "5861/85400 (epoch 13), train_loss = -8.61481757, time/batch = 1.312, learning rate = 0.00002381\n",
      "5871/85400 (epoch 13), train_loss = -8.61557358, time/batch = 1.321, learning rate = 0.00002381\n",
      "5881/85400 (epoch 13), train_loss = -8.61586540, time/batch = 1.324, learning rate = 0.00002381\n",
      "5891/85400 (epoch 13), train_loss = -8.61573638, time/batch = 1.318, learning rate = 0.00002381\n",
      "5901/85400 (epoch 13), train_loss = -8.61545270, time/batch = 1.324, learning rate = 0.00002381\n",
      "5911/85400 (epoch 13), train_loss = -8.61530452, time/batch = 1.323, learning rate = 0.00002381\n",
      "5921/85400 (epoch 13), train_loss = -8.61503654, time/batch = 1.322, learning rate = 0.00002381\n",
      "5931/85400 (epoch 13), train_loss = -8.61493503, time/batch = 1.322, learning rate = 0.00002381\n",
      "5941/85400 (epoch 13), train_loss = -8.61503019, time/batch = 1.314, learning rate = 0.00002381\n",
      "5951/85400 (epoch 13), train_loss = -8.61502706, time/batch = 1.318, learning rate = 0.00002381\n",
      "5961/85400 (epoch 13), train_loss = -8.61478741, time/batch = 1.317, learning rate = 0.00002381\n",
      "5971/85400 (epoch 13), train_loss = -8.61458419, time/batch = 1.327, learning rate = 0.00002381\n",
      "5988/85400 (epoch 14), train_loss = -8.60925655, time/batch = 1.321, learning rate = 0.00002381\n",
      "5998/85400 (epoch 14), train_loss = -8.60412621, time/batch = 1.321, learning rate = 0.00002381\n",
      "Saving model to model-14-6000.pth\n",
      "6008/85400 (epoch 14), train_loss = -8.60580718, time/batch = 1.325, learning rate = 0.00001786\n",
      "6018/85400 (epoch 14), train_loss = -8.61313872, time/batch = 1.312, learning rate = 0.00001786\n",
      "6028/85400 (epoch 14), train_loss = -8.61678181, time/batch = 1.316, learning rate = 0.00001786\n",
      "6038/85400 (epoch 14), train_loss = -8.61797667, time/batch = 1.320, learning rate = 0.00001786\n",
      "6048/85400 (epoch 14), train_loss = -8.61568574, time/batch = 1.315, learning rate = 0.00001786\n",
      "6058/85400 (epoch 14), train_loss = -8.61514673, time/batch = 1.314, learning rate = 0.00001786\n",
      "6068/85400 (epoch 14), train_loss = -8.61442481, time/batch = 1.316, learning rate = 0.00001786\n",
      "6078/85400 (epoch 14), train_loss = -8.61376424, time/batch = 1.312, learning rate = 0.00001786\n",
      "6088/85400 (epoch 14), train_loss = -8.61130518, time/batch = 1.323, learning rate = 0.00001786\n",
      "6098/85400 (epoch 14), train_loss = -8.61037014, time/batch = 1.324, learning rate = 0.00001786\n",
      "6108/85400 (epoch 14), train_loss = -8.61173651, time/batch = 1.325, learning rate = 0.00001786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118/85400 (epoch 14), train_loss = -8.61415524, time/batch = 1.318, learning rate = 0.00001786\n",
      "6128/85400 (epoch 14), train_loss = -8.61493821, time/batch = 1.326, learning rate = 0.00001786\n",
      "6138/85400 (epoch 14), train_loss = -8.61359949, time/batch = 1.324, learning rate = 0.00001786\n",
      "6148/85400 (epoch 14), train_loss = -8.61404626, time/batch = 1.324, learning rate = 0.00001786\n",
      "6158/85400 (epoch 14), train_loss = -8.61424716, time/batch = 1.310, learning rate = 0.00001786\n",
      "6168/85400 (epoch 14), train_loss = -8.61284112, time/batch = 1.326, learning rate = 0.00001786\n",
      "6178/85400 (epoch 14), train_loss = -8.61319747, time/batch = 1.333, learning rate = 0.00001786\n",
      "6188/85400 (epoch 14), train_loss = -8.61342824, time/batch = 1.313, learning rate = 0.00001786\n",
      "6198/85400 (epoch 14), train_loss = -8.61397721, time/batch = 1.319, learning rate = 0.00001786\n",
      "6208/85400 (epoch 14), train_loss = -8.61415679, time/batch = 1.318, learning rate = 0.00001339\n",
      "6218/85400 (epoch 14), train_loss = -8.61449887, time/batch = 1.326, learning rate = 0.00001339\n",
      "6228/85400 (epoch 14), train_loss = -8.61462204, time/batch = 1.320, learning rate = 0.00001339\n",
      "6238/85400 (epoch 14), train_loss = -8.61467340, time/batch = 1.326, learning rate = 0.00001339\n",
      "6248/85400 (epoch 14), train_loss = -8.61429438, time/batch = 1.323, learning rate = 0.00001339\n",
      "6258/85400 (epoch 14), train_loss = -8.61402037, time/batch = 1.321, learning rate = 0.00001339\n",
      "6268/85400 (epoch 14), train_loss = -8.61352359, time/batch = 1.311, learning rate = 0.00001339\n",
      "6278/85400 (epoch 14), train_loss = -8.61277000, time/batch = 1.315, learning rate = 0.00001339\n",
      "6288/85400 (epoch 14), train_loss = -8.61363739, time/batch = 1.323, learning rate = 0.00001339\n",
      "6298/85400 (epoch 14), train_loss = -8.61315358, time/batch = 1.305, learning rate = 0.00001339\n",
      "6308/85400 (epoch 14), train_loss = -8.61310224, time/batch = 1.323, learning rate = 0.00001339\n",
      "6318/85400 (epoch 14), train_loss = -8.61312258, time/batch = 1.317, learning rate = 0.00001339\n",
      "6328/85400 (epoch 14), train_loss = -8.61347224, time/batch = 1.325, learning rate = 0.00001339\n",
      "6338/85400 (epoch 14), train_loss = -8.61357586, time/batch = 1.326, learning rate = 0.00001339\n",
      "6348/85400 (epoch 14), train_loss = -8.61418581, time/batch = 1.325, learning rate = 0.00001339\n",
      "6358/85400 (epoch 14), train_loss = -8.61466590, time/batch = 1.311, learning rate = 0.00001339\n",
      "6368/85400 (epoch 14), train_loss = -8.61509023, time/batch = 1.324, learning rate = 0.00001339\n",
      "6378/85400 (epoch 14), train_loss = -8.61435315, time/batch = 1.312, learning rate = 0.00001339\n",
      "6388/85400 (epoch 14), train_loss = -8.61427706, time/batch = 1.321, learning rate = 0.00001339\n",
      "6398/85400 (epoch 14), train_loss = -8.61494587, time/batch = 1.324, learning rate = 0.00001339\n",
      "6415/85400 (epoch 15), train_loss = -8.61599627, time/batch = 1.323, learning rate = 0.00001005\n",
      "6425/85400 (epoch 15), train_loss = -8.62328038, time/batch = 1.326, learning rate = 0.00001005\n",
      "6435/85400 (epoch 15), train_loss = -8.62355309, time/batch = 1.311, learning rate = 0.00001005\n",
      "6445/85400 (epoch 15), train_loss = -8.62344146, time/batch = 1.323, learning rate = 0.00001005\n",
      "6455/85400 (epoch 15), train_loss = -8.62062613, time/batch = 1.324, learning rate = 0.00001005\n",
      "6465/85400 (epoch 15), train_loss = -8.61737299, time/batch = 1.326, learning rate = 0.00001005\n",
      "6475/85400 (epoch 15), train_loss = -8.61808093, time/batch = 1.322, learning rate = 0.00001005\n",
      "6485/85400 (epoch 15), train_loss = -8.61729330, time/batch = 1.322, learning rate = 0.00001005\n",
      "6495/85400 (epoch 15), train_loss = -8.61940272, time/batch = 1.326, learning rate = 0.00001005\n",
      "Saving model to model-15-6500.pth\n",
      "6505/85400 (epoch 15), train_loss = -8.61979162, time/batch = 1.308, learning rate = 0.00001005\n",
      "6515/85400 (epoch 15), train_loss = -8.61801703, time/batch = 1.315, learning rate = 0.00001005\n",
      "6525/85400 (epoch 15), train_loss = -8.61873471, time/batch = 1.311, learning rate = 0.00001005\n",
      "6535/85400 (epoch 15), train_loss = -8.61763682, time/batch = 1.322, learning rate = 0.00001005\n",
      "6545/85400 (epoch 15), train_loss = -8.61824875, time/batch = 1.321, learning rate = 0.00001005\n",
      "6555/85400 (epoch 15), train_loss = -8.61894644, time/batch = 1.321, learning rate = 0.00001005\n",
      "6565/85400 (epoch 15), train_loss = -8.61878564, time/batch = 1.319, learning rate = 0.00001005\n",
      "6575/85400 (epoch 15), train_loss = -8.61895763, time/batch = 1.312, learning rate = 0.00001005\n",
      "6585/85400 (epoch 15), train_loss = -8.62010241, time/batch = 1.315, learning rate = 0.00001005\n",
      "6595/85400 (epoch 15), train_loss = -8.62105683, time/batch = 1.312, learning rate = 0.00001005\n",
      "6605/85400 (epoch 15), train_loss = -8.62168924, time/batch = 1.319, learning rate = 0.00000753\n",
      "6615/85400 (epoch 15), train_loss = -8.62068530, time/batch = 1.327, learning rate = 0.00000753\n",
      "6625/85400 (epoch 15), train_loss = -8.62092748, time/batch = 1.313, learning rate = 0.00000753\n",
      "6635/85400 (epoch 15), train_loss = -8.62062662, time/batch = 1.307, learning rate = 0.00000753\n",
      "6645/85400 (epoch 15), train_loss = -8.62054539, time/batch = 1.319, learning rate = 0.00000753\n",
      "6655/85400 (epoch 15), train_loss = -8.62019922, time/batch = 1.330, learning rate = 0.00000753\n",
      "6665/85400 (epoch 15), train_loss = -8.61956844, time/batch = 1.332, learning rate = 0.00000753\n",
      "6675/85400 (epoch 15), train_loss = -8.61839900, time/batch = 1.331, learning rate = 0.00000753\n",
      "6685/85400 (epoch 15), train_loss = -8.61900190, time/batch = 1.317, learning rate = 0.00000753\n",
      "6695/85400 (epoch 15), train_loss = -8.61857760, time/batch = 1.321, learning rate = 0.00000753\n",
      "6705/85400 (epoch 15), train_loss = -8.61791656, time/batch = 1.314, learning rate = 0.00000753\n",
      "6715/85400 (epoch 15), train_loss = -8.61807393, time/batch = 1.320, learning rate = 0.00000753\n",
      "6725/85400 (epoch 15), train_loss = -8.61855838, time/batch = 1.317, learning rate = 0.00000753\n",
      "6735/85400 (epoch 15), train_loss = -8.61873166, time/batch = 1.319, learning rate = 0.00000753\n",
      "6745/85400 (epoch 15), train_loss = -8.61862139, time/batch = 1.336, learning rate = 0.00000753\n",
      "6755/85400 (epoch 15), train_loss = -8.61854167, time/batch = 1.314, learning rate = 0.00000753\n",
      "6765/85400 (epoch 15), train_loss = -8.61838401, time/batch = 1.309, learning rate = 0.00000753\n",
      "6775/85400 (epoch 15), train_loss = -8.61857176, time/batch = 1.318, learning rate = 0.00000753\n",
      "6785/85400 (epoch 15), train_loss = -8.61903222, time/batch = 1.321, learning rate = 0.00000753\n",
      "6795/85400 (epoch 15), train_loss = -8.61914529, time/batch = 1.320, learning rate = 0.00000753\n",
      "6805/85400 (epoch 15), train_loss = -8.61919412, time/batch = 1.307, learning rate = 0.00000565\n",
      "6815/85400 (epoch 15), train_loss = -8.61876560, time/batch = 1.325, learning rate = 0.00000565\n",
      "6825/85400 (epoch 15), train_loss = -8.61846444, time/batch = 1.325, learning rate = 0.00000565\n",
      "6842/85400 (epoch 16), train_loss = -8.62153921, time/batch = 1.320, learning rate = 0.00000565\n",
      "6852/85400 (epoch 16), train_loss = -8.62157836, time/batch = 1.318, learning rate = 0.00000565\n",
      "6862/85400 (epoch 16), train_loss = -8.62484194, time/batch = 1.312, learning rate = 0.00000565\n",
      "6872/85400 (epoch 16), train_loss = -8.62164123, time/batch = 1.318, learning rate = 0.00000565\n",
      "6882/85400 (epoch 16), train_loss = -8.62647459, time/batch = 1.325, learning rate = 0.00000565\n",
      "6892/85400 (epoch 16), train_loss = -8.62596351, time/batch = 1.330, learning rate = 0.00000565\n",
      "6902/85400 (epoch 16), train_loss = -8.62615450, time/batch = 1.321, learning rate = 0.00000565\n",
      "6912/85400 (epoch 16), train_loss = -8.62569698, time/batch = 1.320, learning rate = 0.00000565\n",
      "6922/85400 (epoch 16), train_loss = -8.62625349, time/batch = 1.315, learning rate = 0.00000565\n",
      "6932/85400 (epoch 16), train_loss = -8.62821588, time/batch = 1.319, learning rate = 0.00000565\n",
      "6942/85400 (epoch 16), train_loss = -8.62839605, time/batch = 1.312, learning rate = 0.00000565\n",
      "6952/85400 (epoch 16), train_loss = -8.62592393, time/batch = 1.323, learning rate = 0.00000565\n",
      "6962/85400 (epoch 16), train_loss = -8.62545516, time/batch = 1.322, learning rate = 0.00000565\n",
      "6972/85400 (epoch 16), train_loss = -8.62528028, time/batch = 1.321, learning rate = 0.00000565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6982/85400 (epoch 16), train_loss = -8.62549466, time/batch = 1.318, learning rate = 0.00000565\n",
      "6992/85400 (epoch 16), train_loss = -8.62525014, time/batch = 1.326, learning rate = 0.00000565\n",
      "Saving model to model-16-7000.pth\n",
      "7002/85400 (epoch 16), train_loss = -8.62517369, time/batch = 1.318, learning rate = 0.00000424\n",
      "7012/85400 (epoch 16), train_loss = -8.62575791, time/batch = 1.327, learning rate = 0.00000424\n",
      "7022/85400 (epoch 16), train_loss = -8.62652658, time/batch = 1.325, learning rate = 0.00000424\n",
      "7032/85400 (epoch 16), train_loss = -8.62524131, time/batch = 1.328, learning rate = 0.00000424\n",
      "7042/85400 (epoch 16), train_loss = -8.62465772, time/batch = 1.325, learning rate = 0.00000424\n",
      "7052/85400 (epoch 16), train_loss = -8.62463805, time/batch = 1.326, learning rate = 0.00000424\n",
      "7062/85400 (epoch 16), train_loss = -8.62399914, time/batch = 1.322, learning rate = 0.00000424\n",
      "7072/85400 (epoch 16), train_loss = -8.62333847, time/batch = 1.508, learning rate = 0.00000424\n",
      "7082/85400 (epoch 16), train_loss = -8.62238106, time/batch = 1.312, learning rate = 0.00000424\n",
      "7092/85400 (epoch 16), train_loss = -8.62153083, time/batch = 1.315, learning rate = 0.00000424\n",
      "7102/85400 (epoch 16), train_loss = -8.62149748, time/batch = 1.317, learning rate = 0.00000424\n",
      "7112/85400 (epoch 16), train_loss = -8.62097691, time/batch = 1.318, learning rate = 0.00000424\n",
      "7122/85400 (epoch 16), train_loss = -8.62070570, time/batch = 1.315, learning rate = 0.00000424\n",
      "7132/85400 (epoch 16), train_loss = -8.62092147, time/batch = 1.332, learning rate = 0.00000424\n",
      "7142/85400 (epoch 16), train_loss = -8.62085323, time/batch = 1.321, learning rate = 0.00000424\n",
      "7152/85400 (epoch 16), train_loss = -8.62047903, time/batch = 1.321, learning rate = 0.00000424\n",
      "7162/85400 (epoch 16), train_loss = -8.62027181, time/batch = 1.324, learning rate = 0.00000424\n",
      "7172/85400 (epoch 16), train_loss = -8.62045428, time/batch = 1.322, learning rate = 0.00000424\n",
      "7182/85400 (epoch 16), train_loss = -8.62032705, time/batch = 1.321, learning rate = 0.00000424\n",
      "7192/85400 (epoch 16), train_loss = -8.62078131, time/batch = 1.326, learning rate = 0.00000424\n",
      "7202/85400 (epoch 16), train_loss = -8.62092581, time/batch = 1.310, learning rate = 0.00000318\n",
      "7212/85400 (epoch 16), train_loss = -8.62122038, time/batch = 1.313, learning rate = 0.00000318\n",
      "7222/85400 (epoch 16), train_loss = -8.62048286, time/batch = 1.318, learning rate = 0.00000318\n",
      "7232/85400 (epoch 16), train_loss = -8.61938362, time/batch = 1.320, learning rate = 0.00000318\n",
      "7242/85400 (epoch 16), train_loss = -8.61872730, time/batch = 1.319, learning rate = 0.00000318\n",
      "7252/85400 (epoch 16), train_loss = -8.61848360, time/batch = 1.317, learning rate = 0.00000318\n",
      "7269/85400 (epoch 17), train_loss = -8.61726913, time/batch = 1.312, learning rate = 0.00000318\n",
      "7279/85400 (epoch 17), train_loss = -8.60765271, time/batch = 1.313, learning rate = 0.00000318\n",
      "7289/85400 (epoch 17), train_loss = -8.61307131, time/batch = 1.313, learning rate = 0.00000318\n",
      "7299/85400 (epoch 17), train_loss = -8.60450556, time/batch = 1.323, learning rate = 0.00000318\n",
      "7309/85400 (epoch 17), train_loss = -8.60487047, time/batch = 1.320, learning rate = 0.00000318\n",
      "7319/85400 (epoch 17), train_loss = -8.60412227, time/batch = 1.316, learning rate = 0.00000318\n",
      "7329/85400 (epoch 17), train_loss = -8.60550571, time/batch = 1.317, learning rate = 0.00000318\n",
      "7339/85400 (epoch 17), train_loss = -8.60481036, time/batch = 1.323, learning rate = 0.00000318\n",
      "7349/85400 (epoch 17), train_loss = -8.60585785, time/batch = 1.324, learning rate = 0.00000318\n",
      "7359/85400 (epoch 17), train_loss = -8.60338120, time/batch = 1.324, learning rate = 0.00000318\n",
      "7369/85400 (epoch 17), train_loss = -8.60632317, time/batch = 1.317, learning rate = 0.00000318\n",
      "7379/85400 (epoch 17), train_loss = -8.60645635, time/batch = 1.312, learning rate = 0.00000318\n",
      "7389/85400 (epoch 17), train_loss = -8.60660810, time/batch = 1.326, learning rate = 0.00000318\n",
      "7399/85400 (epoch 17), train_loss = -8.60635448, time/batch = 1.323, learning rate = 0.00000318\n",
      "7409/85400 (epoch 17), train_loss = -8.60683361, time/batch = 1.327, learning rate = 0.00000238\n",
      "7419/85400 (epoch 17), train_loss = -8.60892433, time/batch = 1.316, learning rate = 0.00000238\n",
      "7429/85400 (epoch 17), train_loss = -8.60836846, time/batch = 1.320, learning rate = 0.00000238\n",
      "7439/85400 (epoch 17), train_loss = -8.61083315, time/batch = 1.316, learning rate = 0.00000238\n",
      "7449/85400 (epoch 17), train_loss = -8.61026969, time/batch = 1.329, learning rate = 0.00000238\n",
      "7459/85400 (epoch 17), train_loss = -8.61111982, time/batch = 1.312, learning rate = 0.00000238\n",
      "7469/85400 (epoch 17), train_loss = -8.61243700, time/batch = 1.311, learning rate = 0.00000238\n",
      "7479/85400 (epoch 17), train_loss = -8.61251828, time/batch = 1.310, learning rate = 0.00000238\n",
      "7489/85400 (epoch 17), train_loss = -8.61390234, time/batch = 1.336, learning rate = 0.00000238\n",
      "7499/85400 (epoch 17), train_loss = -8.61457411, time/batch = 1.320, learning rate = 0.00000238\n",
      "Saving model to model-17-7500.pth\n",
      "7509/85400 (epoch 17), train_loss = -8.61492462, time/batch = 1.316, learning rate = 0.00000238\n",
      "7519/85400 (epoch 17), train_loss = -8.61402623, time/batch = 1.325, learning rate = 0.00000238\n",
      "7529/85400 (epoch 17), train_loss = -8.61378385, time/batch = 1.315, learning rate = 0.00000238\n",
      "7539/85400 (epoch 17), train_loss = -8.61495974, time/batch = 1.316, learning rate = 0.00000238\n",
      "7549/85400 (epoch 17), train_loss = -8.61527997, time/batch = 1.321, learning rate = 0.00000238\n",
      "7559/85400 (epoch 17), train_loss = -8.61552573, time/batch = 1.318, learning rate = 0.00000238\n",
      "7569/85400 (epoch 17), train_loss = -8.61625480, time/batch = 1.322, learning rate = 0.00000238\n",
      "7579/85400 (epoch 17), train_loss = -8.61555924, time/batch = 1.319, learning rate = 0.00000238\n",
      "7589/85400 (epoch 17), train_loss = -8.61570352, time/batch = 1.323, learning rate = 0.00000238\n",
      "7599/85400 (epoch 17), train_loss = -8.61601534, time/batch = 1.320, learning rate = 0.00000238\n",
      "7609/85400 (epoch 17), train_loss = -8.61565816, time/batch = 1.322, learning rate = 0.00000179\n",
      "7619/85400 (epoch 17), train_loss = -8.61579220, time/batch = 1.316, learning rate = 0.00000179\n",
      "7629/85400 (epoch 17), train_loss = -8.61638469, time/batch = 1.322, learning rate = 0.00000179\n",
      "7639/85400 (epoch 17), train_loss = -8.61677754, time/batch = 1.329, learning rate = 0.00000179\n",
      "7649/85400 (epoch 17), train_loss = -8.61714189, time/batch = 1.321, learning rate = 0.00000179\n",
      "7659/85400 (epoch 17), train_loss = -8.61702992, time/batch = 1.314, learning rate = 0.00000179\n",
      "7669/85400 (epoch 17), train_loss = -8.61734579, time/batch = 1.324, learning rate = 0.00000179\n",
      "7679/85400 (epoch 17), train_loss = -8.61732860, time/batch = 1.319, learning rate = 0.00000179\n",
      "7696/85400 (epoch 18), train_loss = -8.62134256, time/batch = 1.321, learning rate = 0.00000179\n",
      "7706/85400 (epoch 18), train_loss = -8.61642385, time/batch = 1.329, learning rate = 0.00000179\n",
      "7716/85400 (epoch 18), train_loss = -8.62140153, time/batch = 1.317, learning rate = 0.00000179\n",
      "7726/85400 (epoch 18), train_loss = -8.62073257, time/batch = 1.307, learning rate = 0.00000179\n",
      "7736/85400 (epoch 18), train_loss = -8.62229942, time/batch = 1.317, learning rate = 0.00000179\n",
      "7746/85400 (epoch 18), train_loss = -8.62090092, time/batch = 1.331, learning rate = 0.00000179\n",
      "7756/85400 (epoch 18), train_loss = -8.61648585, time/batch = 1.335, learning rate = 0.00000179\n",
      "7766/85400 (epoch 18), train_loss = -8.61815034, time/batch = 1.331, learning rate = 0.00000179\n",
      "7776/85400 (epoch 18), train_loss = -8.61670076, time/batch = 1.308, learning rate = 0.00000179\n",
      "7786/85400 (epoch 18), train_loss = -8.61793582, time/batch = 1.321, learning rate = 0.00000179\n",
      "7796/85400 (epoch 18), train_loss = -8.61683689, time/batch = 1.333, learning rate = 0.00000179\n",
      "7806/85400 (epoch 18), train_loss = -8.61864801, time/batch = 1.327, learning rate = 0.00000134\n",
      "7816/85400 (epoch 18), train_loss = -8.61489488, time/batch = 1.311, learning rate = 0.00000134\n",
      "7826/85400 (epoch 18), train_loss = -8.61644170, time/batch = 1.319, learning rate = 0.00000134\n",
      "7836/85400 (epoch 18), train_loss = -8.61584700, time/batch = 1.325, learning rate = 0.00000134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7846/85400 (epoch 18), train_loss = -8.61515960, time/batch = 1.323, learning rate = 0.00000134\n",
      "7856/85400 (epoch 18), train_loss = -8.61549578, time/batch = 1.329, learning rate = 0.00000134\n",
      "7866/85400 (epoch 18), train_loss = -8.61611216, time/batch = 1.324, learning rate = 0.00000134\n",
      "7876/85400 (epoch 18), train_loss = -8.61551865, time/batch = 1.323, learning rate = 0.00000134\n",
      "7886/85400 (epoch 18), train_loss = -8.61643568, time/batch = 1.309, learning rate = 0.00000134\n",
      "7896/85400 (epoch 18), train_loss = -8.61685306, time/batch = 1.316, learning rate = 0.00000134\n",
      "7906/85400 (epoch 18), train_loss = -8.61464348, time/batch = 1.334, learning rate = 0.00000134\n",
      "7916/85400 (epoch 18), train_loss = -8.61522927, time/batch = 1.315, learning rate = 0.00000134\n",
      "7926/85400 (epoch 18), train_loss = -8.61580834, time/batch = 1.321, learning rate = 0.00000134\n",
      "7936/85400 (epoch 18), train_loss = -8.61572947, time/batch = 1.313, learning rate = 0.00000134\n",
      "7946/85400 (epoch 18), train_loss = -8.61622743, time/batch = 1.313, learning rate = 0.00000134\n",
      "7956/85400 (epoch 18), train_loss = -8.61630375, time/batch = 1.321, learning rate = 0.00000134\n",
      "7966/85400 (epoch 18), train_loss = -8.61676637, time/batch = 1.319, learning rate = 0.00000134\n",
      "7976/85400 (epoch 18), train_loss = -8.61762085, time/batch = 1.310, learning rate = 0.00000134\n",
      "7986/85400 (epoch 18), train_loss = -8.61803183, time/batch = 1.319, learning rate = 0.00000134\n",
      "7996/85400 (epoch 18), train_loss = -8.61836578, time/batch = 1.313, learning rate = 0.00000134\n",
      "Saving model to model-18-8000.pth\n",
      "8006/85400 (epoch 18), train_loss = -8.61809573, time/batch = 1.362, learning rate = 0.00000101\n",
      "8016/85400 (epoch 18), train_loss = -8.61799853, time/batch = 1.342, learning rate = 0.00000101\n",
      "8026/85400 (epoch 18), train_loss = -8.61810861, time/batch = 1.374, learning rate = 0.00000101\n",
      "8036/85400 (epoch 18), train_loss = -8.61781263, time/batch = 1.366, learning rate = 0.00000101\n",
      "8046/85400 (epoch 18), train_loss = -8.61747374, time/batch = 1.356, learning rate = 0.00000101\n",
      "8056/85400 (epoch 18), train_loss = -8.61805126, time/batch = 1.347, learning rate = 0.00000101\n",
      "8066/85400 (epoch 18), train_loss = -8.61768808, time/batch = 1.355, learning rate = 0.00000101\n",
      "8076/85400 (epoch 18), train_loss = -8.61727520, time/batch = 1.313, learning rate = 0.00000101\n",
      "8086/85400 (epoch 18), train_loss = -8.61754354, time/batch = 1.318, learning rate = 0.00000101\n",
      "8096/85400 (epoch 18), train_loss = -8.61786466, time/batch = 1.318, learning rate = 0.00000101\n",
      "8106/85400 (epoch 18), train_loss = -8.61712891, time/batch = 1.324, learning rate = 0.00000101\n",
      "8123/85400 (epoch 19), train_loss = -8.62621641, time/batch = 1.314, learning rate = 0.00000101\n",
      "8133/85400 (epoch 19), train_loss = -8.61816506, time/batch = 1.321, learning rate = 0.00000101\n",
      "8143/85400 (epoch 19), train_loss = -8.61326097, time/batch = 1.337, learning rate = 0.00000101\n",
      "8153/85400 (epoch 19), train_loss = -8.61033247, time/batch = 1.331, learning rate = 0.00000101\n",
      "8163/85400 (epoch 19), train_loss = -8.61399542, time/batch = 1.323, learning rate = 0.00000101\n",
      "8173/85400 (epoch 19), train_loss = -8.61464853, time/batch = 1.333, learning rate = 0.00000101\n",
      "8183/85400 (epoch 19), train_loss = -8.61393640, time/batch = 1.338, learning rate = 0.00000101\n",
      "8193/85400 (epoch 19), train_loss = -8.61541767, time/batch = 1.331, learning rate = 0.00000101\n",
      "8203/85400 (epoch 19), train_loss = -8.61626500, time/batch = 1.317, learning rate = 0.00000075\n",
      "8213/85400 (epoch 19), train_loss = -8.61786269, time/batch = 1.335, learning rate = 0.00000075\n",
      "8223/85400 (epoch 19), train_loss = -8.61814488, time/batch = 1.329, learning rate = 0.00000075\n",
      "8233/85400 (epoch 19), train_loss = -8.61563872, time/batch = 1.325, learning rate = 0.00000075\n",
      "8243/85400 (epoch 19), train_loss = -8.61612643, time/batch = 1.342, learning rate = 0.00000075\n",
      "8253/85400 (epoch 19), train_loss = -8.61707995, time/batch = 1.327, learning rate = 0.00000075\n",
      "8263/85400 (epoch 19), train_loss = -8.61439118, time/batch = 1.326, learning rate = 0.00000075\n",
      "8273/85400 (epoch 19), train_loss = -8.61541601, time/batch = 1.335, learning rate = 0.00000075\n",
      "8283/85400 (epoch 19), train_loss = -8.61564838, time/batch = 1.325, learning rate = 0.00000075\n",
      "8293/85400 (epoch 19), train_loss = -8.61746530, time/batch = 1.336, learning rate = 0.00000075\n",
      "8303/85400 (epoch 19), train_loss = -8.61731870, time/batch = 1.340, learning rate = 0.00000075\n",
      "8313/85400 (epoch 19), train_loss = -8.61773941, time/batch = 1.370, learning rate = 0.00000075\n",
      "8323/85400 (epoch 19), train_loss = -8.61571846, time/batch = 1.349, learning rate = 0.00000075\n",
      "8333/85400 (epoch 19), train_loss = -8.61639367, time/batch = 1.335, learning rate = 0.00000075\n",
      "8343/85400 (epoch 19), train_loss = -8.61472853, time/batch = 1.325, learning rate = 0.00000075\n",
      "8353/85400 (epoch 19), train_loss = -8.61581140, time/batch = 1.321, learning rate = 0.00000075\n",
      "8363/85400 (epoch 19), train_loss = -8.61566131, time/batch = 1.311, learning rate = 0.00000075\n",
      "8373/85400 (epoch 19), train_loss = -8.61528084, time/batch = 1.322, learning rate = 0.00000075\n",
      "8383/85400 (epoch 19), train_loss = -8.61501165, time/batch = 1.318, learning rate = 0.00000075\n",
      "8393/85400 (epoch 19), train_loss = -8.61525575, time/batch = 1.321, learning rate = 0.00000075\n",
      "8403/85400 (epoch 19), train_loss = -8.61583531, time/batch = 1.321, learning rate = 0.00000057\n",
      "8413/85400 (epoch 19), train_loss = -8.61530720, time/batch = 1.314, learning rate = 0.00000057\n",
      "8423/85400 (epoch 19), train_loss = -8.61533778, time/batch = 1.314, learning rate = 0.00000057\n",
      "8433/85400 (epoch 19), train_loss = -8.61585410, time/batch = 1.323, learning rate = 0.00000057\n",
      "8443/85400 (epoch 19), train_loss = -8.61564042, time/batch = 1.321, learning rate = 0.00000057\n",
      "8453/85400 (epoch 19), train_loss = -8.61608509, time/batch = 1.314, learning rate = 0.00000057\n",
      "8463/85400 (epoch 19), train_loss = -8.61608711, time/batch = 1.314, learning rate = 0.00000057\n",
      "8473/85400 (epoch 19), train_loss = -8.61606547, time/batch = 1.329, learning rate = 0.00000057\n",
      "8483/85400 (epoch 19), train_loss = -8.61640931, time/batch = 1.318, learning rate = 0.00000057\n",
      "8493/85400 (epoch 19), train_loss = -8.61654126, time/batch = 1.316, learning rate = 0.00000057\n",
      "Saving model to model-19-8500.pth\n",
      "8503/85400 (epoch 19), train_loss = -8.61692040, time/batch = 1.311, learning rate = 0.00000057\n",
      "8513/85400 (epoch 19), train_loss = -8.61759778, time/batch = 1.361, learning rate = 0.00000057\n",
      "8523/85400 (epoch 19), train_loss = -8.61692071, time/batch = 1.330, learning rate = 0.00000057\n",
      "8533/85400 (epoch 19), train_loss = -8.61642140, time/batch = 1.361, learning rate = 0.00000057\n",
      "8550/85400 (epoch 20), train_loss = -8.57835474, time/batch = 1.382, learning rate = 0.00000057\n",
      "8560/85400 (epoch 20), train_loss = -8.59932284, time/batch = 1.337, learning rate = 0.00000057\n",
      "8570/85400 (epoch 20), train_loss = -8.60557779, time/batch = 1.326, learning rate = 0.00000057\n",
      "8580/85400 (epoch 20), train_loss = -8.60803273, time/batch = 1.322, learning rate = 0.00000057\n",
      "8590/85400 (epoch 20), train_loss = -8.61209755, time/batch = 1.321, learning rate = 0.00000057\n",
      "8600/85400 (epoch 20), train_loss = -8.61196810, time/batch = 1.331, learning rate = 0.00000057\n",
      "8610/85400 (epoch 20), train_loss = -8.61306697, time/batch = 1.325, learning rate = 0.00000042\n",
      "8620/85400 (epoch 20), train_loss = -8.61394286, time/batch = 1.323, learning rate = 0.00000042\n",
      "8630/85400 (epoch 20), train_loss = -8.61195640, time/batch = 1.321, learning rate = 0.00000042\n",
      "8640/85400 (epoch 20), train_loss = -8.61194836, time/batch = 1.315, learning rate = 0.00000042\n",
      "8650/85400 (epoch 20), train_loss = -8.61257870, time/batch = 1.334, learning rate = 0.00000042\n",
      "8660/85400 (epoch 20), train_loss = -8.61381812, time/batch = 1.319, learning rate = 0.00000042\n",
      "8670/85400 (epoch 20), train_loss = -8.61406209, time/batch = 1.322, learning rate = 0.00000042\n",
      "8680/85400 (epoch 20), train_loss = -8.61484132, time/batch = 1.316, learning rate = 0.00000042\n",
      "8690/85400 (epoch 20), train_loss = -8.61516130, time/batch = 1.333, learning rate = 0.00000042\n",
      "8700/85400 (epoch 20), train_loss = -8.61461253, time/batch = 1.319, learning rate = 0.00000042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8710/85400 (epoch 20), train_loss = -8.61388017, time/batch = 1.319, learning rate = 0.00000042\n",
      "8720/85400 (epoch 20), train_loss = -8.61468983, time/batch = 1.320, learning rate = 0.00000042\n",
      "8730/85400 (epoch 20), train_loss = -8.61588215, time/batch = 1.328, learning rate = 0.00000042\n",
      "8740/85400 (epoch 20), train_loss = -8.61483634, time/batch = 1.313, learning rate = 0.00000042\n",
      "8750/85400 (epoch 20), train_loss = -8.61507910, time/batch = 1.327, learning rate = 0.00000042\n",
      "8760/85400 (epoch 20), train_loss = -8.61639262, time/batch = 1.325, learning rate = 0.00000042\n",
      "8770/85400 (epoch 20), train_loss = -8.61701714, time/batch = 1.325, learning rate = 0.00000042\n",
      "8780/85400 (epoch 20), train_loss = -8.61760639, time/batch = 1.315, learning rate = 0.00000042\n",
      "8790/85400 (epoch 20), train_loss = -8.61787378, time/batch = 1.320, learning rate = 0.00000042\n",
      "8800/85400 (epoch 20), train_loss = -8.61756947, time/batch = 1.313, learning rate = 0.00000042\n",
      "8810/85400 (epoch 20), train_loss = -8.61708026, time/batch = 1.316, learning rate = 0.00000032\n",
      "8820/85400 (epoch 20), train_loss = -8.61792833, time/batch = 1.315, learning rate = 0.00000032\n",
      "8830/85400 (epoch 20), train_loss = -8.61785413, time/batch = 1.322, learning rate = 0.00000032\n",
      "8840/85400 (epoch 20), train_loss = -8.61803143, time/batch = 1.329, learning rate = 0.00000032\n",
      "8850/85400 (epoch 20), train_loss = -8.61725767, time/batch = 1.324, learning rate = 0.00000032\n",
      "8860/85400 (epoch 20), train_loss = -8.61739105, time/batch = 1.321, learning rate = 0.00000032\n",
      "8870/85400 (epoch 20), train_loss = -8.61738714, time/batch = 1.330, learning rate = 0.00000032\n",
      "8880/85400 (epoch 20), train_loss = -8.61651645, time/batch = 1.324, learning rate = 0.00000032\n",
      "8890/85400 (epoch 20), train_loss = -8.61608553, time/batch = 1.332, learning rate = 0.00000032\n",
      "8900/85400 (epoch 20), train_loss = -8.61590614, time/batch = 1.332, learning rate = 0.00000032\n",
      "8910/85400 (epoch 20), train_loss = -8.61593788, time/batch = 1.323, learning rate = 0.00000032\n",
      "8920/85400 (epoch 20), train_loss = -8.61622425, time/batch = 1.329, learning rate = 0.00000032\n",
      "8930/85400 (epoch 20), train_loss = -8.61600338, time/batch = 1.329, learning rate = 0.00000032\n",
      "8940/85400 (epoch 20), train_loss = -8.61602794, time/batch = 1.326, learning rate = 0.00000032\n",
      "8950/85400 (epoch 20), train_loss = -8.61579454, time/batch = 1.321, learning rate = 0.00000032\n",
      "8960/85400 (epoch 20), train_loss = -8.61577278, time/batch = 1.323, learning rate = 0.00000032\n",
      "8977/85400 (epoch 21), train_loss = -8.61176815, time/batch = 1.324, learning rate = 0.00000032\n",
      "8987/85400 (epoch 21), train_loss = -8.62063785, time/batch = 1.321, learning rate = 0.00000032\n",
      "8997/85400 (epoch 21), train_loss = -8.61998250, time/batch = 1.312, learning rate = 0.00000032\n",
      "Saving model to model-21-9000.pth\n",
      "9007/85400 (epoch 21), train_loss = -8.62224312, time/batch = 1.313, learning rate = 0.00000024\n",
      "9017/85400 (epoch 21), train_loss = -8.62121191, time/batch = 1.320, learning rate = 0.00000024\n",
      "9027/85400 (epoch 21), train_loss = -8.62016710, time/batch = 1.320, learning rate = 0.00000024\n",
      "9037/85400 (epoch 21), train_loss = -8.62257275, time/batch = 1.312, learning rate = 0.00000024\n",
      "9047/85400 (epoch 21), train_loss = -8.62179686, time/batch = 1.328, learning rate = 0.00000024\n",
      "9057/85400 (epoch 21), train_loss = -8.62282302, time/batch = 1.415, learning rate = 0.00000024\n",
      "9067/85400 (epoch 21), train_loss = -8.62324442, time/batch = 1.325, learning rate = 0.00000024\n",
      "9077/85400 (epoch 21), train_loss = -8.62295693, time/batch = 1.323, learning rate = 0.00000024\n",
      "9087/85400 (epoch 21), train_loss = -8.62282024, time/batch = 1.319, learning rate = 0.00000024\n",
      "9097/85400 (epoch 21), train_loss = -8.62289289, time/batch = 1.328, learning rate = 0.00000024\n",
      "9107/85400 (epoch 21), train_loss = -8.62338867, time/batch = 1.315, learning rate = 0.00000024\n",
      "9117/85400 (epoch 21), train_loss = -8.62204186, time/batch = 1.319, learning rate = 0.00000024\n",
      "9127/85400 (epoch 21), train_loss = -8.62053932, time/batch = 1.326, learning rate = 0.00000024\n",
      "9137/85400 (epoch 21), train_loss = -8.61771535, time/batch = 1.312, learning rate = 0.00000024\n",
      "9147/85400 (epoch 21), train_loss = -8.61473056, time/batch = 1.314, learning rate = 0.00000024\n",
      "9157/85400 (epoch 21), train_loss = -8.61520399, time/batch = 1.327, learning rate = 0.00000024\n",
      "9167/85400 (epoch 21), train_loss = -8.61551708, time/batch = 1.317, learning rate = 0.00000024\n",
      "9177/85400 (epoch 21), train_loss = -8.61673711, time/batch = 1.323, learning rate = 0.00000024\n",
      "9187/85400 (epoch 21), train_loss = -8.61652992, time/batch = 1.320, learning rate = 0.00000024\n",
      "9197/85400 (epoch 21), train_loss = -8.61453821, time/batch = 1.312, learning rate = 0.00000024\n",
      "9207/85400 (epoch 21), train_loss = -8.61491488, time/batch = 1.320, learning rate = 0.00000018\n",
      "9217/85400 (epoch 21), train_loss = -8.61470382, time/batch = 1.328, learning rate = 0.00000018\n",
      "9227/85400 (epoch 21), train_loss = -8.61444172, time/batch = 1.326, learning rate = 0.00000018\n",
      "9237/85400 (epoch 21), train_loss = -8.61442145, time/batch = 1.320, learning rate = 0.00000018\n",
      "9247/85400 (epoch 21), train_loss = -8.61529696, time/batch = 1.323, learning rate = 0.00000018\n",
      "9257/85400 (epoch 21), train_loss = -8.61548400, time/batch = 1.317, learning rate = 0.00000018\n",
      "9267/85400 (epoch 21), train_loss = -8.61605013, time/batch = 1.326, learning rate = 0.00000018\n",
      "9277/85400 (epoch 21), train_loss = -8.61632222, time/batch = 1.322, learning rate = 0.00000018\n",
      "9287/85400 (epoch 21), train_loss = -8.61617551, time/batch = 1.324, learning rate = 0.00000018\n",
      "9297/85400 (epoch 21), train_loss = -8.61638929, time/batch = 1.323, learning rate = 0.00000018\n",
      "9307/85400 (epoch 21), train_loss = -8.61588142, time/batch = 1.326, learning rate = 0.00000018\n",
      "9317/85400 (epoch 21), train_loss = -8.61593903, time/batch = 1.330, learning rate = 0.00000018\n",
      "9327/85400 (epoch 21), train_loss = -8.61572776, time/batch = 1.320, learning rate = 0.00000018\n",
      "9337/85400 (epoch 21), train_loss = -8.61585001, time/batch = 1.326, learning rate = 0.00000018\n",
      "9347/85400 (epoch 21), train_loss = -8.61568058, time/batch = 1.322, learning rate = 0.00000018\n",
      "9357/85400 (epoch 21), train_loss = -8.61560653, time/batch = 1.319, learning rate = 0.00000018\n",
      "9367/85400 (epoch 21), train_loss = -8.61634166, time/batch = 1.317, learning rate = 0.00000018\n",
      "9377/85400 (epoch 21), train_loss = -8.61643974, time/batch = 1.321, learning rate = 0.00000018\n",
      "9387/85400 (epoch 21), train_loss = -8.61646984, time/batch = 1.323, learning rate = 0.00000018\n",
      "9404/85400 (epoch 22), train_loss = -8.60418510, time/batch = 1.316, learning rate = 0.00000013\n",
      "9414/85400 (epoch 22), train_loss = -8.61491075, time/batch = 1.320, learning rate = 0.00000013\n",
      "9424/85400 (epoch 22), train_loss = -8.61262169, time/batch = 1.329, learning rate = 0.00000013\n",
      "9434/85400 (epoch 22), train_loss = -8.60986147, time/batch = 1.323, learning rate = 0.00000013\n",
      "9444/85400 (epoch 22), train_loss = -8.61051165, time/batch = 1.324, learning rate = 0.00000013\n",
      "9454/85400 (epoch 22), train_loss = -8.61085534, time/batch = 1.318, learning rate = 0.00000013\n",
      "9464/85400 (epoch 22), train_loss = -8.61035433, time/batch = 1.323, learning rate = 0.00000013\n",
      "9474/85400 (epoch 22), train_loss = -8.61230077, time/batch = 1.320, learning rate = 0.00000013\n",
      "9484/85400 (epoch 22), train_loss = -8.61224642, time/batch = 1.333, learning rate = 0.00000013\n",
      "9494/85400 (epoch 22), train_loss = -8.61390186, time/batch = 1.315, learning rate = 0.00000013\n",
      "Saving model to model-22-9500.pth\n",
      "9504/85400 (epoch 22), train_loss = -8.61499083, time/batch = 1.310, learning rate = 0.00000013\n",
      "9514/85400 (epoch 22), train_loss = -8.61463235, time/batch = 1.315, learning rate = 0.00000013\n",
      "9524/85400 (epoch 22), train_loss = -8.61491018, time/batch = 1.326, learning rate = 0.00000013\n",
      "9534/85400 (epoch 22), train_loss = -8.61487624, time/batch = 1.320, learning rate = 0.00000013\n",
      "9544/85400 (epoch 22), train_loss = -8.61480530, time/batch = 1.319, learning rate = 0.00000013\n",
      "9554/85400 (epoch 22), train_loss = -8.61622192, time/batch = 1.320, learning rate = 0.00000013\n",
      "9564/85400 (epoch 22), train_loss = -8.61601399, time/batch = 1.314, learning rate = 0.00000013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9574/85400 (epoch 22), train_loss = -8.61535499, time/batch = 1.311, learning rate = 0.00000013\n",
      "9584/85400 (epoch 22), train_loss = -8.61536023, time/batch = 1.322, learning rate = 0.00000013\n",
      "9594/85400 (epoch 22), train_loss = -8.61587519, time/batch = 1.317, learning rate = 0.00000013\n",
      "9604/85400 (epoch 22), train_loss = -8.61614395, time/batch = 1.315, learning rate = 0.00000010\n",
      "9614/85400 (epoch 22), train_loss = -8.61702538, time/batch = 1.322, learning rate = 0.00000010\n",
      "9624/85400 (epoch 22), train_loss = -8.61733136, time/batch = 1.325, learning rate = 0.00000010\n",
      "9634/85400 (epoch 22), train_loss = -8.61754144, time/batch = 1.316, learning rate = 0.00000010\n",
      "9644/85400 (epoch 22), train_loss = -8.61688707, time/batch = 1.329, learning rate = 0.00000010\n",
      "9654/85400 (epoch 22), train_loss = -8.61670218, time/batch = 1.321, learning rate = 0.00000010\n",
      "9664/85400 (epoch 22), train_loss = -8.61580339, time/batch = 1.317, learning rate = 0.00000010\n",
      "9674/85400 (epoch 22), train_loss = -8.61660929, time/batch = 1.333, learning rate = 0.00000010\n",
      "9684/85400 (epoch 22), train_loss = -8.61669577, time/batch = 1.324, learning rate = 0.00000010\n",
      "9694/85400 (epoch 22), train_loss = -8.61651073, time/batch = 1.315, learning rate = 0.00000010\n",
      "9704/85400 (epoch 22), train_loss = -8.61680256, time/batch = 1.322, learning rate = 0.00000010\n",
      "9714/85400 (epoch 22), train_loss = -8.61591284, time/batch = 1.324, learning rate = 0.00000010\n",
      "9724/85400 (epoch 22), train_loss = -8.61667478, time/batch = 1.325, learning rate = 0.00000010\n",
      "9734/85400 (epoch 22), train_loss = -8.61717781, time/batch = 1.315, learning rate = 0.00000010\n",
      "9744/85400 (epoch 22), train_loss = -8.61771539, time/batch = 1.315, learning rate = 0.00000010\n",
      "9754/85400 (epoch 22), train_loss = -8.61762891, time/batch = 1.308, learning rate = 0.00000010\n",
      "9764/85400 (epoch 22), train_loss = -8.61658813, time/batch = 1.315, learning rate = 0.00000010\n",
      "9774/85400 (epoch 22), train_loss = -8.61682447, time/batch = 1.310, learning rate = 0.00000010\n",
      "9784/85400 (epoch 22), train_loss = -8.61719242, time/batch = 1.326, learning rate = 0.00000010\n",
      "9794/85400 (epoch 22), train_loss = -8.61658240, time/batch = 1.317, learning rate = 0.00000010\n",
      "9804/85400 (epoch 22), train_loss = -8.61668314, time/batch = 1.319, learning rate = 0.00000008\n",
      "9814/85400 (epoch 22), train_loss = -8.61714021, time/batch = 1.319, learning rate = 0.00000008\n",
      "9831/85400 (epoch 23), train_loss = -8.64419003, time/batch = 1.323, learning rate = 0.00000008\n",
      "9841/85400 (epoch 23), train_loss = -8.62223415, time/batch = 1.317, learning rate = 0.00000008\n",
      "9851/85400 (epoch 23), train_loss = -8.61738656, time/batch = 1.318, learning rate = 0.00000008\n",
      "9861/85400 (epoch 23), train_loss = -8.62011354, time/batch = 1.320, learning rate = 0.00000008\n",
      "9871/85400 (epoch 23), train_loss = -8.61977087, time/batch = 1.309, learning rate = 0.00000008\n",
      "9881/85400 (epoch 23), train_loss = -8.61673697, time/batch = 1.321, learning rate = 0.00000008\n",
      "9891/85400 (epoch 23), train_loss = -8.61839198, time/batch = 1.320, learning rate = 0.00000008\n",
      "9901/85400 (epoch 23), train_loss = -8.61908708, time/batch = 1.315, learning rate = 0.00000008\n",
      "9911/85400 (epoch 23), train_loss = -8.61773160, time/batch = 1.314, learning rate = 0.00000008\n",
      "9921/85400 (epoch 23), train_loss = -8.61901293, time/batch = 1.327, learning rate = 0.00000008\n",
      "9931/85400 (epoch 23), train_loss = -8.61699526, time/batch = 1.323, learning rate = 0.00000008\n",
      "9941/85400 (epoch 23), train_loss = -8.61712701, time/batch = 1.320, learning rate = 0.00000008\n",
      "9951/85400 (epoch 23), train_loss = -8.61715813, time/batch = 1.323, learning rate = 0.00000008\n",
      "9961/85400 (epoch 23), train_loss = -8.61547112, time/batch = 1.325, learning rate = 0.00000008\n",
      "9971/85400 (epoch 23), train_loss = -8.61534858, time/batch = 1.315, learning rate = 0.00000008\n",
      "9981/85400 (epoch 23), train_loss = -8.61526043, time/batch = 1.310, learning rate = 0.00000008\n",
      "9991/85400 (epoch 23), train_loss = -8.61601558, time/batch = 1.324, learning rate = 0.00000008\n",
      "Saving model to model-23-10000.pth\n",
      "10001/85400 (epoch 23), train_loss = -8.61601452, time/batch = 1.314, learning rate = 0.00000008\n",
      "10011/85400 (epoch 23), train_loss = -8.61664745, time/batch = 1.313, learning rate = 0.00000008\n",
      "10021/85400 (epoch 23), train_loss = -8.61768125, time/batch = 1.315, learning rate = 0.00000008\n",
      "10031/85400 (epoch 23), train_loss = -8.61697976, time/batch = 1.318, learning rate = 0.00000008\n",
      "10041/85400 (epoch 23), train_loss = -8.61669162, time/batch = 1.321, learning rate = 0.00000008\n",
      "10051/85400 (epoch 23), train_loss = -8.61765089, time/batch = 1.314, learning rate = 0.00000008\n",
      "10061/85400 (epoch 23), train_loss = -8.61853631, time/batch = 1.325, learning rate = 0.00000008\n",
      "10071/85400 (epoch 23), train_loss = -8.61883971, time/batch = 1.314, learning rate = 0.00000008\n",
      "10081/85400 (epoch 23), train_loss = -8.61948377, time/batch = 1.316, learning rate = 0.00000008\n",
      "10091/85400 (epoch 23), train_loss = -8.61874949, time/batch = 1.324, learning rate = 0.00000008\n",
      "10101/85400 (epoch 23), train_loss = -8.61881705, time/batch = 1.316, learning rate = 0.00000008\n",
      "10111/85400 (epoch 23), train_loss = -8.61978796, time/batch = 1.324, learning rate = 0.00000008\n",
      "10121/85400 (epoch 23), train_loss = -8.61946388, time/batch = 1.327, learning rate = 0.00000008\n",
      "10131/85400 (epoch 23), train_loss = -8.61939103, time/batch = 1.324, learning rate = 0.00000008\n",
      "10141/85400 (epoch 23), train_loss = -8.61927120, time/batch = 1.320, learning rate = 0.00000008\n",
      "10151/85400 (epoch 23), train_loss = -8.61911851, time/batch = 1.326, learning rate = 0.00000008\n",
      "10161/85400 (epoch 23), train_loss = -8.61889812, time/batch = 1.319, learning rate = 0.00000008\n",
      "10171/85400 (epoch 23), train_loss = -8.61934434, time/batch = 1.314, learning rate = 0.00000008\n",
      "10181/85400 (epoch 23), train_loss = -8.61849366, time/batch = 1.307, learning rate = 0.00000008\n",
      "10191/85400 (epoch 23), train_loss = -8.61886498, time/batch = 1.322, learning rate = 0.00000008\n",
      "10201/85400 (epoch 23), train_loss = -8.61912062, time/batch = 1.318, learning rate = 0.00000008\n",
      "10211/85400 (epoch 23), train_loss = -8.61937507, time/batch = 1.312, learning rate = 0.00000008\n",
      "10221/85400 (epoch 23), train_loss = -8.61947032, time/batch = 1.322, learning rate = 0.00000008\n",
      "10231/85400 (epoch 23), train_loss = -8.61953368, time/batch = 1.324, learning rate = 0.00000008\n",
      "10241/85400 (epoch 23), train_loss = -8.61906726, time/batch = 1.313, learning rate = 0.00000008\n",
      "10258/85400 (epoch 24), train_loss = -8.63409863, time/batch = 1.311, learning rate = 0.00000008\n",
      "10268/85400 (epoch 24), train_loss = -8.61603670, time/batch = 1.318, learning rate = 0.00000008\n",
      "10278/85400 (epoch 24), train_loss = -8.62284406, time/batch = 1.323, learning rate = 0.00000008\n",
      "10288/85400 (epoch 24), train_loss = -8.62333913, time/batch = 1.312, learning rate = 0.00000008\n",
      "10298/85400 (epoch 24), train_loss = -8.62586353, time/batch = 1.316, learning rate = 0.00000008\n",
      "10308/85400 (epoch 24), train_loss = -8.62568326, time/batch = 1.313, learning rate = 0.00000008\n",
      "10318/85400 (epoch 24), train_loss = -8.62648227, time/batch = 1.321, learning rate = 0.00000008\n",
      "10328/85400 (epoch 24), train_loss = -8.62568099, time/batch = 1.332, learning rate = 0.00000008\n",
      "10338/85400 (epoch 24), train_loss = -8.62377680, time/batch = 1.328, learning rate = 0.00000008\n",
      "10348/85400 (epoch 24), train_loss = -8.61955932, time/batch = 1.324, learning rate = 0.00000008\n",
      "10358/85400 (epoch 24), train_loss = -8.61976678, time/batch = 1.326, learning rate = 0.00000008\n",
      "10368/85400 (epoch 24), train_loss = -8.61941553, time/batch = 1.311, learning rate = 0.00000008\n",
      "10378/85400 (epoch 24), train_loss = -8.62025708, time/batch = 1.317, learning rate = 0.00000008\n",
      "10388/85400 (epoch 24), train_loss = -8.61963551, time/batch = 1.332, learning rate = 0.00000008\n",
      "10398/85400 (epoch 24), train_loss = -8.62120753, time/batch = 1.326, learning rate = 0.00000008\n",
      "10408/85400 (epoch 24), train_loss = -8.61940721, time/batch = 1.318, learning rate = 0.00000008\n",
      "10418/85400 (epoch 24), train_loss = -8.61991915, time/batch = 1.334, learning rate = 0.00000008\n",
      "10428/85400 (epoch 24), train_loss = -8.61853292, time/batch = 1.321, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10438/85400 (epoch 24), train_loss = -8.61900254, time/batch = 1.327, learning rate = 0.00000008\n",
      "10448/85400 (epoch 24), train_loss = -8.61778506, time/batch = 1.313, learning rate = 0.00000008\n",
      "10458/85400 (epoch 24), train_loss = -8.61828826, time/batch = 1.313, learning rate = 0.00000008\n",
      "10468/85400 (epoch 24), train_loss = -8.61817500, time/batch = 1.313, learning rate = 0.00000008\n",
      "10478/85400 (epoch 24), train_loss = -8.61857091, time/batch = 1.316, learning rate = 0.00000008\n",
      "10488/85400 (epoch 24), train_loss = -8.61941245, time/batch = 1.320, learning rate = 0.00000008\n",
      "10498/85400 (epoch 24), train_loss = -8.61840782, time/batch = 1.312, learning rate = 0.00000008\n",
      "Saving model to model-24-10500.pth\n",
      "10508/85400 (epoch 24), train_loss = -8.61788344, time/batch = 1.319, learning rate = 0.00000008\n",
      "10518/85400 (epoch 24), train_loss = -8.61780055, time/batch = 1.324, learning rate = 0.00000008\n",
      "10528/85400 (epoch 24), train_loss = -8.61861413, time/batch = 1.322, learning rate = 0.00000008\n",
      "10538/85400 (epoch 24), train_loss = -8.61845258, time/batch = 1.317, learning rate = 0.00000008\n",
      "10548/85400 (epoch 24), train_loss = -8.61921706, time/batch = 1.324, learning rate = 0.00000008\n",
      "10558/85400 (epoch 24), train_loss = -8.61901662, time/batch = 1.313, learning rate = 0.00000008\n",
      "10568/85400 (epoch 24), train_loss = -8.61910011, time/batch = 1.312, learning rate = 0.00000008\n",
      "10578/85400 (epoch 24), train_loss = -8.61899039, time/batch = 1.321, learning rate = 0.00000008\n",
      "10588/85400 (epoch 24), train_loss = -8.61810067, time/batch = 1.319, learning rate = 0.00000008\n",
      "10598/85400 (epoch 24), train_loss = -8.61706618, time/batch = 1.310, learning rate = 0.00000008\n",
      "10608/85400 (epoch 24), train_loss = -8.61661900, time/batch = 1.313, learning rate = 0.00000008\n",
      "10618/85400 (epoch 24), train_loss = -8.61688427, time/batch = 1.319, learning rate = 0.00000008\n",
      "10628/85400 (epoch 24), train_loss = -8.61720914, time/batch = 1.317, learning rate = 0.00000008\n",
      "10638/85400 (epoch 24), train_loss = -8.61752520, time/batch = 1.317, learning rate = 0.00000008\n",
      "10648/85400 (epoch 24), train_loss = -8.61697993, time/batch = 1.320, learning rate = 0.00000008\n",
      "10658/85400 (epoch 24), train_loss = -8.61705265, time/batch = 1.320, learning rate = 0.00000008\n",
      "10668/85400 (epoch 24), train_loss = -8.61681009, time/batch = 1.315, learning rate = 0.00000008\n",
      "10685/85400 (epoch 25), train_loss = -8.57023716, time/batch = 1.322, learning rate = 0.00000008\n",
      "10695/85400 (epoch 25), train_loss = -8.60197811, time/batch = 1.320, learning rate = 0.00000008\n",
      "10705/85400 (epoch 25), train_loss = -8.61237265, time/batch = 1.318, learning rate = 0.00000008\n",
      "10715/85400 (epoch 25), train_loss = -8.61731012, time/batch = 1.318, learning rate = 0.00000008\n",
      "10725/85400 (epoch 25), train_loss = -8.61000725, time/batch = 1.308, learning rate = 0.00000008\n",
      "10735/85400 (epoch 25), train_loss = -8.60738446, time/batch = 1.318, learning rate = 0.00000008\n",
      "10745/85400 (epoch 25), train_loss = -8.60852959, time/batch = 1.328, learning rate = 0.00000008\n",
      "10755/85400 (epoch 25), train_loss = -8.60786489, time/batch = 1.318, learning rate = 0.00000008\n",
      "10765/85400 (epoch 25), train_loss = -8.60820467, time/batch = 1.313, learning rate = 0.00000008\n",
      "10775/85400 (epoch 25), train_loss = -8.61109329, time/batch = 1.312, learning rate = 0.00000008\n",
      "10785/85400 (epoch 25), train_loss = -8.61248053, time/batch = 1.320, learning rate = 0.00000008\n",
      "10795/85400 (epoch 25), train_loss = -8.61351177, time/batch = 1.320, learning rate = 0.00000008\n",
      "10805/85400 (epoch 25), train_loss = -8.60929609, time/batch = 1.324, learning rate = 0.00000008\n",
      "10815/85400 (epoch 25), train_loss = -8.60888329, time/batch = 1.323, learning rate = 0.00000008\n",
      "10825/85400 (epoch 25), train_loss = -8.61058183, time/batch = 1.323, learning rate = 0.00000008\n",
      "10835/85400 (epoch 25), train_loss = -8.61185995, time/batch = 1.325, learning rate = 0.00000008\n",
      "10845/85400 (epoch 25), train_loss = -8.61041118, time/batch = 1.325, learning rate = 0.00000008\n",
      "10855/85400 (epoch 25), train_loss = -8.61089406, time/batch = 1.313, learning rate = 0.00000008\n",
      "10865/85400 (epoch 25), train_loss = -8.61076302, time/batch = 1.314, learning rate = 0.00000008\n",
      "10875/85400 (epoch 25), train_loss = -8.61067578, time/batch = 1.313, learning rate = 0.00000008\n",
      "10885/85400 (epoch 25), train_loss = -8.61186348, time/batch = 1.319, learning rate = 0.00000008\n",
      "10895/85400 (epoch 25), train_loss = -8.61245971, time/batch = 1.318, learning rate = 0.00000008\n",
      "10905/85400 (epoch 25), train_loss = -8.61282747, time/batch = 1.322, learning rate = 0.00000008\n",
      "10915/85400 (epoch 25), train_loss = -8.61351337, time/batch = 1.317, learning rate = 0.00000008\n",
      "10925/85400 (epoch 25), train_loss = -8.61393163, time/batch = 1.316, learning rate = 0.00000008\n",
      "10935/85400 (epoch 25), train_loss = -8.61413881, time/batch = 1.320, learning rate = 0.00000008\n",
      "10945/85400 (epoch 25), train_loss = -8.61445030, time/batch = 1.324, learning rate = 0.00000008\n",
      "10955/85400 (epoch 25), train_loss = -8.61430121, time/batch = 1.315, learning rate = 0.00000008\n",
      "10965/85400 (epoch 25), train_loss = -8.61523289, time/batch = 1.321, learning rate = 0.00000008\n",
      "10975/85400 (epoch 25), train_loss = -8.61543611, time/batch = 1.319, learning rate = 0.00000008\n",
      "10985/85400 (epoch 25), train_loss = -8.61520714, time/batch = 1.322, learning rate = 0.00000008\n",
      "10995/85400 (epoch 25), train_loss = -8.61537989, time/batch = 1.315, learning rate = 0.00000008\n",
      "Saving model to model-25-11000.pth\n",
      "11005/85400 (epoch 25), train_loss = -8.61507978, time/batch = 1.320, learning rate = 0.00000008\n",
      "11015/85400 (epoch 25), train_loss = -8.61506442, time/batch = 1.318, learning rate = 0.00000008\n",
      "11025/85400 (epoch 25), train_loss = -8.61541125, time/batch = 1.324, learning rate = 0.00000008\n",
      "11035/85400 (epoch 25), train_loss = -8.61517486, time/batch = 1.323, learning rate = 0.00000008\n",
      "11045/85400 (epoch 25), train_loss = -8.61574371, time/batch = 1.317, learning rate = 0.00000008\n",
      "11055/85400 (epoch 25), train_loss = -8.61550639, time/batch = 1.318, learning rate = 0.00000008\n",
      "11065/85400 (epoch 25), train_loss = -8.61563841, time/batch = 1.321, learning rate = 0.00000008\n",
      "11075/85400 (epoch 25), train_loss = -8.61597688, time/batch = 1.316, learning rate = 0.00000008\n",
      "11085/85400 (epoch 25), train_loss = -8.61617257, time/batch = 1.312, learning rate = 0.00000008\n",
      "11095/85400 (epoch 25), train_loss = -8.61598369, time/batch = 1.321, learning rate = 0.00000008\n",
      "11112/85400 (epoch 26), train_loss = -8.61360168, time/batch = 1.318, learning rate = 0.00000008\n",
      "11122/85400 (epoch 26), train_loss = -8.60940084, time/batch = 1.329, learning rate = 0.00000008\n",
      "11132/85400 (epoch 26), train_loss = -8.61414709, time/batch = 1.314, learning rate = 0.00000008\n",
      "11142/85400 (epoch 26), train_loss = -8.61592271, time/batch = 1.315, learning rate = 0.00000008\n",
      "11152/85400 (epoch 26), train_loss = -8.61669285, time/batch = 1.317, learning rate = 0.00000008\n",
      "11162/85400 (epoch 26), train_loss = -8.61990573, time/batch = 1.319, learning rate = 0.00000008\n",
      "11172/85400 (epoch 26), train_loss = -8.62048552, time/batch = 1.319, learning rate = 0.00000008\n",
      "11182/85400 (epoch 26), train_loss = -8.61842102, time/batch = 1.315, learning rate = 0.00000008\n",
      "11192/85400 (epoch 26), train_loss = -8.61806749, time/batch = 1.325, learning rate = 0.00000008\n",
      "11202/85400 (epoch 26), train_loss = -8.61828040, time/batch = 1.319, learning rate = 0.00000008\n",
      "11212/85400 (epoch 26), train_loss = -8.61987342, time/batch = 1.313, learning rate = 0.00000008\n",
      "11222/85400 (epoch 26), train_loss = -8.61966936, time/batch = 1.320, learning rate = 0.00000008\n",
      "11232/85400 (epoch 26), train_loss = -8.62060983, time/batch = 1.325, learning rate = 0.00000008\n",
      "11242/85400 (epoch 26), train_loss = -8.62163077, time/batch = 1.320, learning rate = 0.00000008\n",
      "11252/85400 (epoch 26), train_loss = -8.62117648, time/batch = 1.317, learning rate = 0.00000008\n",
      "11262/85400 (epoch 26), train_loss = -8.62141742, time/batch = 1.314, learning rate = 0.00000008\n",
      "11272/85400 (epoch 26), train_loss = -8.62060121, time/batch = 1.323, learning rate = 0.00000008\n",
      "11282/85400 (epoch 26), train_loss = -8.62001378, time/batch = 1.322, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11292/85400 (epoch 26), train_loss = -8.61767863, time/batch = 1.328, learning rate = 0.00000008\n",
      "11302/85400 (epoch 26), train_loss = -8.61673747, time/batch = 1.321, learning rate = 0.00000008\n",
      "11312/85400 (epoch 26), train_loss = -8.61753404, time/batch = 1.326, learning rate = 0.00000008\n",
      "11322/85400 (epoch 26), train_loss = -8.61856110, time/batch = 1.326, learning rate = 0.00000008\n",
      "11332/85400 (epoch 26), train_loss = -8.61792996, time/batch = 1.319, learning rate = 0.00000008\n",
      "11342/85400 (epoch 26), train_loss = -8.61578670, time/batch = 1.326, learning rate = 0.00000008\n",
      "11352/85400 (epoch 26), train_loss = -8.61546994, time/batch = 1.315, learning rate = 0.00000008\n",
      "11362/85400 (epoch 26), train_loss = -8.61566848, time/batch = 1.316, learning rate = 0.00000008\n",
      "11372/85400 (epoch 26), train_loss = -8.61578781, time/batch = 1.325, learning rate = 0.00000008\n",
      "11382/85400 (epoch 26), train_loss = -8.61661468, time/batch = 1.316, learning rate = 0.00000008\n",
      "11392/85400 (epoch 26), train_loss = -8.61723611, time/batch = 1.324, learning rate = 0.00000008\n",
      "11402/85400 (epoch 26), train_loss = -8.61751896, time/batch = 1.326, learning rate = 0.00000008\n",
      "11412/85400 (epoch 26), train_loss = -8.61818227, time/batch = 1.321, learning rate = 0.00000008\n",
      "11422/85400 (epoch 26), train_loss = -8.61747429, time/batch = 1.321, learning rate = 0.00000008\n",
      "11432/85400 (epoch 26), train_loss = -8.61778471, time/batch = 1.319, learning rate = 0.00000008\n",
      "11442/85400 (epoch 26), train_loss = -8.61779683, time/batch = 1.311, learning rate = 0.00000008\n",
      "11452/85400 (epoch 26), train_loss = -8.61694410, time/batch = 1.319, learning rate = 0.00000008\n",
      "11462/85400 (epoch 26), train_loss = -8.61672351, time/batch = 1.323, learning rate = 0.00000008\n",
      "11472/85400 (epoch 26), train_loss = -8.61732803, time/batch = 1.316, learning rate = 0.00000008\n",
      "11482/85400 (epoch 26), train_loss = -8.61731117, time/batch = 1.323, learning rate = 0.00000008\n",
      "11492/85400 (epoch 26), train_loss = -8.61684331, time/batch = 1.318, learning rate = 0.00000008\n",
      "Saving model to model-26-11500.pth\n",
      "11502/85400 (epoch 26), train_loss = -8.61676986, time/batch = 1.315, learning rate = 0.00000008\n",
      "11512/85400 (epoch 26), train_loss = -8.61686132, time/batch = 1.318, learning rate = 0.00000008\n",
      "11522/85400 (epoch 26), train_loss = -8.61719081, time/batch = 1.322, learning rate = 0.00000008\n",
      "11539/85400 (epoch 27), train_loss = -8.64043379, time/batch = 1.320, learning rate = 0.00000008\n",
      "11549/85400 (epoch 27), train_loss = -8.62554049, time/batch = 1.313, learning rate = 0.00000008\n",
      "11559/85400 (epoch 27), train_loss = -8.62560139, time/batch = 1.310, learning rate = 0.00000008\n",
      "11569/85400 (epoch 27), train_loss = -8.62533600, time/batch = 1.327, learning rate = 0.00000008\n",
      "11579/85400 (epoch 27), train_loss = -8.62595976, time/batch = 1.319, learning rate = 0.00000008\n",
      "11589/85400 (epoch 27), train_loss = -8.62528709, time/batch = 1.316, learning rate = 0.00000008\n",
      "11599/85400 (epoch 27), train_loss = -8.62514669, time/batch = 1.317, learning rate = 0.00000008\n",
      "11609/85400 (epoch 27), train_loss = -8.62547332, time/batch = 1.317, learning rate = 0.00000008\n",
      "11619/85400 (epoch 27), train_loss = -8.62636183, time/batch = 1.330, learning rate = 0.00000008\n",
      "11629/85400 (epoch 27), train_loss = -8.62675543, time/batch = 1.328, learning rate = 0.00000008\n",
      "11639/85400 (epoch 27), train_loss = -8.62648726, time/batch = 1.321, learning rate = 0.00000008\n",
      "11649/85400 (epoch 27), train_loss = -8.62679726, time/batch = 1.326, learning rate = 0.00000008\n",
      "11659/85400 (epoch 27), train_loss = -8.62598283, time/batch = 1.328, learning rate = 0.00000008\n",
      "11669/85400 (epoch 27), train_loss = -8.62540648, time/batch = 1.321, learning rate = 0.00000008\n",
      "11679/85400 (epoch 27), train_loss = -8.62582527, time/batch = 1.312, learning rate = 0.00000008\n",
      "11689/85400 (epoch 27), train_loss = -8.62471001, time/batch = 1.325, learning rate = 0.00000008\n",
      "11699/85400 (epoch 27), train_loss = -8.62288097, time/batch = 1.330, learning rate = 0.00000008\n",
      "11709/85400 (epoch 27), train_loss = -8.62271098, time/batch = 1.325, learning rate = 0.00000008\n",
      "11719/85400 (epoch 27), train_loss = -8.62136902, time/batch = 1.322, learning rate = 0.00000008\n",
      "11729/85400 (epoch 27), train_loss = -8.62104787, time/batch = 1.316, learning rate = 0.00000008\n",
      "11739/85400 (epoch 27), train_loss = -8.62071063, time/batch = 1.315, learning rate = 0.00000008\n",
      "11749/85400 (epoch 27), train_loss = -8.61973421, time/batch = 1.321, learning rate = 0.00000008\n",
      "11759/85400 (epoch 27), train_loss = -8.61909925, time/batch = 1.328, learning rate = 0.00000008\n",
      "11769/85400 (epoch 27), train_loss = -8.61818759, time/batch = 1.320, learning rate = 0.00000008\n",
      "11779/85400 (epoch 27), train_loss = -8.61819851, time/batch = 1.319, learning rate = 0.00000008\n",
      "11789/85400 (epoch 27), train_loss = -8.61803134, time/batch = 1.326, learning rate = 0.00000008\n",
      "11799/85400 (epoch 27), train_loss = -8.61858271, time/batch = 1.319, learning rate = 0.00000008\n",
      "11809/85400 (epoch 27), train_loss = -8.61846595, time/batch = 1.321, learning rate = 0.00000008\n",
      "11819/85400 (epoch 27), train_loss = -8.61815406, time/batch = 1.317, learning rate = 0.00000008\n",
      "11829/85400 (epoch 27), train_loss = -8.61862198, time/batch = 1.324, learning rate = 0.00000008\n",
      "11839/85400 (epoch 27), train_loss = -8.61791109, time/batch = 1.326, learning rate = 0.00000008\n",
      "11849/85400 (epoch 27), train_loss = -8.61875013, time/batch = 1.313, learning rate = 0.00000008\n",
      "11859/85400 (epoch 27), train_loss = -8.61869483, time/batch = 1.316, learning rate = 0.00000008\n",
      "11869/85400 (epoch 27), train_loss = -8.61886894, time/batch = 1.322, learning rate = 0.00000008\n",
      "11879/85400 (epoch 27), train_loss = -8.61882538, time/batch = 1.320, learning rate = 0.00000008\n",
      "11889/85400 (epoch 27), train_loss = -8.61846116, time/batch = 1.330, learning rate = 0.00000008\n",
      "11899/85400 (epoch 27), train_loss = -8.61819485, time/batch = 1.327, learning rate = 0.00000008\n",
      "11909/85400 (epoch 27), train_loss = -8.61881617, time/batch = 1.311, learning rate = 0.00000008\n",
      "11919/85400 (epoch 27), train_loss = -8.61899368, time/batch = 1.314, learning rate = 0.00000008\n",
      "11929/85400 (epoch 27), train_loss = -8.61913608, time/batch = 1.315, learning rate = 0.00000008\n",
      "11939/85400 (epoch 27), train_loss = -8.61904527, time/batch = 1.317, learning rate = 0.00000008\n",
      "11949/85400 (epoch 27), train_loss = -8.61822677, time/batch = 1.318, learning rate = 0.00000008\n",
      "11966/85400 (epoch 28), train_loss = -8.64051600, time/batch = 1.329, learning rate = 0.00000008\n",
      "11976/85400 (epoch 28), train_loss = -8.62810302, time/batch = 1.317, learning rate = 0.00000008\n",
      "11986/85400 (epoch 28), train_loss = -8.62894932, time/batch = 1.322, learning rate = 0.00000008\n",
      "11996/85400 (epoch 28), train_loss = -8.62872450, time/batch = 1.319, learning rate = 0.00000008\n",
      "Saving model to model-28-12000.pth\n",
      "12006/85400 (epoch 28), train_loss = -8.62946974, time/batch = 1.318, learning rate = 0.00000008\n",
      "12016/85400 (epoch 28), train_loss = -8.62724700, time/batch = 1.319, learning rate = 0.00000008\n",
      "12026/85400 (epoch 28), train_loss = -8.62282017, time/batch = 1.313, learning rate = 0.00000008\n",
      "12036/85400 (epoch 28), train_loss = -8.62387489, time/batch = 1.317, learning rate = 0.00000008\n",
      "12046/85400 (epoch 28), train_loss = -8.62050007, time/batch = 1.321, learning rate = 0.00000008\n",
      "12056/85400 (epoch 28), train_loss = -8.62056827, time/batch = 1.316, learning rate = 0.00000008\n",
      "12066/85400 (epoch 28), train_loss = -8.62345182, time/batch = 1.308, learning rate = 0.00000008\n",
      "12076/85400 (epoch 28), train_loss = -8.62076277, time/batch = 1.323, learning rate = 0.00000008\n",
      "12086/85400 (epoch 28), train_loss = -8.61976891, time/batch = 1.315, learning rate = 0.00000008\n",
      "12096/85400 (epoch 28), train_loss = -8.61838265, time/batch = 1.308, learning rate = 0.00000008\n",
      "12106/85400 (epoch 28), train_loss = -8.61838853, time/batch = 1.319, learning rate = 0.00000008\n",
      "12116/85400 (epoch 28), train_loss = -8.61969612, time/batch = 1.316, learning rate = 0.00000008\n",
      "12126/85400 (epoch 28), train_loss = -8.61932788, time/batch = 1.327, learning rate = 0.00000008\n",
      "12136/85400 (epoch 28), train_loss = -8.62003839, time/batch = 1.408, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12146/85400 (epoch 28), train_loss = -8.61900105, time/batch = 1.331, learning rate = 0.00000008\n",
      "12156/85400 (epoch 28), train_loss = -8.61838890, time/batch = 1.322, learning rate = 0.00000008\n",
      "12166/85400 (epoch 28), train_loss = -8.61850874, time/batch = 1.321, learning rate = 0.00000008\n",
      "12176/85400 (epoch 28), train_loss = -8.61926938, time/batch = 1.319, learning rate = 0.00000008\n",
      "12186/85400 (epoch 28), train_loss = -8.61879476, time/batch = 1.321, learning rate = 0.00000008\n",
      "12196/85400 (epoch 28), train_loss = -8.61836860, time/batch = 1.320, learning rate = 0.00000008\n",
      "12206/85400 (epoch 28), train_loss = -8.61828646, time/batch = 1.323, learning rate = 0.00000008\n",
      "12216/85400 (epoch 28), train_loss = -8.61736090, time/batch = 1.315, learning rate = 0.00000008\n",
      "12226/85400 (epoch 28), train_loss = -8.61805075, time/batch = 1.317, learning rate = 0.00000008\n",
      "12236/85400 (epoch 28), train_loss = -8.61595893, time/batch = 1.313, learning rate = 0.00000008\n",
      "12246/85400 (epoch 28), train_loss = -8.61574513, time/batch = 1.328, learning rate = 0.00000008\n",
      "12256/85400 (epoch 28), train_loss = -8.61565501, time/batch = 1.320, learning rate = 0.00000008\n",
      "12266/85400 (epoch 28), train_loss = -8.61554776, time/batch = 1.316, learning rate = 0.00000008\n",
      "12276/85400 (epoch 28), train_loss = -8.61530948, time/batch = 1.325, learning rate = 0.00000008\n",
      "12286/85400 (epoch 28), train_loss = -8.61570902, time/batch = 1.326, learning rate = 0.00000008\n",
      "12296/85400 (epoch 28), train_loss = -8.61552418, time/batch = 1.317, learning rate = 0.00000008\n",
      "12306/85400 (epoch 28), train_loss = -8.61526620, time/batch = 1.323, learning rate = 0.00000008\n",
      "12316/85400 (epoch 28), train_loss = -8.61531949, time/batch = 1.317, learning rate = 0.00000008\n",
      "12326/85400 (epoch 28), train_loss = -8.61531516, time/batch = 1.325, learning rate = 0.00000008\n",
      "12336/85400 (epoch 28), train_loss = -8.61558439, time/batch = 1.315, learning rate = 0.00000008\n",
      "12346/85400 (epoch 28), train_loss = -8.61499058, time/batch = 1.326, learning rate = 0.00000008\n",
      "12356/85400 (epoch 28), train_loss = -8.61542491, time/batch = 1.314, learning rate = 0.00000008\n",
      "12366/85400 (epoch 28), train_loss = -8.61576402, time/batch = 1.328, learning rate = 0.00000008\n",
      "12376/85400 (epoch 28), train_loss = -8.61613463, time/batch = 1.324, learning rate = 0.00000008\n",
      "12393/85400 (epoch 29), train_loss = -8.61817417, time/batch = 1.320, learning rate = 0.00000008\n",
      "12403/85400 (epoch 29), train_loss = -8.61598682, time/batch = 1.316, learning rate = 0.00000008\n",
      "12413/85400 (epoch 29), train_loss = -8.60595942, time/batch = 1.311, learning rate = 0.00000008\n",
      "12423/85400 (epoch 29), train_loss = -8.61208601, time/batch = 1.319, learning rate = 0.00000008\n",
      "12433/85400 (epoch 29), train_loss = -8.61108204, time/batch = 1.312, learning rate = 0.00000008\n",
      "12443/85400 (epoch 29), train_loss = -8.61292939, time/batch = 1.319, learning rate = 0.00000008\n",
      "12453/85400 (epoch 29), train_loss = -8.61664058, time/batch = 1.319, learning rate = 0.00000008\n",
      "12463/85400 (epoch 29), train_loss = -8.61701877, time/batch = 1.315, learning rate = 0.00000008\n",
      "12473/85400 (epoch 29), train_loss = -8.61863340, time/batch = 1.332, learning rate = 0.00000008\n",
      "12483/85400 (epoch 29), train_loss = -8.62083303, time/batch = 1.320, learning rate = 0.00000008\n",
      "12493/85400 (epoch 29), train_loss = -8.62052237, time/batch = 1.325, learning rate = 0.00000008\n",
      "Saving model to model-29-12500.pth\n",
      "12503/85400 (epoch 29), train_loss = -8.62083723, time/batch = 1.308, learning rate = 0.00000008\n",
      "12513/85400 (epoch 29), train_loss = -8.62164067, time/batch = 1.322, learning rate = 0.00000008\n",
      "12523/85400 (epoch 29), train_loss = -8.61961047, time/batch = 1.319, learning rate = 0.00000008\n",
      "12533/85400 (epoch 29), train_loss = -8.61892518, time/batch = 1.311, learning rate = 0.00000008\n",
      "12543/85400 (epoch 29), train_loss = -8.61966686, time/batch = 1.328, learning rate = 0.00000008\n",
      "12553/85400 (epoch 29), train_loss = -8.62131673, time/batch = 1.322, learning rate = 0.00000008\n",
      "12563/85400 (epoch 29), train_loss = -8.62088544, time/batch = 1.318, learning rate = 0.00000008\n",
      "12573/85400 (epoch 29), train_loss = -8.62243821, time/batch = 1.322, learning rate = 0.00000008\n",
      "12583/85400 (epoch 29), train_loss = -8.62175915, time/batch = 1.317, learning rate = 0.00000008\n",
      "12593/85400 (epoch 29), train_loss = -8.62136991, time/batch = 1.329, learning rate = 0.00000008\n",
      "12603/85400 (epoch 29), train_loss = -8.62154769, time/batch = 1.323, learning rate = 0.00000008\n",
      "12613/85400 (epoch 29), train_loss = -8.62000185, time/batch = 1.325, learning rate = 0.00000008\n",
      "12623/85400 (epoch 29), train_loss = -8.62000224, time/batch = 1.321, learning rate = 0.00000008\n",
      "12633/85400 (epoch 29), train_loss = -8.62009044, time/batch = 1.321, learning rate = 0.00000008\n",
      "12643/85400 (epoch 29), train_loss = -8.62008818, time/batch = 1.324, learning rate = 0.00000008\n",
      "12653/85400 (epoch 29), train_loss = -8.62030106, time/batch = 1.325, learning rate = 0.00000008\n",
      "12663/85400 (epoch 29), train_loss = -8.62025432, time/batch = 1.313, learning rate = 0.00000008\n",
      "12673/85400 (epoch 29), train_loss = -8.62024950, time/batch = 1.318, learning rate = 0.00000008\n",
      "12683/85400 (epoch 29), train_loss = -8.62006262, time/batch = 1.325, learning rate = 0.00000008\n",
      "12693/85400 (epoch 29), train_loss = -8.62002158, time/batch = 1.311, learning rate = 0.00000008\n",
      "12703/85400 (epoch 29), train_loss = -8.61947477, time/batch = 1.313, learning rate = 0.00000008\n",
      "12713/85400 (epoch 29), train_loss = -8.61951571, time/batch = 1.324, learning rate = 0.00000008\n",
      "12723/85400 (epoch 29), train_loss = -8.61957477, time/batch = 1.313, learning rate = 0.00000008\n",
      "12733/85400 (epoch 29), train_loss = -8.61979200, time/batch = 1.322, learning rate = 0.00000008\n",
      "12743/85400 (epoch 29), train_loss = -8.61981707, time/batch = 1.308, learning rate = 0.00000008\n",
      "12753/85400 (epoch 29), train_loss = -8.61892082, time/batch = 1.318, learning rate = 0.00000008\n",
      "12763/85400 (epoch 29), train_loss = -8.61911969, time/batch = 1.320, learning rate = 0.00000008\n",
      "12773/85400 (epoch 29), train_loss = -8.61855340, time/batch = 1.322, learning rate = 0.00000008\n",
      "12783/85400 (epoch 29), train_loss = -8.61864704, time/batch = 1.324, learning rate = 0.00000008\n",
      "12793/85400 (epoch 29), train_loss = -8.61882772, time/batch = 1.320, learning rate = 0.00000008\n",
      "12803/85400 (epoch 29), train_loss = -8.61926809, time/batch = 1.307, learning rate = 0.00000008\n",
      "12820/85400 (epoch 30), train_loss = -8.61074562, time/batch = 1.328, learning rate = 0.00000008\n",
      "12830/85400 (epoch 30), train_loss = -8.60717645, time/batch = 1.312, learning rate = 0.00000008\n",
      "12840/85400 (epoch 30), train_loss = -8.61310965, time/batch = 1.318, learning rate = 0.00000008\n",
      "12850/85400 (epoch 30), train_loss = -8.61163094, time/batch = 1.322, learning rate = 0.00000008\n",
      "12860/85400 (epoch 30), train_loss = -8.60604462, time/batch = 1.320, learning rate = 0.00000008\n",
      "12870/85400 (epoch 30), train_loss = -8.60714847, time/batch = 1.319, learning rate = 0.00000008\n",
      "12880/85400 (epoch 30), train_loss = -8.60463679, time/batch = 1.320, learning rate = 0.00000008\n",
      "12890/85400 (epoch 30), train_loss = -8.60551138, time/batch = 1.327, learning rate = 0.00000008\n",
      "12900/85400 (epoch 30), train_loss = -8.60838332, time/batch = 1.327, learning rate = 0.00000008\n",
      "12910/85400 (epoch 30), train_loss = -8.61108752, time/batch = 1.321, learning rate = 0.00000008\n",
      "12920/85400 (epoch 30), train_loss = -8.61179656, time/batch = 1.331, learning rate = 0.00000008\n",
      "12930/85400 (epoch 30), train_loss = -8.61225713, time/batch = 1.318, learning rate = 0.00000008\n",
      "12940/85400 (epoch 30), train_loss = -8.61356209, time/batch = 1.316, learning rate = 0.00000008\n",
      "12950/85400 (epoch 30), train_loss = -8.61246901, time/batch = 1.323, learning rate = 0.00000008\n",
      "12960/85400 (epoch 30), train_loss = -8.61257005, time/batch = 1.321, learning rate = 0.00000008\n",
      "12970/85400 (epoch 30), train_loss = -8.61021803, time/batch = 1.321, learning rate = 0.00000008\n",
      "12980/85400 (epoch 30), train_loss = -8.61211384, time/batch = 1.318, learning rate = 0.00000008\n",
      "12990/85400 (epoch 30), train_loss = -8.61232976, time/batch = 1.321, learning rate = 0.00000008\n",
      "13000/85400 (epoch 30), train_loss = -8.61339597, time/batch = 1.313, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to model-30-13000.pth\n",
      "13010/85400 (epoch 30), train_loss = -8.61425742, time/batch = 1.345, learning rate = 0.00000008\n",
      "13020/85400 (epoch 30), train_loss = -8.61370320, time/batch = 1.345, learning rate = 0.00000008\n",
      "13030/85400 (epoch 30), train_loss = -8.61295570, time/batch = 1.365, learning rate = 0.00000008\n",
      "13040/85400 (epoch 30), train_loss = -8.61342203, time/batch = 1.335, learning rate = 0.00000008\n",
      "13050/85400 (epoch 30), train_loss = -8.61330903, time/batch = 1.313, learning rate = 0.00000008\n",
      "13060/85400 (epoch 30), train_loss = -8.61422728, time/batch = 1.323, learning rate = 0.00000008\n",
      "13070/85400 (epoch 30), train_loss = -8.61435626, time/batch = 1.329, learning rate = 0.00000008\n",
      "13080/85400 (epoch 30), train_loss = -8.61442188, time/batch = 1.326, learning rate = 0.00000008\n",
      "13090/85400 (epoch 30), train_loss = -8.61393275, time/batch = 1.311, learning rate = 0.00000008\n",
      "13100/85400 (epoch 30), train_loss = -8.61398938, time/batch = 1.318, learning rate = 0.00000008\n",
      "13110/85400 (epoch 30), train_loss = -8.61458651, time/batch = 1.320, learning rate = 0.00000008\n",
      "13120/85400 (epoch 30), train_loss = -8.61536512, time/batch = 1.310, learning rate = 0.00000008\n",
      "13130/85400 (epoch 30), train_loss = -8.61599342, time/batch = 1.317, learning rate = 0.00000008\n",
      "13140/85400 (epoch 30), train_loss = -8.61576313, time/batch = 1.318, learning rate = 0.00000008\n",
      "13150/85400 (epoch 30), train_loss = -8.61485735, time/batch = 1.318, learning rate = 0.00000008\n",
      "13160/85400 (epoch 30), train_loss = -8.61488218, time/batch = 1.319, learning rate = 0.00000008\n",
      "13170/85400 (epoch 30), train_loss = -8.61545229, time/batch = 1.311, learning rate = 0.00000008\n",
      "13180/85400 (epoch 30), train_loss = -8.61586266, time/batch = 1.320, learning rate = 0.00000008\n",
      "13190/85400 (epoch 30), train_loss = -8.61583977, time/batch = 1.324, learning rate = 0.00000008\n",
      "13200/85400 (epoch 30), train_loss = -8.61643127, time/batch = 1.324, learning rate = 0.00000008\n",
      "13210/85400 (epoch 30), train_loss = -8.61612860, time/batch = 1.327, learning rate = 0.00000008\n",
      "13220/85400 (epoch 30), train_loss = -8.61576563, time/batch = 1.334, learning rate = 0.00000008\n",
      "13230/85400 (epoch 30), train_loss = -8.61595839, time/batch = 1.430, learning rate = 0.00000008\n",
      "13247/85400 (epoch 31), train_loss = -8.62753935, time/batch = 1.312, learning rate = 0.00000008\n",
      "13257/85400 (epoch 31), train_loss = -8.62902756, time/batch = 1.316, learning rate = 0.00000008\n",
      "13267/85400 (epoch 31), train_loss = -8.62387899, time/batch = 1.313, learning rate = 0.00000008\n",
      "13277/85400 (epoch 31), train_loss = -8.62407432, time/batch = 1.320, learning rate = 0.00000008\n",
      "13287/85400 (epoch 31), train_loss = -8.62607658, time/batch = 1.331, learning rate = 0.00000008\n",
      "13297/85400 (epoch 31), train_loss = -8.62712585, time/batch = 1.321, learning rate = 0.00000008\n",
      "13307/85400 (epoch 31), train_loss = -8.62837076, time/batch = 1.314, learning rate = 0.00000008\n",
      "13317/85400 (epoch 31), train_loss = -8.62590028, time/batch = 1.322, learning rate = 0.00000008\n",
      "13327/85400 (epoch 31), train_loss = -8.62574834, time/batch = 1.324, learning rate = 0.00000008\n",
      "13337/85400 (epoch 31), train_loss = -8.62235250, time/batch = 1.327, learning rate = 0.00000008\n",
      "13347/85400 (epoch 31), train_loss = -8.62245475, time/batch = 1.319, learning rate = 0.00000008\n",
      "13357/85400 (epoch 31), train_loss = -8.61899095, time/batch = 1.324, learning rate = 0.00000008\n",
      "13367/85400 (epoch 31), train_loss = -8.61760482, time/batch = 1.323, learning rate = 0.00000008\n",
      "13377/85400 (epoch 31), train_loss = -8.61873339, time/batch = 1.322, learning rate = 0.00000008\n",
      "13387/85400 (epoch 31), train_loss = -8.61902017, time/batch = 1.321, learning rate = 0.00000008\n",
      "13397/85400 (epoch 31), train_loss = -8.61952295, time/batch = 1.322, learning rate = 0.00000008\n",
      "13407/85400 (epoch 31), train_loss = -8.61952487, time/batch = 1.329, learning rate = 0.00000008\n",
      "13417/85400 (epoch 31), train_loss = -8.61664784, time/batch = 1.316, learning rate = 0.00000008\n",
      "13427/85400 (epoch 31), train_loss = -8.61716986, time/batch = 1.325, learning rate = 0.00000008\n",
      "13437/85400 (epoch 31), train_loss = -8.61724063, time/batch = 1.319, learning rate = 0.00000008\n",
      "13447/85400 (epoch 31), train_loss = -8.61772566, time/batch = 1.318, learning rate = 0.00000008\n",
      "13457/85400 (epoch 31), train_loss = -8.61747794, time/batch = 1.324, learning rate = 0.00000008\n",
      "13467/85400 (epoch 31), train_loss = -8.61756372, time/batch = 1.315, learning rate = 0.00000008\n",
      "13477/85400 (epoch 31), train_loss = -8.61685264, time/batch = 1.331, learning rate = 0.00000008\n",
      "13487/85400 (epoch 31), train_loss = -8.61588111, time/batch = 1.314, learning rate = 0.00000008\n",
      "13497/85400 (epoch 31), train_loss = -8.61623206, time/batch = 1.317, learning rate = 0.00000008\n",
      "Saving model to model-31-13500.pth\n",
      "13507/85400 (epoch 31), train_loss = -8.61659823, time/batch = 1.310, learning rate = 0.00000008\n",
      "13517/85400 (epoch 31), train_loss = -8.61687981, time/batch = 1.322, learning rate = 0.00000008\n",
      "13527/85400 (epoch 31), train_loss = -8.61730243, time/batch = 1.320, learning rate = 0.00000008\n",
      "13537/85400 (epoch 31), train_loss = -8.61678137, time/batch = 1.323, learning rate = 0.00000008\n",
      "13547/85400 (epoch 31), train_loss = -8.61684545, time/batch = 1.323, learning rate = 0.00000008\n",
      "13557/85400 (epoch 31), train_loss = -8.61723872, time/batch = 1.318, learning rate = 0.00000008\n",
      "13567/85400 (epoch 31), train_loss = -8.61758572, time/batch = 1.324, learning rate = 0.00000008\n",
      "13577/85400 (epoch 31), train_loss = -8.61744283, time/batch = 1.317, learning rate = 0.00000008\n",
      "13587/85400 (epoch 31), train_loss = -8.61735506, time/batch = 1.322, learning rate = 0.00000008\n",
      "13597/85400 (epoch 31), train_loss = -8.61636930, time/batch = 1.324, learning rate = 0.00000008\n",
      "13607/85400 (epoch 31), train_loss = -8.61654516, time/batch = 1.330, learning rate = 0.00000008\n",
      "13617/85400 (epoch 31), train_loss = -8.61604330, time/batch = 1.319, learning rate = 0.00000008\n",
      "13627/85400 (epoch 31), train_loss = -8.61605812, time/batch = 1.330, learning rate = 0.00000008\n",
      "13637/85400 (epoch 31), train_loss = -8.61625718, time/batch = 1.317, learning rate = 0.00000008\n",
      "13647/85400 (epoch 31), train_loss = -8.61670375, time/batch = 1.318, learning rate = 0.00000008\n",
      "13657/85400 (epoch 31), train_loss = -8.61723619, time/batch = 1.325, learning rate = 0.00000008\n",
      "13674/85400 (epoch 32), train_loss = -8.62246151, time/batch = 1.317, learning rate = 0.00000008\n",
      "13684/85400 (epoch 32), train_loss = -8.61244059, time/batch = 1.322, learning rate = 0.00000008\n",
      "13694/85400 (epoch 32), train_loss = -8.61512251, time/batch = 1.316, learning rate = 0.00000008\n",
      "13704/85400 (epoch 32), train_loss = -8.61881096, time/batch = 1.327, learning rate = 0.00000008\n",
      "13714/85400 (epoch 32), train_loss = -8.61815964, time/batch = 1.318, learning rate = 0.00000008\n",
      "13724/85400 (epoch 32), train_loss = -8.61841667, time/batch = 1.323, learning rate = 0.00000008\n",
      "13734/85400 (epoch 32), train_loss = -8.61835816, time/batch = 1.314, learning rate = 0.00000008\n",
      "13744/85400 (epoch 32), train_loss = -8.61458951, time/batch = 1.327, learning rate = 0.00000008\n",
      "13754/85400 (epoch 32), train_loss = -8.61593174, time/batch = 1.326, learning rate = 0.00000008\n",
      "13764/85400 (epoch 32), train_loss = -8.61536247, time/batch = 1.316, learning rate = 0.00000008\n",
      "13774/85400 (epoch 32), train_loss = -8.61526527, time/batch = 1.325, learning rate = 0.00000008\n",
      "13784/85400 (epoch 32), train_loss = -8.61399740, time/batch = 1.316, learning rate = 0.00000008\n",
      "13794/85400 (epoch 32), train_loss = -8.61555181, time/batch = 1.328, learning rate = 0.00000008\n",
      "13804/85400 (epoch 32), train_loss = -8.61786313, time/batch = 1.312, learning rate = 0.00000008\n",
      "13814/85400 (epoch 32), train_loss = -8.61866114, time/batch = 1.321, learning rate = 0.00000008\n",
      "13824/85400 (epoch 32), train_loss = -8.61947353, time/batch = 1.326, learning rate = 0.00000008\n",
      "13834/85400 (epoch 32), train_loss = -8.61911771, time/batch = 1.324, learning rate = 0.00000008\n",
      "13844/85400 (epoch 32), train_loss = -8.61926364, time/batch = 1.333, learning rate = 0.00000008\n",
      "13854/85400 (epoch 32), train_loss = -8.61811005, time/batch = 1.332, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13864/85400 (epoch 32), train_loss = -8.61713488, time/batch = 1.318, learning rate = 0.00000008\n",
      "13874/85400 (epoch 32), train_loss = -8.61770709, time/batch = 1.318, learning rate = 0.00000008\n",
      "13884/85400 (epoch 32), train_loss = -8.61751578, time/batch = 1.325, learning rate = 0.00000008\n",
      "13894/85400 (epoch 32), train_loss = -8.61702221, time/batch = 1.326, learning rate = 0.00000008\n",
      "13904/85400 (epoch 32), train_loss = -8.61700521, time/batch = 1.314, learning rate = 0.00000008\n",
      "13914/85400 (epoch 32), train_loss = -8.61586790, time/batch = 1.324, learning rate = 0.00000008\n",
      "13924/85400 (epoch 32), train_loss = -8.61629884, time/batch = 1.322, learning rate = 0.00000008\n",
      "13934/85400 (epoch 32), train_loss = -8.61659029, time/batch = 1.321, learning rate = 0.00000008\n",
      "13944/85400 (epoch 32), train_loss = -8.61753105, time/batch = 1.321, learning rate = 0.00000008\n",
      "13954/85400 (epoch 32), train_loss = -8.61763632, time/batch = 1.316, learning rate = 0.00000008\n",
      "13964/85400 (epoch 32), train_loss = -8.61830172, time/batch = 1.327, learning rate = 0.00000008\n",
      "13974/85400 (epoch 32), train_loss = -8.61828227, time/batch = 1.321, learning rate = 0.00000008\n",
      "13984/85400 (epoch 32), train_loss = -8.61854596, time/batch = 1.330, learning rate = 0.00000008\n",
      "13994/85400 (epoch 32), train_loss = -8.61854462, time/batch = 1.318, learning rate = 0.00000008\n",
      "Saving model to model-32-14000.pth\n",
      "14004/85400 (epoch 32), train_loss = -8.61870701, time/batch = 1.323, learning rate = 0.00000008\n",
      "14014/85400 (epoch 32), train_loss = -8.61875419, time/batch = 1.309, learning rate = 0.00000008\n",
      "14024/85400 (epoch 32), train_loss = -8.61836700, time/batch = 1.313, learning rate = 0.00000008\n",
      "14034/85400 (epoch 32), train_loss = -8.61775492, time/batch = 1.319, learning rate = 0.00000008\n",
      "14044/85400 (epoch 32), train_loss = -8.61764445, time/batch = 1.311, learning rate = 0.00000008\n",
      "14054/85400 (epoch 32), train_loss = -8.61801573, time/batch = 1.313, learning rate = 0.00000008\n",
      "14064/85400 (epoch 32), train_loss = -8.61795743, time/batch = 1.321, learning rate = 0.00000008\n",
      "14074/85400 (epoch 32), train_loss = -8.61768758, time/batch = 1.320, learning rate = 0.00000008\n",
      "14084/85400 (epoch 32), train_loss = -8.61759520, time/batch = 1.325, learning rate = 0.00000008\n",
      "14101/85400 (epoch 33), train_loss = -8.60233374, time/batch = 1.313, learning rate = 0.00000008\n",
      "14111/85400 (epoch 33), train_loss = -8.62124357, time/batch = 1.325, learning rate = 0.00000008\n",
      "14121/85400 (epoch 33), train_loss = -8.62107325, time/batch = 1.320, learning rate = 0.00000008\n",
      "14131/85400 (epoch 33), train_loss = -8.62090168, time/batch = 1.321, learning rate = 0.00000008\n",
      "14141/85400 (epoch 33), train_loss = -8.62337748, time/batch = 1.312, learning rate = 0.00000008\n",
      "14151/85400 (epoch 33), train_loss = -8.62300727, time/batch = 1.320, learning rate = 0.00000008\n",
      "14161/85400 (epoch 33), train_loss = -8.62183147, time/batch = 1.318, learning rate = 0.00000008\n",
      "14171/85400 (epoch 33), train_loss = -8.62265468, time/batch = 1.320, learning rate = 0.00000008\n",
      "14181/85400 (epoch 33), train_loss = -8.62181736, time/batch = 1.324, learning rate = 0.00000008\n",
      "14191/85400 (epoch 33), train_loss = -8.62140936, time/batch = 1.323, learning rate = 0.00000008\n",
      "14201/85400 (epoch 33), train_loss = -8.62148205, time/batch = 1.323, learning rate = 0.00000008\n",
      "14211/85400 (epoch 33), train_loss = -8.62291241, time/batch = 1.336, learning rate = 0.00000008\n",
      "14221/85400 (epoch 33), train_loss = -8.62394194, time/batch = 1.312, learning rate = 0.00000008\n",
      "14231/85400 (epoch 33), train_loss = -8.62408370, time/batch = 1.320, learning rate = 0.00000008\n",
      "14241/85400 (epoch 33), train_loss = -8.62376048, time/batch = 1.313, learning rate = 0.00000008\n",
      "14251/85400 (epoch 33), train_loss = -8.62426475, time/batch = 1.324, learning rate = 0.00000008\n",
      "14261/85400 (epoch 33), train_loss = -8.62271056, time/batch = 1.316, learning rate = 0.00000008\n",
      "14271/85400 (epoch 33), train_loss = -8.62279010, time/batch = 1.314, learning rate = 0.00000008\n",
      "14281/85400 (epoch 33), train_loss = -8.62232459, time/batch = 1.318, learning rate = 0.00000008\n",
      "14291/85400 (epoch 33), train_loss = -8.62162605, time/batch = 1.327, learning rate = 0.00000008\n",
      "14301/85400 (epoch 33), train_loss = -8.62208717, time/batch = 1.327, learning rate = 0.00000008\n",
      "14311/85400 (epoch 33), train_loss = -8.62141946, time/batch = 1.323, learning rate = 0.00000008\n",
      "14321/85400 (epoch 33), train_loss = -8.62128655, time/batch = 1.316, learning rate = 0.00000008\n",
      "14331/85400 (epoch 33), train_loss = -8.62138591, time/batch = 1.317, learning rate = 0.00000008\n",
      "14341/85400 (epoch 33), train_loss = -8.62099112, time/batch = 1.313, learning rate = 0.00000008\n",
      "14351/85400 (epoch 33), train_loss = -8.62091319, time/batch = 1.316, learning rate = 0.00000008\n",
      "14361/85400 (epoch 33), train_loss = -8.62091411, time/batch = 1.321, learning rate = 0.00000008\n",
      "14371/85400 (epoch 33), train_loss = -8.62061081, time/batch = 1.316, learning rate = 0.00000008\n",
      "14381/85400 (epoch 33), train_loss = -8.61999911, time/batch = 1.325, learning rate = 0.00000008\n",
      "14391/85400 (epoch 33), train_loss = -8.62014071, time/batch = 1.311, learning rate = 0.00000008\n",
      "14401/85400 (epoch 33), train_loss = -8.62024377, time/batch = 1.321, learning rate = 0.00000008\n",
      "14411/85400 (epoch 33), train_loss = -8.62014315, time/batch = 1.312, learning rate = 0.00000008\n",
      "14421/85400 (epoch 33), train_loss = -8.61984879, time/batch = 1.311, learning rate = 0.00000008\n",
      "14431/85400 (epoch 33), train_loss = -8.61987667, time/batch = 1.314, learning rate = 0.00000008\n",
      "14441/85400 (epoch 33), train_loss = -8.61989361, time/batch = 1.323, learning rate = 0.00000008\n",
      "14451/85400 (epoch 33), train_loss = -8.62012011, time/batch = 1.329, learning rate = 0.00000008\n",
      "14461/85400 (epoch 33), train_loss = -8.62054816, time/batch = 1.318, learning rate = 0.00000008\n",
      "14471/85400 (epoch 33), train_loss = -8.62065311, time/batch = 1.321, learning rate = 0.00000008\n",
      "14481/85400 (epoch 33), train_loss = -8.62079041, time/batch = 1.326, learning rate = 0.00000008\n",
      "14491/85400 (epoch 33), train_loss = -8.62018983, time/batch = 1.320, learning rate = 0.00000008\n",
      "Saving model to model-33-14500.pth\n",
      "14501/85400 (epoch 33), train_loss = -8.61951263, time/batch = 1.311, learning rate = 0.00000008\n",
      "14511/85400 (epoch 33), train_loss = -8.61930249, time/batch = 1.319, learning rate = 0.00000008\n",
      "14528/85400 (epoch 34), train_loss = -8.61626415, time/batch = 1.323, learning rate = 0.00000008\n",
      "14538/85400 (epoch 34), train_loss = -8.62031541, time/batch = 1.329, learning rate = 0.00000008\n",
      "14548/85400 (epoch 34), train_loss = -8.62698018, time/batch = 1.323, learning rate = 0.00000008\n",
      "14558/85400 (epoch 34), train_loss = -8.62235599, time/batch = 1.313, learning rate = 0.00000008\n",
      "14568/85400 (epoch 34), train_loss = -8.61135433, time/batch = 1.318, learning rate = 0.00000008\n",
      "14578/85400 (epoch 34), train_loss = -8.61257087, time/batch = 1.320, learning rate = 0.00000008\n",
      "14588/85400 (epoch 34), train_loss = -8.61232944, time/batch = 1.326, learning rate = 0.00000008\n",
      "14598/85400 (epoch 34), train_loss = -8.60747905, time/batch = 1.324, learning rate = 0.00000008\n",
      "14608/85400 (epoch 34), train_loss = -8.60862336, time/batch = 1.327, learning rate = 0.00000008\n",
      "14618/85400 (epoch 34), train_loss = -8.61058957, time/batch = 1.318, learning rate = 0.00000008\n",
      "14628/85400 (epoch 34), train_loss = -8.61151713, time/batch = 1.311, learning rate = 0.00000008\n",
      "14638/85400 (epoch 34), train_loss = -8.61226541, time/batch = 1.315, learning rate = 0.00000008\n",
      "14648/85400 (epoch 34), train_loss = -8.61326080, time/batch = 1.325, learning rate = 0.00000008\n",
      "14658/85400 (epoch 34), train_loss = -8.61472454, time/batch = 1.323, learning rate = 0.00000008\n",
      "14668/85400 (epoch 34), train_loss = -8.61417366, time/batch = 1.325, learning rate = 0.00000008\n",
      "14678/85400 (epoch 34), train_loss = -8.61450679, time/batch = 1.316, learning rate = 0.00000008\n",
      "14688/85400 (epoch 34), train_loss = -8.61488159, time/batch = 1.320, learning rate = 0.00000008\n",
      "14698/85400 (epoch 34), train_loss = -8.61546009, time/batch = 1.318, learning rate = 0.00000008\n",
      "14708/85400 (epoch 34), train_loss = -8.61549002, time/batch = 1.321, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14718/85400 (epoch 34), train_loss = -8.61663102, time/batch = 1.325, learning rate = 0.00000008\n",
      "14728/85400 (epoch 34), train_loss = -8.61730823, time/batch = 1.314, learning rate = 0.00000008\n",
      "14738/85400 (epoch 34), train_loss = -8.61658216, time/batch = 1.321, learning rate = 0.00000008\n",
      "14748/85400 (epoch 34), train_loss = -8.61720016, time/batch = 1.325, learning rate = 0.00000008\n",
      "14758/85400 (epoch 34), train_loss = -8.61761190, time/batch = 1.335, learning rate = 0.00000008\n",
      "14768/85400 (epoch 34), train_loss = -8.61806275, time/batch = 1.322, learning rate = 0.00000008\n",
      "14778/85400 (epoch 34), train_loss = -8.61705039, time/batch = 1.321, learning rate = 0.00000008\n",
      "14788/85400 (epoch 34), train_loss = -8.61696198, time/batch = 1.323, learning rate = 0.00000008\n",
      "14798/85400 (epoch 34), train_loss = -8.61700431, time/batch = 1.319, learning rate = 0.00000008\n",
      "14808/85400 (epoch 34), train_loss = -8.61743532, time/batch = 1.320, learning rate = 0.00000008\n",
      "14818/85400 (epoch 34), train_loss = -8.61821259, time/batch = 1.312, learning rate = 0.00000008\n",
      "14828/85400 (epoch 34), train_loss = -8.61814537, time/batch = 1.327, learning rate = 0.00000008\n",
      "14838/85400 (epoch 34), train_loss = -8.61856813, time/batch = 1.321, learning rate = 0.00000008\n",
      "14848/85400 (epoch 34), train_loss = -8.61807791, time/batch = 1.318, learning rate = 0.00000008\n",
      "14858/85400 (epoch 34), train_loss = -8.61746458, time/batch = 1.325, learning rate = 0.00000008\n",
      "14868/85400 (epoch 34), train_loss = -8.61758274, time/batch = 1.311, learning rate = 0.00000008\n",
      "14878/85400 (epoch 34), train_loss = -8.61798282, time/batch = 1.312, learning rate = 0.00000008\n",
      "14888/85400 (epoch 34), train_loss = -8.61783586, time/batch = 1.328, learning rate = 0.00000008\n",
      "14898/85400 (epoch 34), train_loss = -8.61762516, time/batch = 1.315, learning rate = 0.00000008\n",
      "14908/85400 (epoch 34), train_loss = -8.61731383, time/batch = 1.321, learning rate = 0.00000008\n",
      "14918/85400 (epoch 34), train_loss = -8.61739888, time/batch = 1.322, learning rate = 0.00000008\n",
      "14928/85400 (epoch 34), train_loss = -8.61752102, time/batch = 1.326, learning rate = 0.00000008\n",
      "14938/85400 (epoch 34), train_loss = -8.61755198, time/batch = 1.316, learning rate = 0.00000008\n",
      "14955/85400 (epoch 35), train_loss = -8.63501844, time/batch = 1.326, learning rate = 0.00000008\n",
      "14965/85400 (epoch 35), train_loss = -8.63112350, time/batch = 1.317, learning rate = 0.00000008\n",
      "14975/85400 (epoch 35), train_loss = -8.62850011, time/batch = 1.323, learning rate = 0.00000008\n",
      "14985/85400 (epoch 35), train_loss = -8.62775974, time/batch = 1.316, learning rate = 0.00000008\n",
      "14995/85400 (epoch 35), train_loss = -8.62941910, time/batch = 1.323, learning rate = 0.00000008\n",
      "Saving model to model-35-15000.pth\n",
      "15005/85400 (epoch 35), train_loss = -8.62755850, time/batch = 1.315, learning rate = 0.00000008\n",
      "15015/85400 (epoch 35), train_loss = -8.62651114, time/batch = 1.323, learning rate = 0.00000008\n",
      "15025/85400 (epoch 35), train_loss = -8.62546054, time/batch = 1.319, learning rate = 0.00000008\n",
      "15035/85400 (epoch 35), train_loss = -8.62544877, time/batch = 1.321, learning rate = 0.00000008\n",
      "15045/85400 (epoch 35), train_loss = -8.62433257, time/batch = 1.313, learning rate = 0.00000008\n",
      "15055/85400 (epoch 35), train_loss = -8.62524049, time/batch = 1.316, learning rate = 0.00000008\n",
      "15065/85400 (epoch 35), train_loss = -8.62507637, time/batch = 1.329, learning rate = 0.00000008\n",
      "15075/85400 (epoch 35), train_loss = -8.62590226, time/batch = 1.324, learning rate = 0.00000008\n",
      "15085/85400 (epoch 35), train_loss = -8.62407870, time/batch = 1.323, learning rate = 0.00000008\n",
      "15095/85400 (epoch 35), train_loss = -8.62401519, time/batch = 1.324, learning rate = 0.00000008\n",
      "15105/85400 (epoch 35), train_loss = -8.62360091, time/batch = 1.321, learning rate = 0.00000008\n",
      "15115/85400 (epoch 35), train_loss = -8.62374100, time/batch = 1.330, learning rate = 0.00000008\n",
      "15125/85400 (epoch 35), train_loss = -8.62356544, time/batch = 1.324, learning rate = 0.00000008\n",
      "15135/85400 (epoch 35), train_loss = -8.62322771, time/batch = 1.324, learning rate = 0.00000008\n",
      "15145/85400 (epoch 35), train_loss = -8.62410871, time/batch = 1.325, learning rate = 0.00000008\n",
      "15155/85400 (epoch 35), train_loss = -8.62388629, time/batch = 1.326, learning rate = 0.00000008\n",
      "15165/85400 (epoch 35), train_loss = -8.62419129, time/batch = 1.322, learning rate = 0.00000008\n",
      "15175/85400 (epoch 35), train_loss = -8.62372590, time/batch = 1.330, learning rate = 0.00000008\n",
      "15185/85400 (epoch 35), train_loss = -8.62389363, time/batch = 1.316, learning rate = 0.00000008\n",
      "15195/85400 (epoch 35), train_loss = -8.62343820, time/batch = 1.317, learning rate = 0.00000008\n",
      "15205/85400 (epoch 35), train_loss = -8.62248820, time/batch = 1.322, learning rate = 0.00000008\n",
      "15215/85400 (epoch 35), train_loss = -8.62016979, time/batch = 1.423, learning rate = 0.00000008\n",
      "15225/85400 (epoch 35), train_loss = -8.61964431, time/batch = 1.313, learning rate = 0.00000008\n",
      "15235/85400 (epoch 35), train_loss = -8.61919004, time/batch = 1.322, learning rate = 0.00000008\n",
      "15245/85400 (epoch 35), train_loss = -8.61834160, time/batch = 1.317, learning rate = 0.00000008\n",
      "15255/85400 (epoch 35), train_loss = -8.61864676, time/batch = 1.332, learning rate = 0.00000008\n",
      "15265/85400 (epoch 35), train_loss = -8.61779323, time/batch = 1.314, learning rate = 0.00000008\n",
      "15275/85400 (epoch 35), train_loss = -8.61715206, time/batch = 1.323, learning rate = 0.00000008\n",
      "15285/85400 (epoch 35), train_loss = -8.61720025, time/batch = 1.310, learning rate = 0.00000008\n",
      "15295/85400 (epoch 35), train_loss = -8.61645555, time/batch = 1.324, learning rate = 0.00000008\n",
      "15305/85400 (epoch 35), train_loss = -8.61659191, time/batch = 1.328, learning rate = 0.00000008\n",
      "15315/85400 (epoch 35), train_loss = -8.61703685, time/batch = 1.325, learning rate = 0.00000008\n",
      "15325/85400 (epoch 35), train_loss = -8.61697240, time/batch = 1.321, learning rate = 0.00000008\n",
      "15335/85400 (epoch 35), train_loss = -8.61670560, time/batch = 1.316, learning rate = 0.00000008\n",
      "15345/85400 (epoch 35), train_loss = -8.61692904, time/batch = 1.324, learning rate = 0.00000008\n",
      "15355/85400 (epoch 35), train_loss = -8.61614918, time/batch = 1.325, learning rate = 0.00000008\n",
      "15365/85400 (epoch 35), train_loss = -8.61655268, time/batch = 1.322, learning rate = 0.00000008\n",
      "15382/85400 (epoch 36), train_loss = -8.63485613, time/batch = 1.323, learning rate = 0.00000008\n",
      "15392/85400 (epoch 36), train_loss = -8.61844482, time/batch = 1.320, learning rate = 0.00000008\n",
      "15402/85400 (epoch 36), train_loss = -8.62420505, time/batch = 1.328, learning rate = 0.00000008\n",
      "15412/85400 (epoch 36), train_loss = -8.62407980, time/batch = 1.318, learning rate = 0.00000008\n",
      "15422/85400 (epoch 36), train_loss = -8.62050016, time/batch = 1.330, learning rate = 0.00000008\n",
      "15432/85400 (epoch 36), train_loss = -8.61977436, time/batch = 1.316, learning rate = 0.00000008\n",
      "15442/85400 (epoch 36), train_loss = -8.61937797, time/batch = 1.315, learning rate = 0.00000008\n",
      "15452/85400 (epoch 36), train_loss = -8.61540818, time/batch = 1.322, learning rate = 0.00000008\n",
      "15462/85400 (epoch 36), train_loss = -8.61604327, time/batch = 1.315, learning rate = 0.00000008\n",
      "15472/85400 (epoch 36), train_loss = -8.61647513, time/batch = 1.316, learning rate = 0.00000008\n",
      "15482/85400 (epoch 36), train_loss = -8.61706277, time/batch = 1.333, learning rate = 0.00000008\n",
      "15492/85400 (epoch 36), train_loss = -8.61763968, time/batch = 1.327, learning rate = 0.00000008\n",
      "Saving model to model-36-15500.pth\n",
      "15502/85400 (epoch 36), train_loss = -8.61703375, time/batch = 1.323, learning rate = 0.00000008\n",
      "15512/85400 (epoch 36), train_loss = -8.61876811, time/batch = 1.319, learning rate = 0.00000008\n",
      "15522/85400 (epoch 36), train_loss = -8.61792468, time/batch = 1.313, learning rate = 0.00000008\n",
      "15532/85400 (epoch 36), train_loss = -8.61841725, time/batch = 1.317, learning rate = 0.00000008\n",
      "15542/85400 (epoch 36), train_loss = -8.61886247, time/batch = 1.321, learning rate = 0.00000008\n",
      "15552/85400 (epoch 36), train_loss = -8.61891815, time/batch = 1.314, learning rate = 0.00000008\n",
      "15562/85400 (epoch 36), train_loss = -8.61952258, time/batch = 1.314, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15572/85400 (epoch 36), train_loss = -8.61851725, time/batch = 1.321, learning rate = 0.00000008\n",
      "15582/85400 (epoch 36), train_loss = -8.61823837, time/batch = 1.325, learning rate = 0.00000008\n",
      "15592/85400 (epoch 36), train_loss = -8.61776602, time/batch = 1.314, learning rate = 0.00000008\n",
      "15602/85400 (epoch 36), train_loss = -8.61747230, time/batch = 1.313, learning rate = 0.00000008\n",
      "15612/85400 (epoch 36), train_loss = -8.61750378, time/batch = 1.313, learning rate = 0.00000008\n",
      "15622/85400 (epoch 36), train_loss = -8.61748259, time/batch = 1.327, learning rate = 0.00000008\n",
      "15632/85400 (epoch 36), train_loss = -8.61656068, time/batch = 1.319, learning rate = 0.00000008\n",
      "15642/85400 (epoch 36), train_loss = -8.61642777, time/batch = 1.321, learning rate = 0.00000008\n",
      "15652/85400 (epoch 36), train_loss = -8.61648816, time/batch = 1.318, learning rate = 0.00000008\n",
      "15662/85400 (epoch 36), train_loss = -8.61690776, time/batch = 1.327, learning rate = 0.00000008\n",
      "15672/85400 (epoch 36), train_loss = -8.61601848, time/batch = 1.315, learning rate = 0.00000008\n",
      "15682/85400 (epoch 36), train_loss = -8.61566041, time/batch = 1.311, learning rate = 0.00000008\n",
      "15692/85400 (epoch 36), train_loss = -8.61607811, time/batch = 1.318, learning rate = 0.00000008\n",
      "15702/85400 (epoch 36), train_loss = -8.61668831, time/batch = 1.319, learning rate = 0.00000008\n",
      "15712/85400 (epoch 36), train_loss = -8.61707790, time/batch = 1.320, learning rate = 0.00000008\n",
      "15722/85400 (epoch 36), train_loss = -8.61741623, time/batch = 1.328, learning rate = 0.00000008\n",
      "15732/85400 (epoch 36), train_loss = -8.61698643, time/batch = 1.307, learning rate = 0.00000008\n",
      "15742/85400 (epoch 36), train_loss = -8.61681669, time/batch = 1.316, learning rate = 0.00000008\n",
      "15752/85400 (epoch 36), train_loss = -8.61734475, time/batch = 1.321, learning rate = 0.00000008\n",
      "15762/85400 (epoch 36), train_loss = -8.61755804, time/batch = 1.323, learning rate = 0.00000008\n",
      "15772/85400 (epoch 36), train_loss = -8.61775377, time/batch = 1.326, learning rate = 0.00000008\n",
      "15782/85400 (epoch 36), train_loss = -8.61760160, time/batch = 1.333, learning rate = 0.00000008\n",
      "15792/85400 (epoch 36), train_loss = -8.61731064, time/batch = 1.318, learning rate = 0.00000008\n",
      "15809/85400 (epoch 37), train_loss = -8.63871517, time/batch = 1.322, learning rate = 0.00000008\n",
      "15819/85400 (epoch 37), train_loss = -8.62654967, time/batch = 1.322, learning rate = 0.00000008\n",
      "15829/85400 (epoch 37), train_loss = -8.62601776, time/batch = 1.324, learning rate = 0.00000008\n",
      "15839/85400 (epoch 37), train_loss = -8.62659273, time/batch = 1.314, learning rate = 0.00000008\n",
      "15849/85400 (epoch 37), train_loss = -8.62533392, time/batch = 1.312, learning rate = 0.00000008\n",
      "15859/85400 (epoch 37), train_loss = -8.62377369, time/batch = 1.317, learning rate = 0.00000008\n",
      "15869/85400 (epoch 37), train_loss = -8.62289160, time/batch = 1.325, learning rate = 0.00000008\n",
      "15879/85400 (epoch 37), train_loss = -8.62453759, time/batch = 1.322, learning rate = 0.00000008\n",
      "15889/85400 (epoch 37), train_loss = -8.62459318, time/batch = 1.320, learning rate = 0.00000008\n",
      "15899/85400 (epoch 37), train_loss = -8.62161530, time/batch = 1.324, learning rate = 0.00000008\n",
      "15909/85400 (epoch 37), train_loss = -8.61804409, time/batch = 1.322, learning rate = 0.00000008\n",
      "15919/85400 (epoch 37), train_loss = -8.61852242, time/batch = 1.318, learning rate = 0.00000008\n",
      "15929/85400 (epoch 37), train_loss = -8.61876120, time/batch = 1.327, learning rate = 0.00000008\n",
      "15939/85400 (epoch 37), train_loss = -8.61847613, time/batch = 1.315, learning rate = 0.00000008\n",
      "15949/85400 (epoch 37), train_loss = -8.61910839, time/batch = 1.336, learning rate = 0.00000008\n",
      "15959/85400 (epoch 37), train_loss = -8.61949934, time/batch = 1.316, learning rate = 0.00000008\n",
      "15969/85400 (epoch 37), train_loss = -8.62068715, time/batch = 1.327, learning rate = 0.00000008\n",
      "15979/85400 (epoch 37), train_loss = -8.62104162, time/batch = 1.326, learning rate = 0.00000008\n",
      "15989/85400 (epoch 37), train_loss = -8.61911187, time/batch = 1.320, learning rate = 0.00000008\n",
      "15999/85400 (epoch 37), train_loss = -8.61920679, time/batch = 1.325, learning rate = 0.00000008\n",
      "Saving model to model-37-16000.pth\n",
      "16009/85400 (epoch 37), train_loss = -8.61921052, time/batch = 1.319, learning rate = 0.00000008\n",
      "16019/85400 (epoch 37), train_loss = -8.61855258, time/batch = 1.321, learning rate = 0.00000008\n",
      "16029/85400 (epoch 37), train_loss = -8.61889613, time/batch = 1.324, learning rate = 0.00000008\n",
      "16039/85400 (epoch 37), train_loss = -8.61836104, time/batch = 1.315, learning rate = 0.00000008\n",
      "16049/85400 (epoch 37), train_loss = -8.61882948, time/batch = 1.336, learning rate = 0.00000008\n",
      "16059/85400 (epoch 37), train_loss = -8.61846989, time/batch = 1.340, learning rate = 0.00000008\n",
      "16069/85400 (epoch 37), train_loss = -8.61765337, time/batch = 1.330, learning rate = 0.00000008\n",
      "16079/85400 (epoch 37), train_loss = -8.61673476, time/batch = 1.325, learning rate = 0.00000008\n",
      "16089/85400 (epoch 37), train_loss = -8.61652203, time/batch = 1.327, learning rate = 0.00000008\n",
      "16099/85400 (epoch 37), train_loss = -8.61493538, time/batch = 1.332, learning rate = 0.00000008\n",
      "16109/85400 (epoch 37), train_loss = -8.61510832, time/batch = 1.317, learning rate = 0.00000008\n",
      "16119/85400 (epoch 37), train_loss = -8.61610820, time/batch = 1.315, learning rate = 0.00000008\n",
      "16129/85400 (epoch 37), train_loss = -8.61614308, time/batch = 1.326, learning rate = 0.00000008\n",
      "16139/85400 (epoch 37), train_loss = -8.61542703, time/batch = 1.321, learning rate = 0.00000008\n",
      "16149/85400 (epoch 37), train_loss = -8.61574100, time/batch = 1.317, learning rate = 0.00000008\n",
      "16159/85400 (epoch 37), train_loss = -8.61594645, time/batch = 1.341, learning rate = 0.00000008\n",
      "16169/85400 (epoch 37), train_loss = -8.61566226, time/batch = 1.314, learning rate = 0.00000008\n",
      "16179/85400 (epoch 37), train_loss = -8.61606383, time/batch = 1.312, learning rate = 0.00000008\n",
      "16189/85400 (epoch 37), train_loss = -8.61631706, time/batch = 1.313, learning rate = 0.00000008\n",
      "16199/85400 (epoch 37), train_loss = -8.61651666, time/batch = 1.314, learning rate = 0.00000008\n",
      "16209/85400 (epoch 37), train_loss = -8.61660931, time/batch = 1.308, learning rate = 0.00000008\n",
      "16219/85400 (epoch 37), train_loss = -8.61690462, time/batch = 1.325, learning rate = 0.00000008\n",
      "16236/85400 (epoch 38), train_loss = -8.60320845, time/batch = 1.314, learning rate = 0.00000008\n",
      "16246/85400 (epoch 38), train_loss = -8.61869268, time/batch = 1.325, learning rate = 0.00000008\n",
      "16256/85400 (epoch 38), train_loss = -8.62362471, time/batch = 1.324, learning rate = 0.00000008\n",
      "16266/85400 (epoch 38), train_loss = -8.62290325, time/batch = 1.315, learning rate = 0.00000008\n",
      "16276/85400 (epoch 38), train_loss = -8.62428492, time/batch = 1.323, learning rate = 0.00000008\n",
      "16286/85400 (epoch 38), train_loss = -8.62486380, time/batch = 1.322, learning rate = 0.00000008\n",
      "16296/85400 (epoch 38), train_loss = -8.62405570, time/batch = 1.311, learning rate = 0.00000008\n",
      "16306/85400 (epoch 38), train_loss = -8.62412151, time/batch = 1.309, learning rate = 0.00000008\n",
      "16316/85400 (epoch 38), train_loss = -8.62474862, time/batch = 1.314, learning rate = 0.00000008\n",
      "16326/85400 (epoch 38), train_loss = -8.62505611, time/batch = 1.318, learning rate = 0.00000008\n",
      "16336/85400 (epoch 38), train_loss = -8.62379985, time/batch = 1.324, learning rate = 0.00000008\n",
      "16346/85400 (epoch 38), train_loss = -8.62321359, time/batch = 1.328, learning rate = 0.00000008\n",
      "16356/85400 (epoch 38), train_loss = -8.62269344, time/batch = 1.325, learning rate = 0.00000008\n",
      "16366/85400 (epoch 38), train_loss = -8.62165122, time/batch = 1.327, learning rate = 0.00000008\n",
      "16376/85400 (epoch 38), train_loss = -8.62016551, time/batch = 1.320, learning rate = 0.00000008\n",
      "16386/85400 (epoch 38), train_loss = -8.62066496, time/batch = 1.322, learning rate = 0.00000008\n",
      "16396/85400 (epoch 38), train_loss = -8.62088028, time/batch = 1.337, learning rate = 0.00000008\n",
      "16406/85400 (epoch 38), train_loss = -8.62133642, time/batch = 1.311, learning rate = 0.00000008\n",
      "16416/85400 (epoch 38), train_loss = -8.62100168, time/batch = 1.326, learning rate = 0.00000008\n",
      "16426/85400 (epoch 38), train_loss = -8.62195006, time/batch = 1.320, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16436/85400 (epoch 38), train_loss = -8.62178847, time/batch = 1.321, learning rate = 0.00000008\n",
      "16446/85400 (epoch 38), train_loss = -8.62134065, time/batch = 1.323, learning rate = 0.00000008\n",
      "16456/85400 (epoch 38), train_loss = -8.62082064, time/batch = 1.323, learning rate = 0.00000008\n",
      "16466/85400 (epoch 38), train_loss = -8.61987348, time/batch = 1.316, learning rate = 0.00000008\n",
      "16476/85400 (epoch 38), train_loss = -8.62062091, time/batch = 1.321, learning rate = 0.00000008\n",
      "16486/85400 (epoch 38), train_loss = -8.62082400, time/batch = 1.315, learning rate = 0.00000008\n",
      "16496/85400 (epoch 38), train_loss = -8.61977073, time/batch = 1.332, learning rate = 0.00000008\n",
      "Saving model to model-38-16500.pth\n",
      "16506/85400 (epoch 38), train_loss = -8.61814369, time/batch = 1.320, learning rate = 0.00000008\n",
      "16516/85400 (epoch 38), train_loss = -8.61791036, time/batch = 1.316, learning rate = 0.00000008\n",
      "16526/85400 (epoch 38), train_loss = -8.61753535, time/batch = 1.308, learning rate = 0.00000008\n",
      "16536/85400 (epoch 38), train_loss = -8.61796252, time/batch = 1.323, learning rate = 0.00000008\n",
      "16546/85400 (epoch 38), train_loss = -8.61739631, time/batch = 1.316, learning rate = 0.00000008\n",
      "16556/85400 (epoch 38), train_loss = -8.61671169, time/batch = 1.322, learning rate = 0.00000008\n",
      "16566/85400 (epoch 38), train_loss = -8.61709936, time/batch = 1.318, learning rate = 0.00000008\n",
      "16576/85400 (epoch 38), train_loss = -8.61601194, time/batch = 1.314, learning rate = 0.00000008\n",
      "16586/85400 (epoch 38), train_loss = -8.61540938, time/batch = 1.320, learning rate = 0.00000008\n",
      "16596/85400 (epoch 38), train_loss = -8.61571663, time/batch = 1.320, learning rate = 0.00000008\n",
      "16606/85400 (epoch 38), train_loss = -8.61601538, time/batch = 1.310, learning rate = 0.00000008\n",
      "16616/85400 (epoch 38), train_loss = -8.61607102, time/batch = 1.320, learning rate = 0.00000008\n",
      "16626/85400 (epoch 38), train_loss = -8.61549130, time/batch = 1.324, learning rate = 0.00000008\n",
      "16636/85400 (epoch 38), train_loss = -8.61557277, time/batch = 1.321, learning rate = 0.00000008\n",
      "16646/85400 (epoch 38), train_loss = -8.61598525, time/batch = 1.316, learning rate = 0.00000008\n",
      "16663/85400 (epoch 39), train_loss = -8.60811119, time/batch = 1.327, learning rate = 0.00000008\n",
      "16673/85400 (epoch 39), train_loss = -8.62367883, time/batch = 1.309, learning rate = 0.00000008\n",
      "16683/85400 (epoch 39), train_loss = -8.62024981, time/batch = 1.325, learning rate = 0.00000008\n",
      "16693/85400 (epoch 39), train_loss = -8.62186499, time/batch = 1.322, learning rate = 0.00000008\n",
      "16703/85400 (epoch 39), train_loss = -8.61877048, time/batch = 1.317, learning rate = 0.00000008\n",
      "16713/85400 (epoch 39), train_loss = -8.61649094, time/batch = 1.322, learning rate = 0.00000008\n",
      "16723/85400 (epoch 39), train_loss = -8.61936357, time/batch = 1.324, learning rate = 0.00000008\n",
      "16733/85400 (epoch 39), train_loss = -8.61596074, time/batch = 1.316, learning rate = 0.00000008\n",
      "16743/85400 (epoch 39), train_loss = -8.61850637, time/batch = 1.326, learning rate = 0.00000008\n",
      "16753/85400 (epoch 39), train_loss = -8.61805145, time/batch = 1.328, learning rate = 0.00000008\n",
      "16763/85400 (epoch 39), train_loss = -8.61780676, time/batch = 1.320, learning rate = 0.00000008\n",
      "16773/85400 (epoch 39), train_loss = -8.61650845, time/batch = 1.318, learning rate = 0.00000008\n",
      "16783/85400 (epoch 39), train_loss = -8.61691586, time/batch = 1.318, learning rate = 0.00000008\n",
      "16793/85400 (epoch 39), train_loss = -8.61736526, time/batch = 1.311, learning rate = 0.00000008\n",
      "16803/85400 (epoch 39), train_loss = -8.61806852, time/batch = 1.316, learning rate = 0.00000008\n",
      "16813/85400 (epoch 39), train_loss = -8.61732850, time/batch = 1.324, learning rate = 0.00000008\n",
      "16823/85400 (epoch 39), train_loss = -8.61768553, time/batch = 1.317, learning rate = 0.00000008\n",
      "16833/85400 (epoch 39), train_loss = -8.61675710, time/batch = 1.318, learning rate = 0.00000008\n",
      "16843/85400 (epoch 39), train_loss = -8.61672977, time/batch = 1.319, learning rate = 0.00000008\n",
      "16853/85400 (epoch 39), train_loss = -8.61668199, time/batch = 1.314, learning rate = 0.00000008\n",
      "16863/85400 (epoch 39), train_loss = -8.61687035, time/batch = 1.313, learning rate = 0.00000008\n",
      "16873/85400 (epoch 39), train_loss = -8.61715948, time/batch = 1.322, learning rate = 0.00000008\n",
      "16883/85400 (epoch 39), train_loss = -8.61716827, time/batch = 1.316, learning rate = 0.00000008\n",
      "16893/85400 (epoch 39), train_loss = -8.61804352, time/batch = 1.313, learning rate = 0.00000008\n",
      "16903/85400 (epoch 39), train_loss = -8.61817120, time/batch = 1.335, learning rate = 0.00000008\n",
      "16913/85400 (epoch 39), train_loss = -8.61797703, time/batch = 1.326, learning rate = 0.00000008\n",
      "16923/85400 (epoch 39), train_loss = -8.61814041, time/batch = 1.331, learning rate = 0.00000008\n",
      "16933/85400 (epoch 39), train_loss = -8.61855708, time/batch = 1.326, learning rate = 0.00000008\n",
      "16943/85400 (epoch 39), train_loss = -8.61877957, time/batch = 1.333, learning rate = 0.00000008\n",
      "16953/85400 (epoch 39), train_loss = -8.61958199, time/batch = 1.323, learning rate = 0.00000008\n",
      "16963/85400 (epoch 39), train_loss = -8.61880202, time/batch = 1.312, learning rate = 0.00000008\n",
      "16973/85400 (epoch 39), train_loss = -8.61834833, time/batch = 1.321, learning rate = 0.00000008\n",
      "16983/85400 (epoch 39), train_loss = -8.61849947, time/batch = 1.311, learning rate = 0.00000008\n",
      "16993/85400 (epoch 39), train_loss = -8.61917787, time/batch = 1.321, learning rate = 0.00000008\n",
      "Saving model to model-39-17000.pth\n",
      "17003/85400 (epoch 39), train_loss = -8.61892817, time/batch = 1.324, learning rate = 0.00000008\n",
      "17013/85400 (epoch 39), train_loss = -8.61905880, time/batch = 1.318, learning rate = 0.00000008\n",
      "17023/85400 (epoch 39), train_loss = -8.61877827, time/batch = 1.320, learning rate = 0.00000008\n",
      "17033/85400 (epoch 39), train_loss = -8.61888430, time/batch = 1.319, learning rate = 0.00000008\n",
      "17043/85400 (epoch 39), train_loss = -8.61876564, time/batch = 1.321, learning rate = 0.00000008\n",
      "17053/85400 (epoch 39), train_loss = -8.61884289, time/batch = 1.323, learning rate = 0.00000008\n",
      "17063/85400 (epoch 39), train_loss = -8.61891925, time/batch = 1.317, learning rate = 0.00000008\n",
      "17073/85400 (epoch 39), train_loss = -8.61895798, time/batch = 1.321, learning rate = 0.00000008\n",
      "17090/85400 (epoch 40), train_loss = -8.63546896, time/batch = 1.320, learning rate = 0.00000008\n",
      "17100/85400 (epoch 40), train_loss = -8.62760801, time/batch = 1.325, learning rate = 0.00000008\n",
      "17110/85400 (epoch 40), train_loss = -8.62196684, time/batch = 1.314, learning rate = 0.00000008\n",
      "17120/85400 (epoch 40), train_loss = -8.61940932, time/batch = 1.320, learning rate = 0.00000008\n",
      "17130/85400 (epoch 40), train_loss = -8.62140396, time/batch = 1.312, learning rate = 0.00000008\n",
      "17140/85400 (epoch 40), train_loss = -8.61925515, time/batch = 1.324, learning rate = 0.00000008\n",
      "17150/85400 (epoch 40), train_loss = -8.62150286, time/batch = 1.322, learning rate = 0.00000008\n",
      "17160/85400 (epoch 40), train_loss = -8.62169678, time/batch = 1.314, learning rate = 0.00000008\n",
      "17170/85400 (epoch 40), train_loss = -8.62084017, time/batch = 1.323, learning rate = 0.00000008\n",
      "17180/85400 (epoch 40), train_loss = -8.61851434, time/batch = 1.312, learning rate = 0.00000008\n",
      "17190/85400 (epoch 40), train_loss = -8.61820820, time/batch = 1.326, learning rate = 0.00000008\n",
      "17200/85400 (epoch 40), train_loss = -8.61783230, time/batch = 1.323, learning rate = 0.00000008\n",
      "17210/85400 (epoch 40), train_loss = -8.61690112, time/batch = 1.323, learning rate = 0.00000008\n",
      "17220/85400 (epoch 40), train_loss = -8.61640340, time/batch = 1.315, learning rate = 0.00000008\n",
      "17230/85400 (epoch 40), train_loss = -8.61585628, time/batch = 1.320, learning rate = 0.00000008\n",
      "17240/85400 (epoch 40), train_loss = -8.61641922, time/batch = 1.324, learning rate = 0.00000008\n",
      "17250/85400 (epoch 40), train_loss = -8.61615417, time/batch = 1.312, learning rate = 0.00000008\n",
      "17260/85400 (epoch 40), train_loss = -8.61704185, time/batch = 1.321, learning rate = 0.00000008\n",
      "17270/85400 (epoch 40), train_loss = -8.61766985, time/batch = 1.322, learning rate = 0.00000008\n",
      "17280/85400 (epoch 40), train_loss = -8.61614611, time/batch = 1.317, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17290/85400 (epoch 40), train_loss = -8.61577349, time/batch = 1.319, learning rate = 0.00000008\n",
      "17300/85400 (epoch 40), train_loss = -8.61705012, time/batch = 1.323, learning rate = 0.00000008\n",
      "17310/85400 (epoch 40), train_loss = -8.61715056, time/batch = 1.327, learning rate = 0.00000008\n",
      "17320/85400 (epoch 40), train_loss = -8.61705796, time/batch = 1.311, learning rate = 0.00000008\n",
      "17330/85400 (epoch 40), train_loss = -8.61691437, time/batch = 1.324, learning rate = 0.00000008\n",
      "17340/85400 (epoch 40), train_loss = -8.61770307, time/batch = 1.321, learning rate = 0.00000008\n",
      "17350/85400 (epoch 40), train_loss = -8.61840628, time/batch = 1.327, learning rate = 0.00000008\n",
      "17360/85400 (epoch 40), train_loss = -8.61724628, time/batch = 1.322, learning rate = 0.00000008\n",
      "17370/85400 (epoch 40), train_loss = -8.61663460, time/batch = 1.328, learning rate = 0.00000008\n",
      "17380/85400 (epoch 40), train_loss = -8.61741155, time/batch = 1.327, learning rate = 0.00000008\n",
      "17390/85400 (epoch 40), train_loss = -8.61736691, time/batch = 1.325, learning rate = 0.00000008\n",
      "17400/85400 (epoch 40), train_loss = -8.61736676, time/batch = 1.325, learning rate = 0.00000008\n",
      "17410/85400 (epoch 40), train_loss = -8.61747701, time/batch = 1.319, learning rate = 0.00000008\n",
      "17420/85400 (epoch 40), train_loss = -8.61752560, time/batch = 1.321, learning rate = 0.00000008\n",
      "17430/85400 (epoch 40), train_loss = -8.61739362, time/batch = 1.324, learning rate = 0.00000008\n",
      "17440/85400 (epoch 40), train_loss = -8.61746311, time/batch = 1.315, learning rate = 0.00000008\n",
      "17450/85400 (epoch 40), train_loss = -8.61722143, time/batch = 1.323, learning rate = 0.00000008\n",
      "17460/85400 (epoch 40), train_loss = -8.61761748, time/batch = 1.311, learning rate = 0.00000008\n",
      "17470/85400 (epoch 40), train_loss = -8.61784672, time/batch = 1.319, learning rate = 0.00000008\n",
      "17480/85400 (epoch 40), train_loss = -8.61765699, time/batch = 1.311, learning rate = 0.00000008\n",
      "17490/85400 (epoch 40), train_loss = -8.61822426, time/batch = 1.326, learning rate = 0.00000008\n",
      "17500/85400 (epoch 40), train_loss = -8.61791279, time/batch = 1.326, learning rate = 0.00000008\n",
      "Saving model to model-40-17500.pth\n",
      "17517/85400 (epoch 41), train_loss = -8.61993494, time/batch = 1.328, learning rate = 0.00000008\n",
      "17527/85400 (epoch 41), train_loss = -8.62486491, time/batch = 1.319, learning rate = 0.00000008\n",
      "17537/85400 (epoch 41), train_loss = -8.61781317, time/batch = 1.316, learning rate = 0.00000008\n",
      "17547/85400 (epoch 41), train_loss = -8.61961737, time/batch = 1.314, learning rate = 0.00000008\n",
      "17557/85400 (epoch 41), train_loss = -8.61447485, time/batch = 1.327, learning rate = 0.00000008\n",
      "17567/85400 (epoch 41), train_loss = -8.61719926, time/batch = 1.309, learning rate = 0.00000008\n",
      "17577/85400 (epoch 41), train_loss = -8.61443589, time/batch = 1.316, learning rate = 0.00000008\n",
      "17587/85400 (epoch 41), train_loss = -8.61426454, time/batch = 1.318, learning rate = 0.00000008\n",
      "17597/85400 (epoch 41), train_loss = -8.61445784, time/batch = 1.324, learning rate = 0.00000008\n",
      "17607/85400 (epoch 41), train_loss = -8.61539174, time/batch = 1.318, learning rate = 0.00000008\n",
      "17617/85400 (epoch 41), train_loss = -8.61721575, time/batch = 1.330, learning rate = 0.00000008\n",
      "17627/85400 (epoch 41), train_loss = -8.61879249, time/batch = 1.311, learning rate = 0.00000008\n",
      "17637/85400 (epoch 41), train_loss = -8.61914025, time/batch = 1.316, learning rate = 0.00000008\n",
      "17647/85400 (epoch 41), train_loss = -8.61978945, time/batch = 1.322, learning rate = 0.00000008\n",
      "17657/85400 (epoch 41), train_loss = -8.62007694, time/batch = 1.328, learning rate = 0.00000008\n",
      "17667/85400 (epoch 41), train_loss = -8.61784830, time/batch = 1.313, learning rate = 0.00000008\n",
      "17677/85400 (epoch 41), train_loss = -8.61682432, time/batch = 1.329, learning rate = 0.00000008\n",
      "17687/85400 (epoch 41), train_loss = -8.61544884, time/batch = 1.403, learning rate = 0.00000008\n",
      "17697/85400 (epoch 41), train_loss = -8.61500099, time/batch = 1.328, learning rate = 0.00000008\n",
      "17707/85400 (epoch 41), train_loss = -8.61547754, time/batch = 1.319, learning rate = 0.00000008\n",
      "17717/85400 (epoch 41), train_loss = -8.61572339, time/batch = 1.314, learning rate = 0.00000008\n",
      "17727/85400 (epoch 41), train_loss = -8.61641807, time/batch = 1.323, learning rate = 0.00000008\n",
      "17737/85400 (epoch 41), train_loss = -8.61713688, time/batch = 1.315, learning rate = 0.00000008\n",
      "17747/85400 (epoch 41), train_loss = -8.61719714, time/batch = 1.321, learning rate = 0.00000008\n",
      "17757/85400 (epoch 41), train_loss = -8.61711835, time/batch = 1.315, learning rate = 0.00000008\n",
      "17767/85400 (epoch 41), train_loss = -8.61724313, time/batch = 1.325, learning rate = 0.00000008\n",
      "17777/85400 (epoch 41), train_loss = -8.61759016, time/batch = 1.312, learning rate = 0.00000008\n",
      "17787/85400 (epoch 41), train_loss = -8.61783530, time/batch = 1.314, learning rate = 0.00000008\n",
      "17797/85400 (epoch 41), train_loss = -8.61767004, time/batch = 1.325, learning rate = 0.00000008\n",
      "17807/85400 (epoch 41), train_loss = -8.61864716, time/batch = 1.326, learning rate = 0.00000008\n",
      "17817/85400 (epoch 41), train_loss = -8.61835144, time/batch = 1.314, learning rate = 0.00000008\n",
      "17827/85400 (epoch 41), train_loss = -8.61845057, time/batch = 1.319, learning rate = 0.00000008\n",
      "17837/85400 (epoch 41), train_loss = -8.61773635, time/batch = 1.321, learning rate = 0.00000008\n",
      "17847/85400 (epoch 41), train_loss = -8.61796427, time/batch = 1.329, learning rate = 0.00000008\n",
      "17857/85400 (epoch 41), train_loss = -8.61799267, time/batch = 1.319, learning rate = 0.00000008\n",
      "17867/85400 (epoch 41), train_loss = -8.61823170, time/batch = 1.325, learning rate = 0.00000008\n",
      "17877/85400 (epoch 41), train_loss = -8.61834202, time/batch = 1.321, learning rate = 0.00000008\n",
      "17887/85400 (epoch 41), train_loss = -8.61841255, time/batch = 1.314, learning rate = 0.00000008\n",
      "17897/85400 (epoch 41), train_loss = -8.61843988, time/batch = 1.320, learning rate = 0.00000008\n",
      "17907/85400 (epoch 41), train_loss = -8.61838896, time/batch = 1.323, learning rate = 0.00000008\n",
      "17917/85400 (epoch 41), train_loss = -8.61858804, time/batch = 1.323, learning rate = 0.00000008\n",
      "17927/85400 (epoch 41), train_loss = -8.61881325, time/batch = 1.314, learning rate = 0.00000008\n",
      "17944/85400 (epoch 42), train_loss = -8.62802477, time/batch = 1.317, learning rate = 0.00000008\n",
      "17954/85400 (epoch 42), train_loss = -8.62545609, time/batch = 1.314, learning rate = 0.00000008\n",
      "17964/85400 (epoch 42), train_loss = -8.62159440, time/batch = 1.313, learning rate = 0.00000008\n",
      "17974/85400 (epoch 42), train_loss = -8.61956940, time/batch = 1.312, learning rate = 0.00000008\n",
      "17984/85400 (epoch 42), train_loss = -8.62151295, time/batch = 1.324, learning rate = 0.00000008\n",
      "17994/85400 (epoch 42), train_loss = -8.62275942, time/batch = 1.314, learning rate = 0.00000008\n",
      "Saving model to model-42-18000.pth\n",
      "18004/85400 (epoch 42), train_loss = -8.62140583, time/batch = 1.324, learning rate = 0.00000008\n",
      "18014/85400 (epoch 42), train_loss = -8.62239676, time/batch = 1.329, learning rate = 0.00000008\n",
      "18024/85400 (epoch 42), train_loss = -8.62453409, time/batch = 1.325, learning rate = 0.00000008\n",
      "18034/85400 (epoch 42), train_loss = -8.62155587, time/batch = 1.322, learning rate = 0.00000008\n",
      "18044/85400 (epoch 42), train_loss = -8.62080958, time/batch = 1.313, learning rate = 0.00000008\n",
      "18054/85400 (epoch 42), train_loss = -8.62120925, time/batch = 1.319, learning rate = 0.00000008\n",
      "18064/85400 (epoch 42), train_loss = -8.62052606, time/batch = 1.320, learning rate = 0.00000008\n",
      "18074/85400 (epoch 42), train_loss = -8.61870300, time/batch = 1.319, learning rate = 0.00000008\n",
      "18084/85400 (epoch 42), train_loss = -8.61909264, time/batch = 1.312, learning rate = 0.00000008\n",
      "18094/85400 (epoch 42), train_loss = -8.61835732, time/batch = 1.310, learning rate = 0.00000008\n",
      "18104/85400 (epoch 42), train_loss = -8.61783592, time/batch = 1.324, learning rate = 0.00000008\n",
      "18114/85400 (epoch 42), train_loss = -8.61675729, time/batch = 1.318, learning rate = 0.00000008\n",
      "18124/85400 (epoch 42), train_loss = -8.61783634, time/batch = 1.319, learning rate = 0.00000008\n",
      "18134/85400 (epoch 42), train_loss = -8.61907332, time/batch = 1.316, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18144/85400 (epoch 42), train_loss = -8.61986477, time/batch = 1.324, learning rate = 0.00000008\n",
      "18154/85400 (epoch 42), train_loss = -8.61945908, time/batch = 1.321, learning rate = 0.00000008\n",
      "18164/85400 (epoch 42), train_loss = -8.61980553, time/batch = 1.318, learning rate = 0.00000008\n",
      "18174/85400 (epoch 42), train_loss = -8.61958034, time/batch = 1.326, learning rate = 0.00000008\n",
      "18184/85400 (epoch 42), train_loss = -8.61891058, time/batch = 1.329, learning rate = 0.00000008\n",
      "18194/85400 (epoch 42), train_loss = -8.61705302, time/batch = 1.335, learning rate = 0.00000008\n",
      "18204/85400 (epoch 42), train_loss = -8.61761343, time/batch = 1.312, learning rate = 0.00000008\n",
      "18214/85400 (epoch 42), train_loss = -8.61833329, time/batch = 1.320, learning rate = 0.00000008\n",
      "18224/85400 (epoch 42), train_loss = -8.61913338, time/batch = 1.312, learning rate = 0.00000008\n",
      "18234/85400 (epoch 42), train_loss = -8.61961893, time/batch = 1.326, learning rate = 0.00000008\n",
      "18244/85400 (epoch 42), train_loss = -8.61938700, time/batch = 1.318, learning rate = 0.00000008\n",
      "18254/85400 (epoch 42), train_loss = -8.61954688, time/batch = 1.322, learning rate = 0.00000008\n",
      "18264/85400 (epoch 42), train_loss = -8.61956829, time/batch = 1.325, learning rate = 0.00000008\n",
      "18274/85400 (epoch 42), train_loss = -8.61874510, time/batch = 1.325, learning rate = 0.00000008\n",
      "18284/85400 (epoch 42), train_loss = -8.61806216, time/batch = 1.319, learning rate = 0.00000008\n",
      "18294/85400 (epoch 42), train_loss = -8.61777384, time/batch = 1.313, learning rate = 0.00000008\n",
      "18304/85400 (epoch 42), train_loss = -8.61751095, time/batch = 1.321, learning rate = 0.00000008\n",
      "18314/85400 (epoch 42), train_loss = -8.61736128, time/batch = 1.320, learning rate = 0.00000008\n",
      "18324/85400 (epoch 42), train_loss = -8.61754238, time/batch = 1.326, learning rate = 0.00000008\n",
      "18334/85400 (epoch 42), train_loss = -8.61775493, time/batch = 1.317, learning rate = 0.00000008\n",
      "18344/85400 (epoch 42), train_loss = -8.61793525, time/batch = 1.318, learning rate = 0.00000008\n",
      "18354/85400 (epoch 42), train_loss = -8.61790062, time/batch = 1.310, learning rate = 0.00000008\n",
      "18371/85400 (epoch 43), train_loss = -8.59835596, time/batch = 1.312, learning rate = 0.00000008\n",
      "18381/85400 (epoch 43), train_loss = -8.61550250, time/batch = 1.327, learning rate = 0.00000008\n",
      "18391/85400 (epoch 43), train_loss = -8.62371092, time/batch = 1.325, learning rate = 0.00000008\n",
      "18401/85400 (epoch 43), train_loss = -8.61937256, time/batch = 1.326, learning rate = 0.00000008\n",
      "18411/85400 (epoch 43), train_loss = -8.62392351, time/batch = 1.330, learning rate = 0.00000008\n",
      "18421/85400 (epoch 43), train_loss = -8.62777775, time/batch = 1.316, learning rate = 0.00000008\n",
      "18431/85400 (epoch 43), train_loss = -8.62407895, time/batch = 1.318, learning rate = 0.00000008\n",
      "18441/85400 (epoch 43), train_loss = -8.62222390, time/batch = 1.326, learning rate = 0.00000008\n",
      "18451/85400 (epoch 43), train_loss = -8.61909253, time/batch = 1.320, learning rate = 0.00000008\n",
      "18461/85400 (epoch 43), train_loss = -8.62047731, time/batch = 1.326, learning rate = 0.00000008\n",
      "18471/85400 (epoch 43), train_loss = -8.61992781, time/batch = 1.327, learning rate = 0.00000008\n",
      "18481/85400 (epoch 43), train_loss = -8.61893029, time/batch = 1.326, learning rate = 0.00000008\n",
      "18491/85400 (epoch 43), train_loss = -8.61910953, time/batch = 1.326, learning rate = 0.00000008\n",
      "Saving model to model-43-18500.pth\n",
      "18501/85400 (epoch 43), train_loss = -8.61964151, time/batch = 1.328, learning rate = 0.00000008\n",
      "18511/85400 (epoch 43), train_loss = -8.61883455, time/batch = 1.311, learning rate = 0.00000008\n",
      "18521/85400 (epoch 43), train_loss = -8.61842623, time/batch = 1.315, learning rate = 0.00000008\n",
      "18531/85400 (epoch 43), train_loss = -8.61882201, time/batch = 1.316, learning rate = 0.00000008\n",
      "18541/85400 (epoch 43), train_loss = -8.61922683, time/batch = 1.316, learning rate = 0.00000008\n",
      "18551/85400 (epoch 43), train_loss = -8.61929224, time/batch = 1.330, learning rate = 0.00000008\n",
      "18561/85400 (epoch 43), train_loss = -8.61983378, time/batch = 1.323, learning rate = 0.00000008\n",
      "18571/85400 (epoch 43), train_loss = -8.61962725, time/batch = 1.309, learning rate = 0.00000008\n",
      "18581/85400 (epoch 43), train_loss = -8.61866938, time/batch = 1.321, learning rate = 0.00000008\n",
      "18591/85400 (epoch 43), train_loss = -8.61850773, time/batch = 1.321, learning rate = 0.00000008\n",
      "18601/85400 (epoch 43), train_loss = -8.61716771, time/batch = 1.328, learning rate = 0.00000008\n",
      "18611/85400 (epoch 43), train_loss = -8.61761197, time/batch = 1.332, learning rate = 0.00000008\n",
      "18621/85400 (epoch 43), train_loss = -8.61847299, time/batch = 1.329, learning rate = 0.00000008\n",
      "18631/85400 (epoch 43), train_loss = -8.61878277, time/batch = 1.318, learning rate = 0.00000008\n",
      "18641/85400 (epoch 43), train_loss = -8.61862571, time/batch = 1.323, learning rate = 0.00000008\n",
      "18651/85400 (epoch 43), train_loss = -8.61788125, time/batch = 1.314, learning rate = 0.00000008\n",
      "18661/85400 (epoch 43), train_loss = -8.61727804, time/batch = 1.324, learning rate = 0.00000008\n",
      "18671/85400 (epoch 43), train_loss = -8.61739739, time/batch = 1.324, learning rate = 0.00000008\n",
      "18681/85400 (epoch 43), train_loss = -8.61741569, time/batch = 1.318, learning rate = 0.00000008\n",
      "18691/85400 (epoch 43), train_loss = -8.61722054, time/batch = 1.318, learning rate = 0.00000008\n",
      "18701/85400 (epoch 43), train_loss = -8.61752660, time/batch = 1.316, learning rate = 0.00000008\n",
      "18711/85400 (epoch 43), train_loss = -8.61784212, time/batch = 1.318, learning rate = 0.00000008\n",
      "18721/85400 (epoch 43), train_loss = -8.61732661, time/batch = 1.323, learning rate = 0.00000008\n",
      "18731/85400 (epoch 43), train_loss = -8.61772085, time/batch = 1.326, learning rate = 0.00000008\n",
      "18741/85400 (epoch 43), train_loss = -8.61792160, time/batch = 1.327, learning rate = 0.00000008\n",
      "18751/85400 (epoch 43), train_loss = -8.61760137, time/batch = 1.323, learning rate = 0.00000008\n",
      "18761/85400 (epoch 43), train_loss = -8.61800512, time/batch = 1.323, learning rate = 0.00000008\n",
      "18771/85400 (epoch 43), train_loss = -8.61788105, time/batch = 1.314, learning rate = 0.00000008\n",
      "18781/85400 (epoch 43), train_loss = -8.61757381, time/batch = 1.317, learning rate = 0.00000008\n",
      "18798/85400 (epoch 44), train_loss = -8.61056938, time/batch = 1.319, learning rate = 0.00000008\n",
      "18808/85400 (epoch 44), train_loss = -8.61613636, time/batch = 1.318, learning rate = 0.00000008\n",
      "18818/85400 (epoch 44), train_loss = -8.60328563, time/batch = 1.326, learning rate = 0.00000008\n",
      "18828/85400 (epoch 44), train_loss = -8.60984693, time/batch = 1.324, learning rate = 0.00000008\n",
      "18838/85400 (epoch 44), train_loss = -8.60975136, time/batch = 1.318, learning rate = 0.00000008\n",
      "18848/85400 (epoch 44), train_loss = -8.61270485, time/batch = 1.318, learning rate = 0.00000008\n",
      "18858/85400 (epoch 44), train_loss = -8.61054348, time/batch = 1.312, learning rate = 0.00000008\n",
      "18868/85400 (epoch 44), train_loss = -8.61173882, time/batch = 1.317, learning rate = 0.00000008\n",
      "18878/85400 (epoch 44), train_loss = -8.61410977, time/batch = 1.334, learning rate = 0.00000008\n",
      "18888/85400 (epoch 44), train_loss = -8.61283175, time/batch = 1.327, learning rate = 0.00000008\n",
      "18898/85400 (epoch 44), train_loss = -8.61469787, time/batch = 1.326, learning rate = 0.00000008\n",
      "18908/85400 (epoch 44), train_loss = -8.61593600, time/batch = 1.313, learning rate = 0.00000008\n",
      "18918/85400 (epoch 44), train_loss = -8.61562793, time/batch = 1.317, learning rate = 0.00000008\n",
      "18928/85400 (epoch 44), train_loss = -8.61546118, time/batch = 1.322, learning rate = 0.00000008\n",
      "18938/85400 (epoch 44), train_loss = -8.61644679, time/batch = 1.330, learning rate = 0.00000008\n",
      "18948/85400 (epoch 44), train_loss = -8.61675456, time/batch = 1.321, learning rate = 0.00000008\n",
      "18958/85400 (epoch 44), train_loss = -8.61667131, time/batch = 1.312, learning rate = 0.00000008\n",
      "18968/85400 (epoch 44), train_loss = -8.61667702, time/batch = 1.314, learning rate = 0.00000008\n",
      "18978/85400 (epoch 44), train_loss = -8.61778049, time/batch = 1.486, learning rate = 0.00000008\n",
      "18988/85400 (epoch 44), train_loss = -8.61628193, time/batch = 1.323, learning rate = 0.00000008\n",
      "18998/85400 (epoch 44), train_loss = -8.61550180, time/batch = 1.325, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to model-44-19000.pth\n",
      "19008/85400 (epoch 44), train_loss = -8.61598009, time/batch = 1.318, learning rate = 0.00000008\n",
      "19018/85400 (epoch 44), train_loss = -8.61729552, time/batch = 1.321, learning rate = 0.00000008\n",
      "19028/85400 (epoch 44), train_loss = -8.61761816, time/batch = 1.321, learning rate = 0.00000008\n",
      "19038/85400 (epoch 44), train_loss = -8.61719210, time/batch = 1.318, learning rate = 0.00000008\n",
      "19048/85400 (epoch 44), train_loss = -8.61758362, time/batch = 1.323, learning rate = 0.00000008\n",
      "19058/85400 (epoch 44), train_loss = -8.61736379, time/batch = 1.317, learning rate = 0.00000008\n",
      "19068/85400 (epoch 44), train_loss = -8.61740903, time/batch = 1.322, learning rate = 0.00000008\n",
      "19078/85400 (epoch 44), train_loss = -8.61550127, time/batch = 1.313, learning rate = 0.00000008\n",
      "19088/85400 (epoch 44), train_loss = -8.61579688, time/batch = 1.314, learning rate = 0.00000008\n",
      "19098/85400 (epoch 44), train_loss = -8.61564187, time/batch = 1.327, learning rate = 0.00000008\n",
      "19108/85400 (epoch 44), train_loss = -8.61588988, time/batch = 1.325, learning rate = 0.00000008\n",
      "19118/85400 (epoch 44), train_loss = -8.61517902, time/batch = 1.324, learning rate = 0.00000008\n",
      "19128/85400 (epoch 44), train_loss = -8.61491885, time/batch = 1.315, learning rate = 0.00000008\n",
      "19138/85400 (epoch 44), train_loss = -8.61511266, time/batch = 1.327, learning rate = 0.00000008\n",
      "19148/85400 (epoch 44), train_loss = -8.61561075, time/batch = 1.339, learning rate = 0.00000008\n",
      "19158/85400 (epoch 44), train_loss = -8.61622460, time/batch = 1.311, learning rate = 0.00000008\n",
      "19168/85400 (epoch 44), train_loss = -8.61657167, time/batch = 1.317, learning rate = 0.00000008\n",
      "19178/85400 (epoch 44), train_loss = -8.61684465, time/batch = 1.323, learning rate = 0.00000008\n",
      "19188/85400 (epoch 44), train_loss = -8.61730925, time/batch = 1.323, learning rate = 0.00000008\n",
      "19198/85400 (epoch 44), train_loss = -8.61734762, time/batch = 1.324, learning rate = 0.00000008\n",
      "19208/85400 (epoch 44), train_loss = -8.61761411, time/batch = 1.323, learning rate = 0.00000008\n",
      "19225/85400 (epoch 45), train_loss = -8.61388016, time/batch = 1.329, learning rate = 0.00000008\n",
      "19235/85400 (epoch 45), train_loss = -8.61714816, time/batch = 1.316, learning rate = 0.00000008\n",
      "19245/85400 (epoch 45), train_loss = -8.61368157, time/batch = 1.325, learning rate = 0.00000008\n",
      "19255/85400 (epoch 45), train_loss = -8.61805925, time/batch = 1.318, learning rate = 0.00000008\n",
      "19265/85400 (epoch 45), train_loss = -8.61617039, time/batch = 1.314, learning rate = 0.00000008\n",
      "19275/85400 (epoch 45), train_loss = -8.61474964, time/batch = 1.320, learning rate = 0.00000008\n",
      "19285/85400 (epoch 45), train_loss = -8.61691222, time/batch = 1.332, learning rate = 0.00000008\n",
      "19295/85400 (epoch 45), train_loss = -8.61917354, time/batch = 1.309, learning rate = 0.00000008\n",
      "19305/85400 (epoch 45), train_loss = -8.61770336, time/batch = 1.317, learning rate = 0.00000008\n",
      "19315/85400 (epoch 45), train_loss = -8.61953299, time/batch = 1.311, learning rate = 0.00000008\n",
      "19325/85400 (epoch 45), train_loss = -8.61807475, time/batch = 1.314, learning rate = 0.00000008\n",
      "19335/85400 (epoch 45), train_loss = -8.61908010, time/batch = 1.323, learning rate = 0.00000008\n",
      "19345/85400 (epoch 45), train_loss = -8.61951810, time/batch = 1.313, learning rate = 0.00000008\n",
      "19355/85400 (epoch 45), train_loss = -8.61987938, time/batch = 1.323, learning rate = 0.00000008\n",
      "19365/85400 (epoch 45), train_loss = -8.62015249, time/batch = 1.314, learning rate = 0.00000008\n",
      "19375/85400 (epoch 45), train_loss = -8.62106344, time/batch = 1.315, learning rate = 0.00000008\n",
      "19385/85400 (epoch 45), train_loss = -8.62110505, time/batch = 1.320, learning rate = 0.00000008\n",
      "19395/85400 (epoch 45), train_loss = -8.62131974, time/batch = 1.326, learning rate = 0.00000008\n",
      "19405/85400 (epoch 45), train_loss = -8.62260914, time/batch = 1.322, learning rate = 0.00000008\n",
      "19415/85400 (epoch 45), train_loss = -8.62193343, time/batch = 1.318, learning rate = 0.00000008\n",
      "19425/85400 (epoch 45), train_loss = -8.62193787, time/batch = 1.325, learning rate = 0.00000008\n",
      "19435/85400 (epoch 45), train_loss = -8.62187235, time/batch = 1.322, learning rate = 0.00000008\n",
      "19445/85400 (epoch 45), train_loss = -8.62276706, time/batch = 1.328, learning rate = 0.00000008\n",
      "19455/85400 (epoch 45), train_loss = -8.62161794, time/batch = 1.323, learning rate = 0.00000008\n",
      "19465/85400 (epoch 45), train_loss = -8.62061758, time/batch = 1.321, learning rate = 0.00000008\n",
      "19475/85400 (epoch 45), train_loss = -8.61887118, time/batch = 1.326, learning rate = 0.00000008\n",
      "19485/85400 (epoch 45), train_loss = -8.61862561, time/batch = 1.318, learning rate = 0.00000008\n",
      "19495/85400 (epoch 45), train_loss = -8.61862264, time/batch = 1.322, learning rate = 0.00000008\n",
      "Saving model to model-45-19500.pth\n",
      "19505/85400 (epoch 45), train_loss = -8.61841338, time/batch = 1.325, learning rate = 0.00000008\n",
      "19515/85400 (epoch 45), train_loss = -8.61854154, time/batch = 1.322, learning rate = 0.00000008\n",
      "19525/85400 (epoch 45), train_loss = -8.61946977, time/batch = 1.318, learning rate = 0.00000008\n",
      "19535/85400 (epoch 45), train_loss = -8.61983900, time/batch = 1.321, learning rate = 0.00000008\n",
      "19545/85400 (epoch 45), train_loss = -8.62036185, time/batch = 1.315, learning rate = 0.00000008\n",
      "19555/85400 (epoch 45), train_loss = -8.62058375, time/batch = 1.315, learning rate = 0.00000008\n",
      "19565/85400 (epoch 45), train_loss = -8.62034815, time/batch = 1.320, learning rate = 0.00000008\n",
      "19575/85400 (epoch 45), train_loss = -8.61928144, time/batch = 1.314, learning rate = 0.00000008\n",
      "19585/85400 (epoch 45), train_loss = -8.61876849, time/batch = 1.322, learning rate = 0.00000008\n",
      "19595/85400 (epoch 45), train_loss = -8.61906296, time/batch = 1.326, learning rate = 0.00000008\n",
      "19605/85400 (epoch 45), train_loss = -8.61876411, time/batch = 1.317, learning rate = 0.00000008\n",
      "19615/85400 (epoch 45), train_loss = -8.61929875, time/batch = 1.328, learning rate = 0.00000008\n",
      "19625/85400 (epoch 45), train_loss = -8.61889514, time/batch = 1.321, learning rate = 0.00000008\n",
      "19635/85400 (epoch 45), train_loss = -8.61891779, time/batch = 1.318, learning rate = 0.00000008\n",
      "19652/85400 (epoch 46), train_loss = -8.60252457, time/batch = 1.321, learning rate = 0.00000008\n",
      "19662/85400 (epoch 46), train_loss = -8.61052160, time/batch = 1.315, learning rate = 0.00000008\n",
      "19672/85400 (epoch 46), train_loss = -8.61819636, time/batch = 1.323, learning rate = 0.00000008\n",
      "19682/85400 (epoch 46), train_loss = -8.62270029, time/batch = 1.312, learning rate = 0.00000008\n",
      "19692/85400 (epoch 46), train_loss = -8.62027084, time/batch = 1.316, learning rate = 0.00000008\n",
      "19702/85400 (epoch 46), train_loss = -8.62020308, time/batch = 1.313, learning rate = 0.00000008\n",
      "19712/85400 (epoch 46), train_loss = -8.61809529, time/batch = 1.314, learning rate = 0.00000008\n",
      "19722/85400 (epoch 46), train_loss = -8.61871263, time/batch = 1.318, learning rate = 0.00000008\n",
      "19732/85400 (epoch 46), train_loss = -8.61928063, time/batch = 1.312, learning rate = 0.00000008\n",
      "19742/85400 (epoch 46), train_loss = -8.62071210, time/batch = 1.321, learning rate = 0.00000008\n",
      "19752/85400 (epoch 46), train_loss = -8.62053577, time/batch = 1.317, learning rate = 0.00000008\n",
      "19762/85400 (epoch 46), train_loss = -8.62063031, time/batch = 1.323, learning rate = 0.00000008\n",
      "19772/85400 (epoch 46), train_loss = -8.61963253, time/batch = 1.327, learning rate = 0.00000008\n",
      "19782/85400 (epoch 46), train_loss = -8.62081960, time/batch = 1.327, learning rate = 0.00000008\n",
      "19792/85400 (epoch 46), train_loss = -8.61965611, time/batch = 1.314, learning rate = 0.00000008\n",
      "19802/85400 (epoch 46), train_loss = -8.62076465, time/batch = 1.322, learning rate = 0.00000008\n",
      "19812/85400 (epoch 46), train_loss = -8.62051359, time/batch = 1.318, learning rate = 0.00000008\n",
      "19822/85400 (epoch 46), train_loss = -8.62001691, time/batch = 1.326, learning rate = 0.00000008\n",
      "19832/85400 (epoch 46), train_loss = -8.62108702, time/batch = 1.325, learning rate = 0.00000008\n",
      "19842/85400 (epoch 46), train_loss = -8.62047951, time/batch = 1.322, learning rate = 0.00000008\n",
      "19852/85400 (epoch 46), train_loss = -8.62062440, time/batch = 1.324, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19862/85400 (epoch 46), train_loss = -8.62058298, time/batch = 1.313, learning rate = 0.00000008\n",
      "19872/85400 (epoch 46), train_loss = -8.62055302, time/batch = 1.320, learning rate = 0.00000008\n",
      "19882/85400 (epoch 46), train_loss = -8.62014938, time/batch = 1.323, learning rate = 0.00000008\n",
      "19892/85400 (epoch 46), train_loss = -8.62079733, time/batch = 1.323, learning rate = 0.00000008\n",
      "19902/85400 (epoch 46), train_loss = -8.61974954, time/batch = 1.321, learning rate = 0.00000008\n",
      "19912/85400 (epoch 46), train_loss = -8.61992437, time/batch = 1.316, learning rate = 0.00000008\n",
      "19922/85400 (epoch 46), train_loss = -8.61973197, time/batch = 1.322, learning rate = 0.00000008\n",
      "19932/85400 (epoch 46), train_loss = -8.61863653, time/batch = 1.318, learning rate = 0.00000008\n",
      "19942/85400 (epoch 46), train_loss = -8.61850559, time/batch = 1.314, learning rate = 0.00000008\n",
      "19952/85400 (epoch 46), train_loss = -8.61828080, time/batch = 1.320, learning rate = 0.00000008\n",
      "19962/85400 (epoch 46), train_loss = -8.61793859, time/batch = 1.317, learning rate = 0.00000008\n",
      "19972/85400 (epoch 46), train_loss = -8.61797366, time/batch = 1.318, learning rate = 0.00000008\n",
      "19982/85400 (epoch 46), train_loss = -8.61825460, time/batch = 1.322, learning rate = 0.00000008\n",
      "19992/85400 (epoch 46), train_loss = -8.61750831, time/batch = 1.313, learning rate = 0.00000008\n",
      "Saving model to model-46-20000.pth\n",
      "20002/85400 (epoch 46), train_loss = -8.61719761, time/batch = 1.312, learning rate = 0.00000008\n",
      "20012/85400 (epoch 46), train_loss = -8.61771300, time/batch = 1.317, learning rate = 0.00000008\n",
      "20022/85400 (epoch 46), train_loss = -8.61751658, time/batch = 1.315, learning rate = 0.00000008\n",
      "20032/85400 (epoch 46), train_loss = -8.61784619, time/batch = 1.325, learning rate = 0.00000008\n",
      "20042/85400 (epoch 46), train_loss = -8.61817090, time/batch = 1.320, learning rate = 0.00000008\n",
      "20052/85400 (epoch 46), train_loss = -8.61869728, time/batch = 1.315, learning rate = 0.00000008\n",
      "20062/85400 (epoch 46), train_loss = -8.61848999, time/batch = 1.320, learning rate = 0.00000008\n",
      "20079/85400 (epoch 47), train_loss = -8.59306097, time/batch = 1.331, learning rate = 0.00000008\n",
      "20089/85400 (epoch 47), train_loss = -8.58261299, time/batch = 1.322, learning rate = 0.00000008\n",
      "20099/85400 (epoch 47), train_loss = -8.59756196, time/batch = 1.325, learning rate = 0.00000008\n",
      "20109/85400 (epoch 47), train_loss = -8.60259411, time/batch = 1.327, learning rate = 0.00000008\n",
      "20119/85400 (epoch 47), train_loss = -8.60594730, time/batch = 1.315, learning rate = 0.00000008\n",
      "20129/85400 (epoch 47), train_loss = -8.60818400, time/batch = 1.325, learning rate = 0.00000008\n",
      "20139/85400 (epoch 47), train_loss = -8.60895959, time/batch = 1.320, learning rate = 0.00000008\n",
      "20149/85400 (epoch 47), train_loss = -8.61090180, time/batch = 1.329, learning rate = 0.00000008\n",
      "20159/85400 (epoch 47), train_loss = -8.60851063, time/batch = 1.314, learning rate = 0.00000008\n",
      "20169/85400 (epoch 47), train_loss = -8.60780406, time/batch = 1.323, learning rate = 0.00000008\n",
      "20179/85400 (epoch 47), train_loss = -8.60959720, time/batch = 1.325, learning rate = 0.00000008\n",
      "20189/85400 (epoch 47), train_loss = -8.60852467, time/batch = 1.325, learning rate = 0.00000008\n",
      "20199/85400 (epoch 47), train_loss = -8.61017165, time/batch = 1.321, learning rate = 0.00000008\n",
      "20209/85400 (epoch 47), train_loss = -8.61054971, time/batch = 1.323, learning rate = 0.00000008\n",
      "20219/85400 (epoch 47), train_loss = -8.61061281, time/batch = 1.324, learning rate = 0.00000008\n",
      "20229/85400 (epoch 47), train_loss = -8.61138922, time/batch = 1.329, learning rate = 0.00000008\n",
      "20239/85400 (epoch 47), train_loss = -8.61274596, time/batch = 1.322, learning rate = 0.00000008\n",
      "20249/85400 (epoch 47), train_loss = -8.61335904, time/batch = 1.331, learning rate = 0.00000008\n",
      "20259/85400 (epoch 47), train_loss = -8.61425369, time/batch = 1.319, learning rate = 0.00000008\n",
      "20269/85400 (epoch 47), train_loss = -8.61509218, time/batch = 1.322, learning rate = 0.00000008\n",
      "20279/85400 (epoch 47), train_loss = -8.61551674, time/batch = 1.320, learning rate = 0.00000008\n",
      "20289/85400 (epoch 47), train_loss = -8.61485448, time/batch = 1.316, learning rate = 0.00000008\n",
      "20299/85400 (epoch 47), train_loss = -8.61518493, time/batch = 1.324, learning rate = 0.00000008\n",
      "20309/85400 (epoch 47), train_loss = -8.61501251, time/batch = 1.327, learning rate = 0.00000008\n",
      "20319/85400 (epoch 47), train_loss = -8.61490230, time/batch = 1.326, learning rate = 0.00000008\n",
      "20329/85400 (epoch 47), train_loss = -8.61491519, time/batch = 1.325, learning rate = 0.00000008\n",
      "20339/85400 (epoch 47), train_loss = -8.61542400, time/batch = 1.322, learning rate = 0.00000008\n",
      "20349/85400 (epoch 47), train_loss = -8.61582912, time/batch = 1.325, learning rate = 0.00000008\n",
      "20359/85400 (epoch 47), train_loss = -8.61557251, time/batch = 1.316, learning rate = 0.00000008\n",
      "20369/85400 (epoch 47), train_loss = -8.61521784, time/batch = 1.322, learning rate = 0.00000008\n",
      "20379/85400 (epoch 47), train_loss = -8.61563462, time/batch = 1.323, learning rate = 0.00000008\n",
      "20389/85400 (epoch 47), train_loss = -8.61587739, time/batch = 1.315, learning rate = 0.00000008\n",
      "20399/85400 (epoch 47), train_loss = -8.61653830, time/batch = 1.311, learning rate = 0.00000008\n",
      "20409/85400 (epoch 47), train_loss = -8.61718247, time/batch = 1.315, learning rate = 0.00000008\n",
      "20419/85400 (epoch 47), train_loss = -8.61657150, time/batch = 1.325, learning rate = 0.00000008\n",
      "20429/85400 (epoch 47), train_loss = -8.61710997, time/batch = 1.336, learning rate = 0.00000008\n",
      "20439/85400 (epoch 47), train_loss = -8.61721086, time/batch = 1.314, learning rate = 0.00000008\n",
      "20449/85400 (epoch 47), train_loss = -8.61755123, time/batch = 1.325, learning rate = 0.00000008\n",
      "20459/85400 (epoch 47), train_loss = -8.61775136, time/batch = 1.317, learning rate = 0.00000008\n",
      "20469/85400 (epoch 47), train_loss = -8.61810971, time/batch = 1.329, learning rate = 0.00000008\n",
      "20479/85400 (epoch 47), train_loss = -8.61835664, time/batch = 1.326, learning rate = 0.00000008\n",
      "20489/85400 (epoch 47), train_loss = -8.61818523, time/batch = 1.315, learning rate = 0.00000008\n",
      "Saving model to model-48-20500.pth\n",
      "20506/85400 (epoch 48), train_loss = -8.63482113, time/batch = 1.320, learning rate = 0.00000008\n",
      "20516/85400 (epoch 48), train_loss = -8.63315372, time/batch = 1.322, learning rate = 0.00000008\n",
      "20526/85400 (epoch 48), train_loss = -8.63383554, time/batch = 1.324, learning rate = 0.00000008\n",
      "20536/85400 (epoch 48), train_loss = -8.62616048, time/batch = 1.317, learning rate = 0.00000008\n",
      "20546/85400 (epoch 48), train_loss = -8.61906435, time/batch = 1.329, learning rate = 0.00000008\n",
      "20556/85400 (epoch 48), train_loss = -8.61494131, time/batch = 1.329, learning rate = 0.00000008\n",
      "20566/85400 (epoch 48), train_loss = -8.61754062, time/batch = 1.317, learning rate = 0.00000008\n",
      "20576/85400 (epoch 48), train_loss = -8.62143366, time/batch = 1.317, learning rate = 0.00000008\n",
      "20586/85400 (epoch 48), train_loss = -8.61960796, time/batch = 1.321, learning rate = 0.00000008\n",
      "20596/85400 (epoch 48), train_loss = -8.61872245, time/batch = 1.327, learning rate = 0.00000008\n",
      "20606/85400 (epoch 48), train_loss = -8.61820525, time/batch = 1.320, learning rate = 0.00000008\n",
      "20616/85400 (epoch 48), train_loss = -8.61714787, time/batch = 1.321, learning rate = 0.00000008\n",
      "20626/85400 (epoch 48), train_loss = -8.61511113, time/batch = 1.319, learning rate = 0.00000008\n",
      "20636/85400 (epoch 48), train_loss = -8.61460890, time/batch = 1.319, learning rate = 0.00000008\n",
      "20646/85400 (epoch 48), train_loss = -8.61529100, time/batch = 1.317, learning rate = 0.00000008\n",
      "20656/85400 (epoch 48), train_loss = -8.61586544, time/batch = 1.320, learning rate = 0.00000008\n",
      "20666/85400 (epoch 48), train_loss = -8.61750054, time/batch = 1.323, learning rate = 0.00000008\n",
      "20676/85400 (epoch 48), train_loss = -8.61702116, time/batch = 1.316, learning rate = 0.00000008\n",
      "20686/85400 (epoch 48), train_loss = -8.61689746, time/batch = 1.314, learning rate = 0.00000008\n",
      "20696/85400 (epoch 48), train_loss = -8.61723784, time/batch = 1.327, learning rate = 0.00000008\n",
      "20706/85400 (epoch 48), train_loss = -8.61756847, time/batch = 1.323, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20716/85400 (epoch 48), train_loss = -8.61757268, time/batch = 1.319, learning rate = 0.00000008\n",
      "20726/85400 (epoch 48), train_loss = -8.61655813, time/batch = 1.335, learning rate = 0.00000008\n",
      "20736/85400 (epoch 48), train_loss = -8.61637895, time/batch = 1.324, learning rate = 0.00000008\n",
      "20746/85400 (epoch 48), train_loss = -8.61724707, time/batch = 1.317, learning rate = 0.00000008\n",
      "20756/85400 (epoch 48), train_loss = -8.61736565, time/batch = 1.319, learning rate = 0.00000008\n",
      "20766/85400 (epoch 48), train_loss = -8.61689314, time/batch = 1.325, learning rate = 0.00000008\n",
      "20776/85400 (epoch 48), train_loss = -8.61718768, time/batch = 1.319, learning rate = 0.00000008\n",
      "20786/85400 (epoch 48), train_loss = -8.61744201, time/batch = 1.322, learning rate = 0.00000008\n",
      "20796/85400 (epoch 48), train_loss = -8.61754229, time/batch = 1.335, learning rate = 0.00000008\n",
      "20806/85400 (epoch 48), train_loss = -8.61586093, time/batch = 1.322, learning rate = 0.00000008\n",
      "20816/85400 (epoch 48), train_loss = -8.61635568, time/batch = 1.321, learning rate = 0.00000008\n",
      "20826/85400 (epoch 48), train_loss = -8.61709644, time/batch = 1.321, learning rate = 0.00000008\n",
      "20836/85400 (epoch 48), train_loss = -8.61704487, time/batch = 1.321, learning rate = 0.00000008\n",
      "20846/85400 (epoch 48), train_loss = -8.61698545, time/batch = 1.321, learning rate = 0.00000008\n",
      "20856/85400 (epoch 48), train_loss = -8.61676818, time/batch = 1.320, learning rate = 0.00000008\n",
      "20866/85400 (epoch 48), train_loss = -8.61679153, time/batch = 1.326, learning rate = 0.00000008\n",
      "20876/85400 (epoch 48), train_loss = -8.61698006, time/batch = 1.320, learning rate = 0.00000008\n",
      "20886/85400 (epoch 48), train_loss = -8.61661567, time/batch = 1.322, learning rate = 0.00000008\n",
      "20896/85400 (epoch 48), train_loss = -8.61654321, time/batch = 1.322, learning rate = 0.00000008\n",
      "20906/85400 (epoch 48), train_loss = -8.61718546, time/batch = 1.322, learning rate = 0.00000008\n",
      "20916/85400 (epoch 48), train_loss = -8.61704222, time/batch = 1.323, learning rate = 0.00000008\n",
      "20933/85400 (epoch 49), train_loss = -8.62792540, time/batch = 1.332, learning rate = 0.00000008\n",
      "20943/85400 (epoch 49), train_loss = -8.61836162, time/batch = 1.322, learning rate = 0.00000008\n",
      "20953/85400 (epoch 49), train_loss = -8.61957843, time/batch = 1.323, learning rate = 0.00000008\n",
      "20963/85400 (epoch 49), train_loss = -8.62541981, time/batch = 1.428, learning rate = 0.00000008\n",
      "20973/85400 (epoch 49), train_loss = -8.62377901, time/batch = 1.329, learning rate = 0.00000008\n",
      "20983/85400 (epoch 49), train_loss = -8.62342359, time/batch = 1.328, learning rate = 0.00000008\n",
      "20993/85400 (epoch 49), train_loss = -8.62348765, time/batch = 1.325, learning rate = 0.00000008\n",
      "Saving model to model-49-21000.pth\n",
      "21003/85400 (epoch 49), train_loss = -8.62636501, time/batch = 1.333, learning rate = 0.00000008\n",
      "21013/85400 (epoch 49), train_loss = -8.62526060, time/batch = 1.321, learning rate = 0.00000008\n",
      "21023/85400 (epoch 49), train_loss = -8.62549436, time/batch = 1.308, learning rate = 0.00000008\n",
      "21033/85400 (epoch 49), train_loss = -8.62423858, time/batch = 1.319, learning rate = 0.00000008\n",
      "21043/85400 (epoch 49), train_loss = -8.62362840, time/batch = 1.311, learning rate = 0.00000008\n",
      "21053/85400 (epoch 49), train_loss = -8.62415339, time/batch = 1.319, learning rate = 0.00000008\n",
      "21063/85400 (epoch 49), train_loss = -8.62463015, time/batch = 1.324, learning rate = 0.00000008\n",
      "21073/85400 (epoch 49), train_loss = -8.62468581, time/batch = 1.312, learning rate = 0.00000008\n",
      "21083/85400 (epoch 49), train_loss = -8.62508034, time/batch = 1.314, learning rate = 0.00000008\n",
      "21093/85400 (epoch 49), train_loss = -8.62485994, time/batch = 1.331, learning rate = 0.00000008\n",
      "21103/85400 (epoch 49), train_loss = -8.62354257, time/batch = 1.314, learning rate = 0.00000008\n",
      "21113/85400 (epoch 49), train_loss = -8.62242668, time/batch = 1.330, learning rate = 0.00000008\n",
      "21123/85400 (epoch 49), train_loss = -8.62230690, time/batch = 1.330, learning rate = 0.00000008\n",
      "21133/85400 (epoch 49), train_loss = -8.62105364, time/batch = 1.317, learning rate = 0.00000008\n",
      "21143/85400 (epoch 49), train_loss = -8.62028369, time/batch = 1.310, learning rate = 0.00000008\n",
      "21153/85400 (epoch 49), train_loss = -8.61973811, time/batch = 1.318, learning rate = 0.00000008\n",
      "21163/85400 (epoch 49), train_loss = -8.61982875, time/batch = 1.322, learning rate = 0.00000008\n",
      "21173/85400 (epoch 49), train_loss = -8.61983806, time/batch = 1.328, learning rate = 0.00000008\n",
      "21183/85400 (epoch 49), train_loss = -8.61975838, time/batch = 1.329, learning rate = 0.00000008\n",
      "21193/85400 (epoch 49), train_loss = -8.62028003, time/batch = 1.325, learning rate = 0.00000008\n",
      "21203/85400 (epoch 49), train_loss = -8.61945859, time/batch = 1.323, learning rate = 0.00000008\n",
      "21213/85400 (epoch 49), train_loss = -8.61903374, time/batch = 1.317, learning rate = 0.00000008\n",
      "21223/85400 (epoch 49), train_loss = -8.61866452, time/batch = 1.325, learning rate = 0.00000008\n",
      "21233/85400 (epoch 49), train_loss = -8.61800085, time/batch = 1.325, learning rate = 0.00000008\n",
      "21243/85400 (epoch 49), train_loss = -8.61786982, time/batch = 1.311, learning rate = 0.00000008\n",
      "21253/85400 (epoch 49), train_loss = -8.61804435, time/batch = 1.317, learning rate = 0.00000008\n",
      "21263/85400 (epoch 49), train_loss = -8.61853166, time/batch = 1.321, learning rate = 0.00000008\n",
      "21273/85400 (epoch 49), train_loss = -8.61837454, time/batch = 1.319, learning rate = 0.00000008\n",
      "21283/85400 (epoch 49), train_loss = -8.61872547, time/batch = 1.321, learning rate = 0.00000008\n",
      "21293/85400 (epoch 49), train_loss = -8.61928935, time/batch = 1.324, learning rate = 0.00000008\n",
      "21303/85400 (epoch 49), train_loss = -8.61933389, time/batch = 1.320, learning rate = 0.00000008\n",
      "21313/85400 (epoch 49), train_loss = -8.61956343, time/batch = 1.331, learning rate = 0.00000008\n",
      "21323/85400 (epoch 49), train_loss = -8.61863054, time/batch = 1.326, learning rate = 0.00000008\n",
      "21333/85400 (epoch 49), train_loss = -8.61855037, time/batch = 1.325, learning rate = 0.00000008\n",
      "21343/85400 (epoch 49), train_loss = -8.61872994, time/batch = 1.325, learning rate = 0.00000008\n",
      "21360/85400 (epoch 50), train_loss = -8.61499214, time/batch = 1.324, learning rate = 0.00000008\n",
      "21370/85400 (epoch 50), train_loss = -8.62213612, time/batch = 1.318, learning rate = 0.00000008\n",
      "21380/85400 (epoch 50), train_loss = -8.61964534, time/batch = 1.327, learning rate = 0.00000008\n",
      "21390/85400 (epoch 50), train_loss = -8.62328999, time/batch = 1.326, learning rate = 0.00000008\n",
      "21400/85400 (epoch 50), train_loss = -8.61465101, time/batch = 1.312, learning rate = 0.00000008\n",
      "21410/85400 (epoch 50), train_loss = -8.61374729, time/batch = 1.323, learning rate = 0.00000008\n",
      "21420/85400 (epoch 50), train_loss = -8.61303007, time/batch = 1.327, learning rate = 0.00000008\n",
      "21430/85400 (epoch 50), train_loss = -8.61448789, time/batch = 1.325, learning rate = 0.00000008\n",
      "21440/85400 (epoch 50), train_loss = -8.61684617, time/batch = 1.330, learning rate = 0.00000008\n",
      "21450/85400 (epoch 50), train_loss = -8.61751595, time/batch = 1.320, learning rate = 0.00000008\n",
      "21460/85400 (epoch 50), train_loss = -8.61731899, time/batch = 1.324, learning rate = 0.00000008\n",
      "21470/85400 (epoch 50), train_loss = -8.61699641, time/batch = 1.312, learning rate = 0.00000008\n",
      "21480/85400 (epoch 50), train_loss = -8.61708725, time/batch = 1.326, learning rate = 0.00000008\n",
      "21490/85400 (epoch 50), train_loss = -8.61615342, time/batch = 1.330, learning rate = 0.00000008\n",
      "21500/85400 (epoch 50), train_loss = -8.61610399, time/batch = 1.320, learning rate = 0.00000008\n",
      "Saving model to model-50-21500.pth\n",
      "21510/85400 (epoch 50), train_loss = -8.61714870, time/batch = 1.320, learning rate = 0.00000008\n",
      "21520/85400 (epoch 50), train_loss = -8.61706386, time/batch = 1.317, learning rate = 0.00000008\n",
      "21530/85400 (epoch 50), train_loss = -8.61731897, time/batch = 1.327, learning rate = 0.00000008\n",
      "21540/85400 (epoch 50), train_loss = -8.61854418, time/batch = 1.315, learning rate = 0.00000008\n",
      "21550/85400 (epoch 50), train_loss = -8.61854016, time/batch = 1.318, learning rate = 0.00000008\n",
      "21560/85400 (epoch 50), train_loss = -8.61943398, time/batch = 1.325, learning rate = 0.00000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21570/85400 (epoch 50), train_loss = -8.61901620, time/batch = 1.310, learning rate = 0.00000008\n",
      "21580/85400 (epoch 50), train_loss = -8.61867263, time/batch = 1.321, learning rate = 0.00000008\n",
      "21590/85400 (epoch 50), train_loss = -8.61902293, time/batch = 1.318, learning rate = 0.00000008\n",
      "21600/85400 (epoch 50), train_loss = -8.61842897, time/batch = 1.318, learning rate = 0.00000008\n",
      "21610/85400 (epoch 50), train_loss = -8.61838048, time/batch = 1.318, learning rate = 0.00000008\n",
      "21620/85400 (epoch 50), train_loss = -8.61891691, time/batch = 1.317, learning rate = 0.00000008\n",
      "21630/85400 (epoch 50), train_loss = -8.61915876, time/batch = 1.307, learning rate = 0.00000008\n",
      "21640/85400 (epoch 50), train_loss = -8.61977597, time/batch = 1.315, learning rate = 0.00000008\n",
      "21650/85400 (epoch 50), train_loss = -8.61998819, time/batch = 1.324, learning rate = 0.00000008\n",
      "21660/85400 (epoch 50), train_loss = -8.61962874, time/batch = 1.330, learning rate = 0.00000008\n",
      "21670/85400 (epoch 50), train_loss = -8.61972345, time/batch = 1.315, learning rate = 0.00000008\n",
      "21680/85400 (epoch 50), train_loss = -8.62063389, time/batch = 1.323, learning rate = 0.00000008\n",
      "21690/85400 (epoch 50), train_loss = -8.62088862, time/batch = 1.328, learning rate = 0.00000008\n",
      "21700/85400 (epoch 50), train_loss = -8.62097858, time/batch = 1.328, learning rate = 0.00000008\n",
      "21710/85400 (epoch 50), train_loss = -8.62072294, time/batch = 1.319, learning rate = 0.00000008\n",
      "21720/85400 (epoch 50), train_loss = -8.62101613, time/batch = 1.321, learning rate = 0.00000008\n",
      "21730/85400 (epoch 50), train_loss = -8.62075710, time/batch = 1.317, learning rate = 0.00000008\n",
      "21740/85400 (epoch 50), train_loss = -8.62076931, time/batch = 1.323, learning rate = 0.00000008\n",
      "21750/85400 (epoch 50), train_loss = -8.62057468, time/batch = 1.323, learning rate = 0.00000008\n",
      "21760/85400 (epoch 50), train_loss = -8.61912903, time/batch = 1.324, learning rate = 0.00000008\n",
      "21770/85400 (epoch 50), train_loss = -8.61889656, time/batch = 1.311, learning rate = 0.00000008\n",
      "21787/85400 (epoch 51), train_loss = -8.61136112, time/batch = 1.322, learning rate = 0.00000008\n",
      "21797/85400 (epoch 51), train_loss = -8.62401934, time/batch = 1.321, learning rate = 0.00000008\n",
      "21807/85400 (epoch 51), train_loss = -8.62006178, time/batch = 1.319, learning rate = 0.00000008\n",
      "21817/85400 (epoch 51), train_loss = -8.62031631, time/batch = 1.325, learning rate = 0.00000008\n",
      "21827/85400 (epoch 51), train_loss = -8.61549686, time/batch = 1.324, learning rate = 0.00000008\n",
      "21837/85400 (epoch 51), train_loss = -8.61305701, time/batch = 1.313, learning rate = 0.00000008\n",
      "21847/85400 (epoch 51), train_loss = -8.61555670, time/batch = 1.318, learning rate = 0.00000008\n",
      "21857/85400 (epoch 51), train_loss = -8.61775123, time/batch = 1.321, learning rate = 0.00000008\n",
      "21867/85400 (epoch 51), train_loss = -8.61682685, time/batch = 1.323, learning rate = 0.00000008\n",
      "21877/85400 (epoch 51), train_loss = -8.61459649, time/batch = 1.342, learning rate = 0.00000008\n",
      "21887/85400 (epoch 51), train_loss = -8.61593272, time/batch = 1.314, learning rate = 0.00000008\n",
      "21897/85400 (epoch 51), train_loss = -8.61835821, time/batch = 1.319, learning rate = 0.00000008\n",
      "21907/85400 (epoch 51), train_loss = -8.61858825, time/batch = 1.312, learning rate = 0.00000008\n",
      "21917/85400 (epoch 51), train_loss = -8.61975974, time/batch = 1.313, learning rate = 0.00000008\n",
      "21927/85400 (epoch 51), train_loss = -8.61970967, time/batch = 1.318, learning rate = 0.00000008\n",
      "21937/85400 (epoch 51), train_loss = -8.61959504, time/batch = 1.324, learning rate = 0.00000008\n",
      "21947/85400 (epoch 51), train_loss = -8.62034808, time/batch = 1.324, learning rate = 0.00000008\n",
      "21957/85400 (epoch 51), train_loss = -8.62089956, time/batch = 1.314, learning rate = 0.00000008\n",
      "21967/85400 (epoch 51), train_loss = -8.62151574, time/batch = 1.332, learning rate = 0.00000008\n",
      "21977/85400 (epoch 51), train_loss = -8.62244812, time/batch = 1.322, learning rate = 0.00000008\n",
      "21987/85400 (epoch 51), train_loss = -8.62269526, time/batch = 1.310, learning rate = 0.00000008\n",
      "21997/85400 (epoch 51), train_loss = -8.62141832, time/batch = 1.326, learning rate = 0.00000008\n",
      "Saving model to model-51-22000.pth\n",
      "22007/85400 (epoch 51), train_loss = -8.62165747, time/batch = 1.309, learning rate = 0.00000008\n",
      "22017/85400 (epoch 51), train_loss = -8.62195944, time/batch = 1.313, learning rate = 0.00000008\n",
      "22027/85400 (epoch 51), train_loss = -8.62247446, time/batch = 1.315, learning rate = 0.00000008\n",
      "22037/85400 (epoch 51), train_loss = -8.62212020, time/batch = 1.319, learning rate = 0.00000008\n",
      "22047/85400 (epoch 51), train_loss = -8.62182191, time/batch = 1.327, learning rate = 0.00000008\n",
      "22057/85400 (epoch 51), train_loss = -8.62201960, time/batch = 1.321, learning rate = 0.00000008\n",
      "22067/85400 (epoch 51), train_loss = -8.62259540, time/batch = 1.331, learning rate = 0.00000008\n",
      "22077/85400 (epoch 51), train_loss = -8.62205757, time/batch = 1.320, learning rate = 0.00000008\n",
      "22087/85400 (epoch 51), train_loss = -8.62145083, time/batch = 1.318, learning rate = 0.00000008\n",
      "22097/85400 (epoch 51), train_loss = -8.62144171, time/batch = 1.319, learning rate = 0.00000008\n",
      "22107/85400 (epoch 51), train_loss = -8.62143625, time/batch = 1.315, learning rate = 0.00000008\n",
      "22117/85400 (epoch 51), train_loss = -8.62074859, time/batch = 1.321, learning rate = 0.00000008\n",
      "22127/85400 (epoch 51), train_loss = -8.62053538, time/batch = 1.319, learning rate = 0.00000008\n",
      "22137/85400 (epoch 51), train_loss = -8.62086541, time/batch = 1.330, learning rate = 0.00000008\n",
      "22147/85400 (epoch 51), train_loss = -8.62108700, time/batch = 1.320, learning rate = 0.00000008\n",
      "22157/85400 (epoch 51), train_loss = -8.62062152, time/batch = 1.322, learning rate = 0.00000008\n",
      "22167/85400 (epoch 51), train_loss = -8.62030355, time/batch = 1.316, learning rate = 0.00000008\n",
      "22177/85400 (epoch 51), train_loss = -8.62061545, time/batch = 1.323, learning rate = 0.00000008\n",
      "22187/85400 (epoch 51), train_loss = -8.61982423, time/batch = 1.322, learning rate = 0.00000008\n",
      "22197/85400 (epoch 51), train_loss = -8.62003599, time/batch = 1.317, learning rate = 0.00000008\n",
      "22214/85400 (epoch 52), train_loss = -8.62116613, time/batch = 1.319, learning rate = 0.00000008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fba4c7e9d9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msingle_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgps_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3ef643f4fca8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mnum_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mlabel_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0msub_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0msub_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.model.train()\n",
    "for e in range(args.num_epochs):\n",
    "#for e in range(1):\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(dataloader, 0):\n",
    "        x,y = data.values()\n",
    "        \n",
    "        single_loss, batch_time, gps_loss = trainer.train(x,y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_loss += single_loss\n",
    "            if args.tensorboard:\n",
    "                data2tensorboard(writer,single_loss,train_loss/(b+1),e*len(dataloader)+(b+1))\n",
    "                writer.add_scalars('gps_loss',\n",
    "                   {'gp'+str(i):gps_loss[i] for i in range(args.num_gp) if gps_loss[i] != 0},\n",
    "                   e*len(dataloader)+(b+1))\n",
    "            if ((b+1)%args.display == 0):\n",
    "                display_loss(e*len(dataloader)+(b+1),args.num_epochs*len(dataloader),e,\n",
    "                              train_loss/(b+1),batch_time,trainer.scheduler.get_last_lr()[0])          \n",
    "            if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                trainer.save_model('model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "            if trainer.scheduler.get_last_lr()[0] > args.learning_rate_clip and (e * len(dataloader) + (b+1)) % 200 == 0:\n",
    "                trainer.scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
