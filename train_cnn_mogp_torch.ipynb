{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T12:07:14.688593Z",
     "start_time": "2020-07-13T12:07:14.685697Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#from apex import amp,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:08:46.823480Z",
     "start_time": "2020-07-13T21:08:46.051469Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:08:48.878875Z",
     "start_time": "2020-07-13T21:08:48.510484Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/train_cnn_gp_torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:08:51.118763Z",
     "start_time": "2020-07-13T21:08:51.087829Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=300, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.005, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.98, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=.01, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/mogp_net_torch', help='rnn, gru, or lstm')\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=50, help='display frequency')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:08:56.621417Z",
     "start_time": "2020-07-13T21:08:52.915354Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import gpflow.multioutput.kernels as mk\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset\n",
    "import time\n",
    "\n",
    "'''\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform,\n",
    "                              get_pair = False)\n",
    "'''\n",
    "x_train = torch.from_numpy(np.genfromtxt('/notebooks/michigan_nn_data/toy/x_train.csv', delimiter=',')).float()\n",
    "y_train = torch.from_numpy(np.genfromtxt('/notebooks/michigan_nn_data/toy/y_train.csv', delimiter=',')).float()\n",
    "dataset = TensorDataset(x_train,y_train)\n",
    "\n",
    "#[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "#torch.save([args.norm_mean, args.norm_std], '/notebooks/global_localization/norm_mean_std.pt')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T12:07:18.190640Z",
     "start_time": "2020-07-13T12:07:18.188909Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#args.norm_mean, args.norm_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:08:56.635483Z",
     "start_time": "2020-07-13T21:08:56.623441Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([3])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a MultitaskVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=3\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        #self.net = Model()\n",
    "        #self.net.load_state_dict(torch.load(os.path.join(args.model_dir,'model-23-96000.pth')))\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([3]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([3])),\n",
    "            batch_shape=torch.Size([3])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "class GPModel(gpytorch.Module):\n",
    "    def __init__(self, inducing_points):\n",
    "        super(GPModel, self).__init__()\n",
    "        self.gp = MultitaskGPModel(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.gp(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:09:01.276107Z",
     "start_time": "2020-07-13T21:08:58.748920Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstate_dict = torch.load(os.path.join('/notebooks/global_localization/dual_resnet_torch','pretrained.pth'))\\nfor name,param in state_dict.items():\\n    print(name, param.shape)\\nprint('Parameters layer:',len(state_dict.keys()))\\nmodel.net.load_state_dict(state_dict)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "model = GPModel(torch.zeros(3, args.batch_size, 128)).to(device)\n",
    "# Load Resnet\n",
    "'''\n",
    "state_dict = torch.load(os.path.join('/notebooks/global_localization/dual_resnet_torch','pretrained.pth'))\n",
    "for name,param in state_dict.items():\n",
    "    print(name, param.shape)\n",
    "print('Parameters layer:',len(state_dict.keys()))\n",
    "model.net.load_state_dict(state_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:09:01.300356Z",
     "start_time": "2020-07-13T21:09:01.278998Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gp.variational_strategy.base_variational_strategy.inducing_points torch.Size([3, 300, 128])\n",
      "gp.variational_strategy.base_variational_strategy.variational_params_initialized torch.Size([])\n",
      "gp.variational_strategy.base_variational_strategy.updated_strategy torch.Size([])\n",
      "gp.variational_strategy.base_variational_strategy._variational_distribution.variational_mean torch.Size([3, 300])\n",
      "gp.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar torch.Size([3, 300, 300])\n",
      "gp.mean_module.constant torch.Size([3, 1])\n",
      "gp.covar_module.raw_outputscale torch.Size([3])\n",
      "gp.covar_module.base_kernel.raw_lengthscale torch.Size([3, 1, 1])\n",
      "likelihood.raw_noise torch.Size([1])\n",
      "likelihood.noise_covar.raw_noise torch.Size([3])\n",
      "Parameters layer: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_dict = torch.load(os.path.join(args.model_dir,'pretrained.pth'))\n",
    "for name,param in state_dict.items():\n",
    "    print(name, param.shape)\n",
    "print('Parameters layer:',len(state_dict.keys()))\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:09:02.361304Z",
     "start_time": "2020-07-13T21:09:02.354744Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Disable net\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#for param in model.net.parameters():\n",
    "#    param.requires_grad = False\n",
    "    \n",
    "# Display Learn parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T01:00:39.528150Z",
     "start_time": "2020-07-13T01:00:39.523370Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#args.norm_mean = args.norm_mean.to(device)\n",
    "#args.norm_std = args.norm_std.to(device)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.gp.parameters(), \\\n",
    "     'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "    {'params': model.likelihood.parameters(), \\\n",
    "     'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "])\n",
    "'''\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.gp.parameters(), \\\n",
    "     'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "    {'params': model.likelihood.parameters(), \\\n",
    "     'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "])\n",
    "'''\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "#mll = gpytorch.mlls.VariationalELBO(model.likelihood, model.gp, num_data=len(dataset.Targets))\n",
    "mll = gpytorch.mlls.PredictiveLogLikelihood(model.likelihood, model.gp, num_data=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T01:00:39.534902Z",
     "start_time": "2020-07-13T01:00:39.529471Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model parameters: 386109\n",
      "Likelihood parameters: 4\n"
     ]
    }
   ],
   "source": [
    "#print('CNN model parameters:', sum(param.numel() for param in model.net.global_context.parameters()))\n",
    "#print('Regressor model parameters:', sum(param.numel() for param in model.net.global_regressor.parameters()))\n",
    "print('GP model parameters:', sum(param.numel() for param in model.gp.parameters()))\n",
    "print('Likelihood parameters:', sum(param.numel() for param in model.likelihood.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T01:00:39.546488Z",
     "start_time": "2020-07-13T01:00:39.536353Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    train_loss = 0.\n",
    "    with gpytorch.settings.num_likelihood_samples(100):\n",
    "        for b, data in enumerate(dataloader, 0):\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                x,y = data\n",
    "                x,y = x.to(device),y.to(device)\n",
    "                # normalize targets\n",
    "                trans_target, _ = torch.split(y, [3, 4], dim=1)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            trans_loss = -mll(output, trans_target)\n",
    "            #rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "            total_loss = trans_loss #+ args.lamda_weights * rot_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            end = time.time()\n",
    "            with torch.no_grad():\n",
    "                train_loss += float(total_loss)\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                writer.add_scalars('training loss',\n",
    "                  {'item loss':float(total_loss),\n",
    "                  'batch loss':train_loss/(b+1)},\n",
    "                  e * len(dataloader) + (b+1))\n",
    "                if ((b+1)%args.display == 0):\n",
    "                     print(\n",
    "                        \"{}/{} (epoch {}), train_loss = {}, time/batch = {:.3f}, learning rate = {:.9f}\"\n",
    "                        .format(\n",
    "                        e * len(dataloader) + (b+1),\n",
    "                        args.num_epochs * len(dataloader),\n",
    "                        e,\n",
    "                        train_loss/(b+1),\n",
    "                        end - start,\n",
    "                        lr)) # scheduler.get_last_lr()[0]\n",
    "                if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                    checkpoint_path = os.path.join(args.model_dir, 'model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "                    torch.save(model.state_dict(),checkpoint_path)\n",
    "                    print('saving model to model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T01:00:39.555860Z",
     "start_time": "2020-07-13T01:00:39.547939Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultitaskGaussianLikelihood(\n",
       "  (noise_covar): MultitaskHomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       "  (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gp.train()\n",
    "model.likelihood.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T01:06:19.812160Z",
     "start_time": "2020-07-13T01:00:39.557161Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/10400 (epoch 0), train_loss = -6.708509473800659, time/batch = 0.029, learning rate = 0.005000000\n",
      "102/10400 (epoch 1), train_loss = -7.041602039337159, time/batch = 0.029, learning rate = 0.004900000\n",
      "154/10400 (epoch 2), train_loss = -7.036065826416015, time/batch = 0.029, learning rate = 0.004802000\n",
      "206/10400 (epoch 3), train_loss = -7.060898513793945, time/batch = 0.029, learning rate = 0.004705960\n",
      "258/10400 (epoch 4), train_loss = -7.047835235595703, time/batch = 0.029, learning rate = 0.004611841\n",
      "310/10400 (epoch 5), train_loss = -7.034045782089233, time/batch = 0.029, learning rate = 0.004519604\n",
      "362/10400 (epoch 6), train_loss = -7.049473524093628, time/batch = 0.030, learning rate = 0.004429212\n",
      "414/10400 (epoch 7), train_loss = -7.051306943893433, time/batch = 0.030, learning rate = 0.004340628\n",
      "466/10400 (epoch 8), train_loss = -7.065126686096192, time/batch = 0.029, learning rate = 0.004253815\n",
      "518/10400 (epoch 9), train_loss = -7.069210090637207, time/batch = 0.029, learning rate = 0.004168739\n",
      "570/10400 (epoch 10), train_loss = -7.033244047164917, time/batch = 0.029, learning rate = 0.004085364\n",
      "622/10400 (epoch 11), train_loss = -7.056660261154175, time/batch = 0.029, learning rate = 0.004003657\n",
      "674/10400 (epoch 12), train_loss = -7.081927928924561, time/batch = 0.029, learning rate = 0.003923584\n",
      "726/10400 (epoch 13), train_loss = -7.073632678985596, time/batch = 0.029, learning rate = 0.003845112\n",
      "778/10400 (epoch 14), train_loss = -7.063881816864014, time/batch = 0.029, learning rate = 0.003768210\n",
      "830/10400 (epoch 15), train_loss = -7.062331848144531, time/batch = 0.029, learning rate = 0.003692846\n",
      "882/10400 (epoch 16), train_loss = -7.077545938491821, time/batch = 0.029, learning rate = 0.003618989\n",
      "934/10400 (epoch 17), train_loss = -7.076038036346436, time/batch = 0.029, learning rate = 0.003546609\n",
      "986/10400 (epoch 18), train_loss = -7.07555682182312, time/batch = 0.029, learning rate = 0.003475677\n",
      "1038/10400 (epoch 19), train_loss = -7.072245645523071, time/batch = 0.029, learning rate = 0.003406163\n",
      "1090/10400 (epoch 20), train_loss = -7.052580614089965, time/batch = 0.029, learning rate = 0.003338040\n",
      "1142/10400 (epoch 21), train_loss = -7.076681337356567, time/batch = 0.029, learning rate = 0.003271279\n",
      "1194/10400 (epoch 22), train_loss = -7.094073934555054, time/batch = 0.030, learning rate = 0.003205853\n",
      "1246/10400 (epoch 23), train_loss = -7.0868108940124515, time/batch = 0.029, learning rate = 0.003141736\n",
      "1298/10400 (epoch 24), train_loss = -7.0813185977935795, time/batch = 0.029, learning rate = 0.003078902\n",
      "1350/10400 (epoch 25), train_loss = -7.0897612953186036, time/batch = 0.029, learning rate = 0.003017324\n",
      "1402/10400 (epoch 26), train_loss = -7.080597400665283, time/batch = 0.029, learning rate = 0.002956977\n",
      "1454/10400 (epoch 27), train_loss = -7.086226615905762, time/batch = 0.029, learning rate = 0.002897838\n",
      "1506/10400 (epoch 28), train_loss = -7.09746862411499, time/batch = 0.029, learning rate = 0.002839881\n",
      "1558/10400 (epoch 29), train_loss = -7.100860204696655, time/batch = 0.029, learning rate = 0.002783083\n",
      "1610/10400 (epoch 30), train_loss = -7.093300485610962, time/batch = 0.029, learning rate = 0.002727422\n",
      "1662/10400 (epoch 31), train_loss = -7.100080575942993, time/batch = 0.029, learning rate = 0.002672873\n",
      "1714/10400 (epoch 32), train_loss = -7.11019889831543, time/batch = 0.029, learning rate = 0.002619416\n",
      "1766/10400 (epoch 33), train_loss = -7.10205340385437, time/batch = 0.029, learning rate = 0.002567027\n",
      "1818/10400 (epoch 34), train_loss = -7.101824054718017, time/batch = 0.029, learning rate = 0.002515687\n",
      "1870/10400 (epoch 35), train_loss = -7.108539304733276, time/batch = 0.029, learning rate = 0.002465373\n",
      "1922/10400 (epoch 36), train_loss = -7.100868158340454, time/batch = 0.029, learning rate = 0.002416066\n",
      "1974/10400 (epoch 37), train_loss = -7.105020027160645, time/batch = 0.029, learning rate = 0.002367744\n",
      "saving model to model-38-2000.pth\n",
      "2026/10400 (epoch 38), train_loss = -7.106542148590088, time/batch = 0.029, learning rate = 0.002320389\n",
      "2078/10400 (epoch 39), train_loss = -7.100893259048462, time/batch = 0.029, learning rate = 0.002273982\n",
      "2130/10400 (epoch 40), train_loss = -7.110977029800415, time/batch = 0.029, learning rate = 0.002228502\n",
      "2182/10400 (epoch 41), train_loss = -7.117321949005127, time/batch = 0.029, learning rate = 0.002183932\n",
      "2234/10400 (epoch 42), train_loss = -7.114745359420777, time/batch = 0.029, learning rate = 0.002140253\n",
      "2286/10400 (epoch 43), train_loss = -7.102327899932861, time/batch = 0.030, learning rate = 0.002097448\n",
      "2338/10400 (epoch 44), train_loss = -7.114499607086182, time/batch = 0.030, learning rate = 0.002055499\n",
      "2390/10400 (epoch 45), train_loss = -7.111214427947998, time/batch = 0.030, learning rate = 0.002014389\n",
      "2442/10400 (epoch 46), train_loss = -7.117791595458985, time/batch = 0.030, learning rate = 0.001974102\n",
      "2494/10400 (epoch 47), train_loss = -7.115776453018189, time/batch = 0.029, learning rate = 0.001934620\n",
      "2546/10400 (epoch 48), train_loss = -7.127410564422608, time/batch = 0.029, learning rate = 0.001895927\n",
      "2598/10400 (epoch 49), train_loss = -7.114200115203857, time/batch = 0.029, learning rate = 0.001858009\n",
      "2650/10400 (epoch 50), train_loss = -7.123135461807251, time/batch = 0.029, learning rate = 0.001820848\n",
      "2702/10400 (epoch 51), train_loss = -7.132621145248413, time/batch = 0.029, learning rate = 0.001784431\n",
      "2754/10400 (epoch 52), train_loss = -7.123349609375, time/batch = 0.029, learning rate = 0.001748743\n",
      "2806/10400 (epoch 53), train_loss = -7.124782772064209, time/batch = 0.029, learning rate = 0.001713768\n",
      "2858/10400 (epoch 54), train_loss = -7.129375638961792, time/batch = 0.029, learning rate = 0.001679493\n",
      "2910/10400 (epoch 55), train_loss = -7.136005802154541, time/batch = 0.029, learning rate = 0.001645903\n",
      "2962/10400 (epoch 56), train_loss = -7.139903297424317, time/batch = 0.029, learning rate = 0.001612985\n",
      "3014/10400 (epoch 57), train_loss = -7.128629941940307, time/batch = 0.029, learning rate = 0.001580725\n",
      "3066/10400 (epoch 58), train_loss = -7.135012397766113, time/batch = 0.029, learning rate = 0.001549110\n",
      "3118/10400 (epoch 59), train_loss = -7.131054449081421, time/batch = 0.029, learning rate = 0.001518128\n",
      "3170/10400 (epoch 60), train_loss = -7.122007837295532, time/batch = 0.030, learning rate = 0.001487766\n",
      "3222/10400 (epoch 61), train_loss = -7.128855590820312, time/batch = 0.030, learning rate = 0.001458010\n",
      "3274/10400 (epoch 62), train_loss = -7.140843420028687, time/batch = 0.029, learning rate = 0.001428850\n",
      "3326/10400 (epoch 63), train_loss = -7.139295806884766, time/batch = 0.029, learning rate = 0.001400273\n",
      "3378/10400 (epoch 64), train_loss = -7.139327764511108, time/batch = 0.029, learning rate = 0.001372268\n",
      "3430/10400 (epoch 65), train_loss = -7.143184061050415, time/batch = 0.029, learning rate = 0.001344822\n",
      "3482/10400 (epoch 66), train_loss = -7.139469022750855, time/batch = 0.029, learning rate = 0.001317926\n",
      "3534/10400 (epoch 67), train_loss = -7.140232772827148, time/batch = 0.029, learning rate = 0.001291567\n",
      "3586/10400 (epoch 68), train_loss = -7.137217617034912, time/batch = 0.029, learning rate = 0.001265736\n",
      "3638/10400 (epoch 69), train_loss = -7.143825407028198, time/batch = 0.029, learning rate = 0.001240421\n",
      "3690/10400 (epoch 70), train_loss = -7.145404214859009, time/batch = 0.029, learning rate = 0.001215613\n",
      "3742/10400 (epoch 71), train_loss = -7.1467150115966795, time/batch = 0.029, learning rate = 0.001191301\n",
      "3794/10400 (epoch 72), train_loss = -7.149223432540894, time/batch = 0.029, learning rate = 0.001167475\n",
      "3846/10400 (epoch 73), train_loss = -7.13887020111084, time/batch = 0.029, learning rate = 0.001144125\n",
      "3898/10400 (epoch 74), train_loss = -7.147094230651856, time/batch = 0.029, learning rate = 0.001121243\n",
      "3950/10400 (epoch 75), train_loss = -7.149308977127075, time/batch = 0.029, learning rate = 0.001098818\n",
      "saving model to model-76-4000.pth\n",
      "4002/10400 (epoch 76), train_loss = -7.147776517868042, time/batch = 0.029, learning rate = 0.001076841\n",
      "4054/10400 (epoch 77), train_loss = -7.149995498657226, time/batch = 0.029, learning rate = 0.001055305\n",
      "4106/10400 (epoch 78), train_loss = -7.142713222503662, time/batch = 0.029, learning rate = 0.001034199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158/10400 (epoch 79), train_loss = -7.14430950164795, time/batch = 0.029, learning rate = 0.001013515\n",
      "4210/10400 (epoch 80), train_loss = -7.15402325630188, time/batch = 0.029, learning rate = 0.000993244\n",
      "4262/10400 (epoch 81), train_loss = -7.153731698989868, time/batch = 0.029, learning rate = 0.000973379\n",
      "4314/10400 (epoch 82), train_loss = -7.1505357837677, time/batch = 0.029, learning rate = 0.000953912\n",
      "4366/10400 (epoch 83), train_loss = -7.144216890335083, time/batch = 0.029, learning rate = 0.000934834\n",
      "4418/10400 (epoch 84), train_loss = -7.152528467178345, time/batch = 0.029, learning rate = 0.000916137\n",
      "4470/10400 (epoch 85), train_loss = -7.156867475509643, time/batch = 0.029, learning rate = 0.000897814\n",
      "4522/10400 (epoch 86), train_loss = -7.1508048629760745, time/batch = 0.029, learning rate = 0.000879858\n",
      "4574/10400 (epoch 87), train_loss = -7.156377058029175, time/batch = 0.029, learning rate = 0.000862261\n",
      "4626/10400 (epoch 88), train_loss = -7.155984210968017, time/batch = 0.029, learning rate = 0.000845015\n",
      "4678/10400 (epoch 89), train_loss = -7.161839685440063, time/batch = 0.029, learning rate = 0.000828115\n",
      "4730/10400 (epoch 90), train_loss = -7.157907829284668, time/batch = 0.029, learning rate = 0.000811553\n",
      "4782/10400 (epoch 91), train_loss = -7.15141583442688, time/batch = 0.029, learning rate = 0.000795322\n",
      "4834/10400 (epoch 92), train_loss = -7.150876264572144, time/batch = 0.029, learning rate = 0.000779415\n",
      "4886/10400 (epoch 93), train_loss = -7.157806425094605, time/batch = 0.029, learning rate = 0.000763827\n",
      "4938/10400 (epoch 94), train_loss = -7.170156335830688, time/batch = 0.029, learning rate = 0.000748551\n",
      "4990/10400 (epoch 95), train_loss = -7.171542024612426, time/batch = 0.029, learning rate = 0.000733580\n",
      "5042/10400 (epoch 96), train_loss = -7.15747329711914, time/batch = 0.029, learning rate = 0.000718908\n",
      "5094/10400 (epoch 97), train_loss = -7.156427593231201, time/batch = 0.029, learning rate = 0.000704530\n",
      "5146/10400 (epoch 98), train_loss = -7.158444175720215, time/batch = 0.029, learning rate = 0.000690439\n",
      "5198/10400 (epoch 99), train_loss = -7.161369142532348, time/batch = 0.029, learning rate = 0.000676630\n",
      "5250/10400 (epoch 100), train_loss = -7.1654606056213375, time/batch = 0.029, learning rate = 0.000663098\n",
      "5302/10400 (epoch 101), train_loss = -7.162608909606933, time/batch = 0.029, learning rate = 0.000649836\n",
      "5354/10400 (epoch 102), train_loss = -7.1636213779449465, time/batch = 0.029, learning rate = 0.000636839\n",
      "5406/10400 (epoch 103), train_loss = -7.163756217956543, time/batch = 0.029, learning rate = 0.000624102\n",
      "5458/10400 (epoch 104), train_loss = -7.164745273590088, time/batch = 0.029, learning rate = 0.000611620\n",
      "5510/10400 (epoch 105), train_loss = -7.15877815246582, time/batch = 0.029, learning rate = 0.000599388\n",
      "5562/10400 (epoch 106), train_loss = -7.160611047744751, time/batch = 0.029, learning rate = 0.000587400\n",
      "5614/10400 (epoch 107), train_loss = -7.166846790313721, time/batch = 0.029, learning rate = 0.000575652\n",
      "5666/10400 (epoch 108), train_loss = -7.1603306293487545, time/batch = 0.029, learning rate = 0.000564139\n",
      "5718/10400 (epoch 109), train_loss = -7.165804634094238, time/batch = 0.029, learning rate = 0.000552856\n",
      "5770/10400 (epoch 110), train_loss = -7.169707164764405, time/batch = 0.029, learning rate = 0.000541799\n",
      "5822/10400 (epoch 111), train_loss = -7.165557994842529, time/batch = 0.029, learning rate = 0.000530963\n",
      "5874/10400 (epoch 112), train_loss = -7.170227289199829, time/batch = 0.029, learning rate = 0.000520344\n",
      "5926/10400 (epoch 113), train_loss = -7.1683314990997316, time/batch = 0.029, learning rate = 0.000509937\n",
      "5978/10400 (epoch 114), train_loss = -7.171431550979614, time/batch = 0.029, learning rate = 0.000499738\n",
      "saving model to model-115-6000.pth\n",
      "6030/10400 (epoch 115), train_loss = -7.164692335128784, time/batch = 0.029, learning rate = 0.000489744\n",
      "6082/10400 (epoch 116), train_loss = -7.166588706970215, time/batch = 0.029, learning rate = 0.000479949\n",
      "6134/10400 (epoch 117), train_loss = -7.1667665767669675, time/batch = 0.029, learning rate = 0.000470350\n",
      "6186/10400 (epoch 118), train_loss = -7.167033271789551, time/batch = 0.029, learning rate = 0.000460943\n",
      "6238/10400 (epoch 119), train_loss = -7.168123254776001, time/batch = 0.029, learning rate = 0.000451724\n",
      "6290/10400 (epoch 120), train_loss = -7.165928745269776, time/batch = 0.029, learning rate = 0.000442689\n",
      "6342/10400 (epoch 121), train_loss = -7.169359407424927, time/batch = 0.029, learning rate = 0.000433836\n",
      "6394/10400 (epoch 122), train_loss = -7.1675543689727785, time/batch = 0.029, learning rate = 0.000425159\n",
      "6446/10400 (epoch 123), train_loss = -7.174916095733643, time/batch = 0.029, learning rate = 0.000416656\n",
      "6498/10400 (epoch 124), train_loss = -7.169532670974731, time/batch = 0.029, learning rate = 0.000408323\n",
      "6550/10400 (epoch 125), train_loss = -7.177638158798218, time/batch = 0.030, learning rate = 0.000400156\n",
      "6602/10400 (epoch 126), train_loss = -7.172115383148193, time/batch = 0.029, learning rate = 0.000392153\n",
      "6654/10400 (epoch 127), train_loss = -7.173761186599731, time/batch = 0.029, learning rate = 0.000384310\n",
      "6706/10400 (epoch 128), train_loss = -7.178846197128296, time/batch = 0.029, learning rate = 0.000376624\n",
      "6758/10400 (epoch 129), train_loss = -7.173172903060913, time/batch = 0.029, learning rate = 0.000369091\n",
      "6810/10400 (epoch 130), train_loss = -7.17061411857605, time/batch = 0.029, learning rate = 0.000361709\n",
      "6862/10400 (epoch 131), train_loss = -7.175649557113648, time/batch = 0.029, learning rate = 0.000354475\n",
      "6914/10400 (epoch 132), train_loss = -7.175309467315674, time/batch = 0.029, learning rate = 0.000347386\n",
      "6966/10400 (epoch 133), train_loss = -7.1765649700164795, time/batch = 0.029, learning rate = 0.000340438\n",
      "7018/10400 (epoch 134), train_loss = -7.175427055358886, time/batch = 0.029, learning rate = 0.000333629\n",
      "7070/10400 (epoch 135), train_loss = -7.178118743896484, time/batch = 0.029, learning rate = 0.000326957\n",
      "7122/10400 (epoch 136), train_loss = -7.176658897399903, time/batch = 0.029, learning rate = 0.000320418\n",
      "7174/10400 (epoch 137), train_loss = -7.175377435684204, time/batch = 0.029, learning rate = 0.000314009\n",
      "7226/10400 (epoch 138), train_loss = -7.17812349319458, time/batch = 0.029, learning rate = 0.000307729\n",
      "7278/10400 (epoch 139), train_loss = -7.180178966522217, time/batch = 0.029, learning rate = 0.000301574\n",
      "7330/10400 (epoch 140), train_loss = -7.178844766616821, time/batch = 0.029, learning rate = 0.000295543\n",
      "7382/10400 (epoch 141), train_loss = -7.179604063034057, time/batch = 0.029, learning rate = 0.000289632\n",
      "7434/10400 (epoch 142), train_loss = -7.175550804138184, time/batch = 0.029, learning rate = 0.000283839\n",
      "7486/10400 (epoch 143), train_loss = -7.175536966323852, time/batch = 0.029, learning rate = 0.000278163\n",
      "7538/10400 (epoch 144), train_loss = -7.174467105865478, time/batch = 0.029, learning rate = 0.000272599\n",
      "7590/10400 (epoch 145), train_loss = -7.17480809211731, time/batch = 0.029, learning rate = 0.000267147\n",
      "7642/10400 (epoch 146), train_loss = -7.181522836685181, time/batch = 0.029, learning rate = 0.000261804\n",
      "7694/10400 (epoch 147), train_loss = -7.187838230133057, time/batch = 0.029, learning rate = 0.000256568\n",
      "7746/10400 (epoch 148), train_loss = -7.178398923873901, time/batch = 0.029, learning rate = 0.000251437\n",
      "7798/10400 (epoch 149), train_loss = -7.17874771118164, time/batch = 0.029, learning rate = 0.000246408\n",
      "7850/10400 (epoch 150), train_loss = -7.171119213104248, time/batch = 0.029, learning rate = 0.000241480\n",
      "7902/10400 (epoch 151), train_loss = -7.177341747283935, time/batch = 0.029, learning rate = 0.000236651\n",
      "7954/10400 (epoch 152), train_loss = -7.172991695404053, time/batch = 0.029, learning rate = 0.000231917\n",
      "saving model to model-153-8000.pth\n",
      "8006/10400 (epoch 153), train_loss = -7.178497591018677, time/batch = 0.029, learning rate = 0.000227279\n",
      "8058/10400 (epoch 154), train_loss = -7.178815336227417, time/batch = 0.029, learning rate = 0.000222734\n",
      "8110/10400 (epoch 155), train_loss = -7.175880212783813, time/batch = 0.029, learning rate = 0.000218279\n",
      "8162/10400 (epoch 156), train_loss = -7.1789466381073, time/batch = 0.029, learning rate = 0.000213913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8214/10400 (epoch 157), train_loss = -7.179686641693115, time/batch = 0.030, learning rate = 0.000209635\n",
      "8266/10400 (epoch 158), train_loss = -7.178277568817139, time/batch = 0.030, learning rate = 0.000205442\n",
      "8318/10400 (epoch 159), train_loss = -7.179118385314942, time/batch = 0.029, learning rate = 0.000201333\n",
      "8370/10400 (epoch 160), train_loss = -7.176823396682739, time/batch = 0.029, learning rate = 0.000197307\n",
      "8422/10400 (epoch 161), train_loss = -7.179263410568237, time/batch = 0.029, learning rate = 0.000193361\n",
      "8474/10400 (epoch 162), train_loss = -7.179762773513794, time/batch = 0.029, learning rate = 0.000189493\n",
      "8526/10400 (epoch 163), train_loss = -7.182591714859009, time/batch = 0.030, learning rate = 0.000185704\n",
      "8578/10400 (epoch 164), train_loss = -7.182411890029908, time/batch = 0.029, learning rate = 0.000181990\n",
      "8630/10400 (epoch 165), train_loss = -7.177892236709595, time/batch = 0.029, learning rate = 0.000178350\n",
      "8682/10400 (epoch 166), train_loss = -7.1836565971374515, time/batch = 0.029, learning rate = 0.000174783\n",
      "8734/10400 (epoch 167), train_loss = -7.184665880203247, time/batch = 0.029, learning rate = 0.000171287\n",
      "8786/10400 (epoch 168), train_loss = -7.183992004394531, time/batch = 0.029, learning rate = 0.000167861\n",
      "8838/10400 (epoch 169), train_loss = -7.184107284545899, time/batch = 0.029, learning rate = 0.000164504\n",
      "8890/10400 (epoch 170), train_loss = -7.175932970046997, time/batch = 0.029, learning rate = 0.000161214\n",
      "8942/10400 (epoch 171), train_loss = -7.190842361450195, time/batch = 0.029, learning rate = 0.000157990\n",
      "8994/10400 (epoch 172), train_loss = -7.181290607452393, time/batch = 0.029, learning rate = 0.000154830\n",
      "9046/10400 (epoch 173), train_loss = -7.185643329620361, time/batch = 0.029, learning rate = 0.000151733\n",
      "9098/10400 (epoch 174), train_loss = -7.183902740478516, time/batch = 0.029, learning rate = 0.000148699\n",
      "9150/10400 (epoch 175), train_loss = -7.183792181015015, time/batch = 0.029, learning rate = 0.000145725\n",
      "9202/10400 (epoch 176), train_loss = -7.183313770294189, time/batch = 0.029, learning rate = 0.000142810\n",
      "9254/10400 (epoch 177), train_loss = -7.189833517074585, time/batch = 0.029, learning rate = 0.000139954\n",
      "9306/10400 (epoch 178), train_loss = -7.184174633026123, time/batch = 0.029, learning rate = 0.000137155\n",
      "9358/10400 (epoch 179), train_loss = -7.181689872741699, time/batch = 0.029, learning rate = 0.000134412\n",
      "9410/10400 (epoch 180), train_loss = -7.1787076091766355, time/batch = 0.029, learning rate = 0.000131724\n",
      "9462/10400 (epoch 181), train_loss = -7.184551925659179, time/batch = 0.029, learning rate = 0.000129089\n",
      "9514/10400 (epoch 182), train_loss = -7.181113767623901, time/batch = 0.029, learning rate = 0.000126507\n",
      "9566/10400 (epoch 183), train_loss = -7.184271965026856, time/batch = 0.029, learning rate = 0.000123977\n",
      "9618/10400 (epoch 184), train_loss = -7.188343849182129, time/batch = 0.029, learning rate = 0.000121498\n",
      "9670/10400 (epoch 185), train_loss = -7.187983455657959, time/batch = 0.029, learning rate = 0.000119068\n",
      "9722/10400 (epoch 186), train_loss = -7.188504276275634, time/batch = 0.029, learning rate = 0.000116686\n",
      "9774/10400 (epoch 187), train_loss = -7.183337888717651, time/batch = 0.029, learning rate = 0.000114353\n",
      "9826/10400 (epoch 188), train_loss = -7.180452871322632, time/batch = 0.029, learning rate = 0.000112066\n",
      "9878/10400 (epoch 189), train_loss = -7.186263999938965, time/batch = 0.029, learning rate = 0.000109824\n",
      "9930/10400 (epoch 190), train_loss = -7.179749031066894, time/batch = 0.029, learning rate = 0.000107628\n",
      "9982/10400 (epoch 191), train_loss = -7.18194278717041, time/batch = 0.029, learning rate = 0.000105475\n",
      "saving model to model-192-10000.pth\n",
      "10034/10400 (epoch 192), train_loss = -7.179659509658814, time/batch = 0.029, learning rate = 0.000103366\n",
      "10086/10400 (epoch 193), train_loss = -7.184840650558471, time/batch = 0.029, learning rate = 0.000101298\n",
      "10138/10400 (epoch 194), train_loss = -7.186326656341553, time/batch = 0.029, learning rate = 0.000099272\n",
      "10190/10400 (epoch 195), train_loss = -7.185855197906494, time/batch = 0.029, learning rate = 0.000097287\n",
      "10242/10400 (epoch 196), train_loss = -7.188557786941528, time/batch = 0.029, learning rate = 0.000095341\n",
      "10294/10400 (epoch 197), train_loss = -7.1872756385803225, time/batch = 0.029, learning rate = 0.000093434\n",
      "10346/10400 (epoch 198), train_loss = -7.1872182464599605, time/batch = 0.029, learning rate = 0.000091566\n",
      "10398/10400 (epoch 199), train_loss = -7.186997728347778, time/batch = 0.029, learning rate = 0.000089734\n"
     ]
    }
   ],
   "source": [
    "for e in range(args.num_epochs):\n",
    "#for e in range(10):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:15:01.328874Z",
     "start_time": "2020-07-13T21:15:00.125338Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4118478, 3.1710045, 2.858386 , ..., 4.6196055, 1.0426774,\n",
       "       2.1239047], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denormalize_navie(normed_target, norm_mean, norm_std):\n",
    "    target_trans_unscaled = normed_target * norm_std\n",
    "    target_trans_uncentered = target_trans_unscaled + norm_mean\n",
    "    \n",
    "    return target_trans_uncentered\n",
    "\n",
    "args.norm_mean = torch.from_numpy(\n",
    "    np.genfromtxt('/notebooks/michigan_nn_data/toy/norm_mean.csv', delimiter=',')\n",
    "    ).float()\n",
    "args.norm_std = torch.from_numpy(\n",
    "    np.genfromtxt('/notebooks/michigan_nn_data/toy/norm_std.csv', delimiter=',')\n",
    "    ).float()\n",
    "args.norm_mean = args.norm_mean.to(device)\n",
    "args.norm_std = args.norm_std.to(device)\n",
    "\n",
    "model.eval()\n",
    "loss = 0\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        x,y = data\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        # normalize targets\n",
    "        trans_target, _ = torch.split(y, [3, 4], dim=1)\n",
    "        output = model(x)\n",
    "        \n",
    "        trans_pred = denormalize_navie(output.mean,args.norm_mean,args.norm_std).cpu().numpy()\n",
    "        trans_gt = denormalize_navie(y[:,:3],args.norm_mean,args.norm_std).cpu().numpy()\n",
    "        \n",
    "        #batch_loss = np.sum(np.abs((trans_pred - trans_gt)),axis = 0)\n",
    "        batch_loss = np.sqrt(np.sum((trans_pred - trans_gt)**2,axis=1))\n",
    "        if b == 0:\n",
    "            loss = batch_loss\n",
    "        else:\n",
    "            loss = np.hstack((loss,batch_loss))\n",
    "#loss = loss/len(dataset)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T23:59:48.558253Z",
     "start_time": "2020-07-13T23:59:48.554986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3217773"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T21:36:50.455768Z",
     "start_time": "2020-07-13T21:36:50.445953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4699101"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T00:05:51.114737Z",
     "start_time": "2020-07-14T00:05:50.133848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.728456 ,  4.7990246,  1.2676868, ...,  7.4782877,  3.5493639,\n",
       "        7.1898255], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.from_numpy(np.genfromtxt('/notebooks/michigan_nn_data/toy/x_test.csv', delimiter=',')).float()\n",
    "y_test = torch.from_numpy(np.genfromtxt('/notebooks/michigan_nn_data/toy/y_test.csv', delimiter=',')).float()\n",
    "dataset_test = TensorDataset(x_test,y_test)\n",
    "\n",
    "#[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "#torch.save([args.norm_mean, args.norm_std], '/notebooks/global_localization/norm_mean_std.pt')\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "loss = 0\n",
    "for b, data in enumerate(dataloader_test, 0):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        x,y = data\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        # normalize targets\n",
    "        trans_target, _ = torch.split(y, [3, 4], dim=1)\n",
    "        output = model(x)\n",
    "        \n",
    "        trans_pred = denormalize_navie(output.mean,args.norm_mean,args.norm_std).cpu().numpy()\n",
    "        trans_gt = denormalize_navie(y[:,:3],args.norm_mean,args.norm_std).cpu().numpy()\n",
    "        \n",
    "        #batch_loss = np.sum(np.abs((trans_pred - trans_gt)),axis = 0)\n",
    "        batch_loss = np.sqrt(np.sum((trans_pred - trans_gt)**2,axis=1))\n",
    "        if b == 0:\n",
    "            loss = batch_loss\n",
    "        else:\n",
    "            loss = np.hstack((loss,batch_loss))\n",
    "#loss = loss/len(dataset)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T00:05:59.666761Z",
     "start_time": "2020-07-14T00:05:59.660485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.408299"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T00:06:10.417566Z",
     "start_time": "2020-07-14T00:06:10.411378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5619557"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
