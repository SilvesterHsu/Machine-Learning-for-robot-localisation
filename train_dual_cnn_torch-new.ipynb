{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:31.262853Z",
     "start_time": "2020-06-28T20:06:30.886416Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:31.431924Z",
     "start_time": "2020-06-28T20:06:31.264413Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/train_dual_cnn_torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:31.446719Z",
     "start_time": "2020-06-28T20:06:31.433939Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.9, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.1, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:38.612879Z",
     "start_time": "2020-06-28T20:06:31.926107Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5593/5593 [00:06<00:00, 838.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet, cnn_auxiliary\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform)\n",
    "[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:41.728947Z",
     "start_time": "2020-06-28T20:06:38.614350Z"
    },
    "code_folding": [
     0,
     1,
     18,
     26,
     31,
     36,
     46,
     56,
     66,
     76,
     81,
     107
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class CNN_Model:\n",
    "    def __init__(self, training = True, device = \"cpu\"):\n",
    "        # device\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # data\n",
    "        self.model = cnn_auxiliary.Model(training).to(device)\n",
    "        self.norm_mean = args.norm_mean.to(device)\n",
    "        self.norm_std = args.norm_std.to(device)\n",
    "        \n",
    "        # training tool\n",
    "        if training:\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), \n",
    "                                        lr=args.learning_rate, \n",
    "                                        weight_decay=args.weight_decay)\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                         lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth', display_info = True):\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, file_name))\n",
    "        if display_info:\n",
    "            for name,param in state_dict.items():\n",
    "                print(name, param.shape)\n",
    "            print('Parameters layer:',len(state_dict.keys()))\n",
    "        self.model.load_state_dict(state_dict,strict = False)\n",
    "        \n",
    "    def display_structure(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Parameters layer:',len(self.model.state_dict().keys()))\n",
    "    \n",
    "    def display_require_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.shape)\n",
    "    \n",
    "    def power_resnet(self, status = False):\n",
    "        if status == 'off':\n",
    "            for param in self.model.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif status == 'on':\n",
    "            for param in self.model.resnet.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            raise Exception(\"status must be 'on' or 'off'.\")\n",
    "            \n",
    "    def power_context(self, status = False):\n",
    "        if status == 'off':\n",
    "            for param in self.model.global_context.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif status == 'on':\n",
    "            for param in self.model.global_context.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            raise Exception(\"status must be 'on' or 'off'.\")\n",
    "    \n",
    "    def power_regressor(self, status = False):\n",
    "        if status == 'off':\n",
    "            for param in self.model.global_regressor.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif status == 'on':\n",
    "            for param in self.model.global_regressor.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            raise Exception(\"status must be 'on' or 'off'.\")\n",
    "            \n",
    "    def power_all(self, status = False):\n",
    "        if status == 'off':\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif status == 'on':\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            raise Exception(\"status must be 'on' or 'off'.\")\n",
    "            \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('saving model to' +  file_name)\n",
    "            \n",
    "    def loss(self,x0, x1, y0, y1):\n",
    "        start = time.time()\n",
    "        \n",
    "        x0,x1,y0,y1 = x0.to(self.device),x1.to(self.device),y0.to(self.device),y1.to(self.device)\n",
    "        y0_norm, y1_norm = [normalize(y,self.norm_mean, self.norm_std) for y in [y0,y1]]\n",
    "        \n",
    "        relative_target_normed = get_relative_pose(y0_norm, y1_norm)\n",
    "        \n",
    "        #self.optimizer.zero_grad()\n",
    "        \n",
    "        global_output0,global_output1 = self.model(x0, x1)\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, \\\n",
    "                                                    gt=y1_norm, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + geometry_consistent_loss        \n",
    "        #total_loss.backward()\n",
    "        #self.optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        batch_time = end - start\n",
    "        return batch_time, total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y):\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        \n",
    "        global_output = self.model(x)\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        global_output_demormed = denormalize(global_output, self.norm_mean, self.norm_std)\n",
    "        trans_prediction, rot_prediction = torch.split(global_output_demormed, [3, 4], dim=1)\n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target\n",
    "\n",
    "cnn_model = CNN_Model(training=True,device=\"cuda:1\")\n",
    "cnn_model.load_model('pretrained.pth',display_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:41.738127Z",
     "start_time": "2020-06-28T20:06:41.730349Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith torch.no_grad():\\n    graphs = Model()\\n    x0,x1 = next(iter(dataloader))['image']\\n    writer.add_graph(graphs, (x0,x1))\\ndel x0,x1,graphs\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    graphs = Model()\n",
    "    x0,x1 = next(iter(dataloader))['image']\n",
    "    writer.add_graph(graphs, (x0,x1))\n",
    "del x0,x1,graphs\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:41.781216Z",
     "start_time": "2020-06-28T20:06:41.739956Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n",
      "Parameters layer: 346\n"
     ]
    }
   ],
   "source": [
    "cnn_model.display_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:06:41.807582Z",
     "start_time": "2020-06-28T20:06:41.782740Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "cnn_model.display_require_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T20:10:20.265180Z",
     "start_time": "2020-06-28T20:06:41.813768Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/34800 (epoch 0), train_loss = 0.07620230, time/batch = 0.186, learning rate = 0.00001000\n",
      "20/34800 (epoch 0), train_loss = 0.05192423, time/batch = 0.186, learning rate = 0.00001000\n",
      "30/34800 (epoch 0), train_loss = 0.04194622, time/batch = 0.191, learning rate = 0.00001000\n",
      "40/34800 (epoch 0), train_loss = 0.03538321, time/batch = 0.187, learning rate = 0.00001000\n",
      "50/34800 (epoch 0), train_loss = 0.03033039, time/batch = 0.187, learning rate = 0.00001000\n",
      "60/34800 (epoch 0), train_loss = 0.02704226, time/batch = 0.195, learning rate = 0.00001000\n",
      "70/34800 (epoch 0), train_loss = 0.02436105, time/batch = 0.188, learning rate = 0.00001000\n",
      "80/34800 (epoch 0), train_loss = 0.02230586, time/batch = 0.188, learning rate = 0.00001000\n",
      "90/34800 (epoch 0), train_loss = 0.02060954, time/batch = 0.188, learning rate = 0.00001000\n",
      "100/34800 (epoch 0), train_loss = 0.01918047, time/batch = 0.189, learning rate = 0.00001000\n",
      "110/34800 (epoch 0), train_loss = 0.01797053, time/batch = 0.189, learning rate = 0.00001000\n",
      "120/34800 (epoch 0), train_loss = 0.01709077, time/batch = 0.193, learning rate = 0.00001000\n",
      "130/34800 (epoch 0), train_loss = 0.01623001, time/batch = 0.188, learning rate = 0.00001000\n",
      "140/34800 (epoch 0), train_loss = 0.01551562, time/batch = 0.188, learning rate = 0.00001000\n",
      "150/34800 (epoch 0), train_loss = 0.01481608, time/batch = 0.193, learning rate = 0.00001000\n",
      "160/34800 (epoch 0), train_loss = 0.01417691, time/batch = 0.189, learning rate = 0.00001000\n",
      "170/34800 (epoch 0), train_loss = 0.01363325, time/batch = 0.189, learning rate = 0.00001000\n",
      "184/34800 (epoch 1), train_loss = 0.00595064, time/batch = 0.190, learning rate = 0.00000900\n",
      "194/34800 (epoch 1), train_loss = 0.00545378, time/batch = 0.189, learning rate = 0.00000900\n",
      "saving model tomodel-1-200.pth\n",
      "204/34800 (epoch 1), train_loss = 0.00511506, time/batch = 0.190, learning rate = 0.00000900\n",
      "214/34800 (epoch 1), train_loss = 0.00485649, time/batch = 0.188, learning rate = 0.00000900\n",
      "224/34800 (epoch 1), train_loss = 0.00475864, time/batch = 0.190, learning rate = 0.00000900\n",
      "234/34800 (epoch 1), train_loss = 0.00474367, time/batch = 0.189, learning rate = 0.00000900\n",
      "244/34800 (epoch 1), train_loss = 0.00477383, time/batch = 0.188, learning rate = 0.00000900\n",
      "254/34800 (epoch 1), train_loss = 0.00475928, time/batch = 0.189, learning rate = 0.00000900\n",
      "264/34800 (epoch 1), train_loss = 0.00467310, time/batch = 0.190, learning rate = 0.00000900\n",
      "274/34800 (epoch 1), train_loss = 0.00465329, time/batch = 0.190, learning rate = 0.00000900\n",
      "284/34800 (epoch 1), train_loss = 0.00461800, time/batch = 0.189, learning rate = 0.00000900\n",
      "294/34800 (epoch 1), train_loss = 0.00457081, time/batch = 0.189, learning rate = 0.00000900\n",
      "304/34800 (epoch 1), train_loss = 0.00454549, time/batch = 0.190, learning rate = 0.00000900\n",
      "314/34800 (epoch 1), train_loss = 0.00449688, time/batch = 0.189, learning rate = 0.00000900\n",
      "324/34800 (epoch 1), train_loss = 0.00448779, time/batch = 0.189, learning rate = 0.00000900\n",
      "334/34800 (epoch 1), train_loss = 0.00442692, time/batch = 0.192, learning rate = 0.00000900\n",
      "344/34800 (epoch 1), train_loss = 0.00443265, time/batch = 0.190, learning rate = 0.00000900\n"
     ]
    }
   ],
   "source": [
    "cnn_model.model.train()\n",
    "#for e in range(args.num_epochs):\n",
    "for e in range(2):\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(dataloader, 0):\n",
    "        x0, x1 = data['image']\n",
    "        y0, y1 = data['target']\n",
    "        \n",
    "        cnn_model.optimizer.zero_grad()\n",
    "        batch_time, loss = cnn_model.loss(x0,x1,y0,y1)\n",
    "        loss.backward()\n",
    "        cnn_model.optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_loss += float(loss)\n",
    "            data2tensorboard(writer,float(loss),train_loss/(b+1),e*len(dataloader)+(b+1))\n",
    "            if ((b+1)%args.display == 0):\n",
    "                 display_loss(e*len(dataloader)+(b+1),args.num_epochs*len(dataloader),e,\n",
    "                              train_loss/(b+1),batch_time,cnn_model.scheduler.get_last_lr()[0])          \n",
    "            if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                cnn_model.save_model('model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "                \n",
    "    cnn_model.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
