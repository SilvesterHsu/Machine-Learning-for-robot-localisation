{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:51.007359Z",
     "start_time": "2020-06-08T20:55:51.005002Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#from apex import amp,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:51.771722Z",
     "start_time": "2020-06-08T20:55:51.037774Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:51.808590Z",
     "start_time": "2020-06-08T20:55:51.805854Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:51.838850Z",
     "start_time": "2020-06-08T20:55:51.825870Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.95, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.1, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:52.327821Z",
     "start_time": "2020-06-08T20:55:51.856646Z"
    },
    "code_folding": [
     13
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "import time\n",
    "\n",
    "class LocalizationDataset(Dataset):\n",
    "    \"\"\"class:`LocalizationDataset`\n",
    "\n",
    "    This class is used to load data. Designed for pytorch's DataLoader.\n",
    "    Get a dictionary of data for each iteration, which contains 2 lists\n",
    "    of data pairs.\n",
    "    E.g. {'image': [image_0, image_1], 'target': [target_0, target_1]}\n",
    "\n",
    "    Attributes:\n",
    "        dataset_dirs->list: A list of the path of the dataset. Each path\n",
    "            should contain at least the index.txt file and the images \n",
    "            and poses folders.\n",
    "        image_size->list: Specify the shape of image, e.g. [300,300].\n",
    "        sampling_rate->int: The sample rate. Must be greater than 0, \n",
    "            e.g. 5\n",
    "        frames->int: The number of relative frames. Must be greater than 0.\n",
    "        transform->torchvision.transforms: Image transform only.\n",
    "        normalize->bool: Whether the target needs to be normalized.\n",
    "        \n",
    "    Raises:\n",
    "        TypeError: `sampling_rate` and `frames` should be integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dirs, image_size=[300,300], frames=3, sampling_rate=2, \\\n",
    "                 transform=None, normalize=False):\n",
    "        self.dataset_dirs = dataset_dirs\n",
    "        self.image_size = image_size\n",
    "        self.num_connected_frames = frames\n",
    "        self.sample = sampling_rate\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.Images = list()\n",
    "        self.Targets = list()\n",
    "        \n",
    "        self.__loaddata()\n",
    "        self.norm_mean,self.norm_std = self.get_norm()\n",
    "        \n",
    "    def __loaddata(self):\n",
    "        for dataset_dir in self.dataset_dirs:\n",
    "            # Read index from files\n",
    "            with open(os.path.join(dataset_dir, 'index.txt'), \"r\") as f:\n",
    "                file_index = f.readlines() # a list of indexes\n",
    "                file_index = file_index[self.sample-1::self.sample]\n",
    "\n",
    "            for index in tqdm(file_index):\n",
    "                # Read images\n",
    "                img = np.array(Image.open(os.path.join(dataset_dir, 'images', index[:-1] + '.png')), dtype=np.uint8)\n",
    "                img = img[:, :, np.newaxis]\n",
    "                # Read poses\n",
    "                target = np.loadtxt(os.path.join(dataset_dir, 'poses', index[:-1] + '.txt'))\n",
    "                [px, py, pz, ex, ey, ez] = target\n",
    "                [qx, qy, qz, qw] = tf_tran.quaternion_from_euler(ex, ey, ez, 'rxyz')\n",
    "                target = np.array([px, py, pz, qx, qy, qz, qw], dtype=np.float32)\n",
    "                #np.testing.assert_approx_equal(np.sum(target[-4:] ** 2), 1.0, significant=5)\n",
    "                \n",
    "                img,target = self._image_argumentation(img,target,self.image_size)\n",
    "                \n",
    "                self.Images.append(img)\n",
    "                self.Targets.append(target)\n",
    "    \n",
    "    def _normalize(self, target):\n",
    "        target_trans = target[:3]\n",
    "        target_trans = (target_trans-self.norm_mean)/self.norm_std\n",
    "        target_normed = np.hstack([target_trans,target[3:]])\n",
    "        return target_normed \n",
    "    \n",
    "    def _image_argumentation(self, img, target, target_image_size=[200, 200], mode='train', rot_angle=None):\n",
    "        RES = 100.0 / 400.0\n",
    "\n",
    "        margin_row = img.shape[0] - target_image_size[0]\n",
    "        margin_col = img.shape[1] - target_image_size[1]\n",
    "\n",
    "        [px, py, pz, qx, qy, qz, qw] = target\n",
    "\n",
    "        if mode == 'train':\n",
    "            np.random.seed(0) # fixed\n",
    "            offset_row = int(max(0.0, min(1.0, np.random.normal(0.5, 0.1))) * margin_row)\n",
    "            offset_col = int(max(0.0, min(1.0, np.random.normal(0.5, 0.1))) * margin_col)\n",
    "            # offset_row = random.randint(0, margin_row)\n",
    "            # offset_col = random.randint(0, margin_col)\n",
    "\n",
    "            '''sensor offset'''\n",
    "            # T_base2laser = tf_tran.euler_matrix(0.807*math.pi/180., 0.166*math.pi/180., -90.703*math.pi/ 180., 'rxyz')\n",
    "            # T_base2laser[0:3, 3] = [0.002, -0.004, -0.957]\n",
    "            # T_laser2base = np.linalg.inv(T_base2laser)\n",
    "\n",
    "            '''argumentation'''\n",
    "            offset_x = (offset_row - int(margin_row / 2)) * RES\n",
    "            offset_y = -(offset_col - int(margin_col / 2)) * RES\n",
    "\n",
    "            if rot_angle is None:\n",
    "                # not rotate\n",
    "                deltaT = tf_tran.identity_matrix()\n",
    "            else:\n",
    "                # rotate image 180 with noise\n",
    "                angle = rot_angle\n",
    "\n",
    "                deltaT = tf_tran.rotation_matrix(angle, (0,0,1))\n",
    "                img = imutils.rotate(img[:, :, 0], angle/math.pi*180)\n",
    "                img = np.array(img)[:, :, np.newaxis]\n",
    "\n",
    "            deltaT[0:3, 3] = [offset_x, offset_y, 0.]\n",
    "\n",
    "            # transform offset from /laser to /base\n",
    "            # deltaT = np.matmul(deltaT, T_laser2base)\n",
    "\n",
    "            '''gt'''\n",
    "            T = tf_tran.quaternion_matrix([qx, qy, qz, qw])\n",
    "            T[0:3, 3] = [px, py, pz]\n",
    "\n",
    "            # apply on global pose\n",
    "            T = np.matmul(deltaT, T)\n",
    "\n",
    "            position = np.array(tf_tran.translation_from_matrix(T), dtype=np.single)\n",
    "            quaternion = np.array(tf_tran.quaternion_from_matrix(T), dtype=np.single)\n",
    "\n",
    "            target = np.concatenate((position, quaternion), axis=0)\n",
    "\n",
    "        else:\n",
    "            offset_row = int(margin_row / 2)\n",
    "            offset_col = int(margin_col / 2)\n",
    "\n",
    "        img = img[offset_row:offset_row + target_image_size[0], offset_col:offset_col + target_image_size[1], :]\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Targets) - self.num_connected_frames\n",
    "    \n",
    "    def __getitem_single__(self, idx): # next\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx +=  self.num_connected_frames\n",
    "            \n",
    "        image = self.Images[idx]\n",
    "        target = self.Targets[idx]\n",
    "        image,target = self._image_argumentation(image,target,self.image_size)\n",
    "        \n",
    "        np.testing.assert_approx_equal(np.sum(target[-4:] ** 2), 1.0, significant=5)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'image': image, 'target': target}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx): # next_pair\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx +=  self.num_connected_frames\n",
    "        \n",
    "        random.seed(0) # fixed\n",
    "        paired_frame_offset = random.randint(1, self.num_connected_frames)\n",
    "        image_0 = self.Images[idx-paired_frame_offset]\n",
    "        image_1 = self.Images[idx]\n",
    "        target_0 = self.Targets[idx-paired_frame_offset]\n",
    "        target_1 = self.Targets[idx]\n",
    "        \n",
    "        # move to load part\n",
    "        #image_0, target_0 = self._image_argumentation(image_0,target_0,self.image_size)\n",
    "        #image_1, target_1 = self._image_argumentation(image_1,target_1,self.image_size)\n",
    "        \n",
    "        if self.normalize:\n",
    "            target_0 = self._normalize(target_0)\n",
    "            target_1 = self._normalize(target_1)\n",
    "        \n",
    "        #np.testing.assert_approx_equal(np.sum(target_0[-4:] ** 2), 1.0, significant=5)\n",
    "        #np.testing.assert_approx_equal(np.sum(target_1[-4:] ** 2), 1.0, significant=5)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_0 = self.transform(image_0)\n",
    "            image_1 = self.transform(image_1)\n",
    "        sample = {'image': [image_0, image_1], 'target': [target_0, target_1]}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def get_norm(self):\n",
    "        ''' get the mean and std of pose [px, py, pz]\n",
    "        Args:\n",
    "            pos->np.array: [[px, py, pz],...,[px, py, pz]], with shape of (m, 3)\n",
    "        Returns:\n",
    "            norm_mean->np.array: [x_mean, y_mean, z_mean], with shape of (3,)\n",
    "            norm_std->np.array: [x_std, y_std, z_std], with shape of (3,)\n",
    "        '''\n",
    "        pos = np.array(self.Targets)[:,:3]\n",
    "        norm_mean = np.mean(pos, axis=0)\n",
    "        norm_std = np.std(pos - norm_mean,axis=0)\n",
    "        return norm_mean,norm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:59.133356Z",
     "start_time": "2020-06-08T20:55:52.348575Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5593/5593 [00:06<00:00, 825.93it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform)\n",
    "[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:59.181896Z",
     "start_time": "2020-06-08T20:55:59.152181Z"
    },
    "code_folding": [
     22,
     74,
     89,
     103,
     109
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True) # dense_feat\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        #self.relative_context = vggnet(input_channel=4096,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self, input_data_t0, input_data_t1):\n",
    "        dense_feat0 = self.resnet(input_data_t0)\n",
    "        dense_feat1 = self.resnet(input_data_t1)\n",
    "        #dense_feat_relative = torch.cat([dense_feat0,dense_feat1],dim=1)\n",
    "        \n",
    "        global_context_feat0 = self.global_context(dense_feat0)\n",
    "        global_context_feat1 = self.global_context(dense_feat1)\n",
    "        #relative_context_feat = self.relative_context(dense_feat_relative)\n",
    "        \n",
    "        global_output0 = self.global_regressor(global_context_feat0)\n",
    "        global_output1 = self.global_regressor(global_context_feat1)\n",
    "        \n",
    "        return global_output0,global_output1#,relative_context_feat \n",
    "        \n",
    "def quanternion2matrix(q):\n",
    "    tx, ty, tz, qx, qy, qz, qw = torch.split(q,[1, 1, 1, 1, 1, 1, 1], dim=-1)\n",
    "    M11 = 1.0 - 2 * (torch.square(qy) + torch.square(qz))\n",
    "    M12 = 2. * qx * qy - 2. * qw * qz\n",
    "    M13 = 2. * qw * qy + 2. * qx * qz\n",
    "    M14 = tx\n",
    "\n",
    "    M21 = 2. * qx * qy + 2. * qw * qz\n",
    "    M22 = 1. - 2. * (torch.square(qx) + torch.square(qz))\n",
    "    M23 = -2. * qw * qx + 2. * qy * qz\n",
    "    M24 = ty\n",
    "\n",
    "    M31 = -2. * qw * qy + 2. * qx * qz\n",
    "    M32 = 2. * qw * qx + 2. * qy * qz\n",
    "    M33 = 1. - 2. * (torch.square(qx) + torch.square(qy))\n",
    "    M34 = tz\n",
    "\n",
    "    M41 = torch.zeros_like(M11)\n",
    "    M42 = torch.zeros_like(M11)\n",
    "    M43 = torch.zeros_like(M11)\n",
    "    M44 = torch.ones_like(M11)\n",
    "\n",
    "    #M11.unsqueeze_(-1)\n",
    "    M11 = torch.unsqueeze(M11, axis=-1)\n",
    "    M12 = torch.unsqueeze(M12, axis=-1)\n",
    "    M13 = torch.unsqueeze(M13, axis=-1)\n",
    "    M14 = torch.unsqueeze(M14, axis=-1)\n",
    "\n",
    "    M21 = torch.unsqueeze(M21, axis=-1)\n",
    "    M22 = torch.unsqueeze(M22, axis=-1)\n",
    "    M23 = torch.unsqueeze(M23, axis=-1)\n",
    "    M24 = torch.unsqueeze(M24, axis=-1)\n",
    "\n",
    "    M31 = torch.unsqueeze(M31, axis=-1)\n",
    "    M32 = torch.unsqueeze(M32, axis=-1)\n",
    "    M33 = torch.unsqueeze(M33, axis=-1)\n",
    "    M34 = torch.unsqueeze(M34, axis=-1)\n",
    "\n",
    "    M41 = torch.unsqueeze(M41, axis=-1)\n",
    "    M42 = torch.unsqueeze(M42, axis=-1)\n",
    "    M43 = torch.unsqueeze(M43, axis=-1)\n",
    "    M44 = torch.unsqueeze(M44, axis=-1)\n",
    "\n",
    "    M_l1 = torch.cat([M11, M12, M13, M14], axis=2)\n",
    "    M_l2 = torch.cat([M21, M22, M23, M24], axis=2)\n",
    "    M_l3 = torch.cat([M31, M32, M33, M34], axis=2)\n",
    "    M_l4 = torch.cat([M41, M42, M43, M44], axis=2)\n",
    "\n",
    "    M = torch.cat([M_l1, M_l2, M_l3, M_l4], axis=1)\n",
    "\n",
    "    return M\n",
    "\n",
    "def matrix2quternion(M):\n",
    "    tx = M[:, 0, 3].unsqueeze(-1)\n",
    "    ty = M[:, 1, 3].unsqueeze(-1)\n",
    "    tz = M[:, 2, 3].unsqueeze(-1)\n",
    "    qw = 0.5 * torch.sqrt(M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] + M[:, 3, 3]).unsqueeze(-1)\n",
    "\n",
    "    mask = torch.abs(qw)<10e-6\n",
    "    qw = qw if mask.sum()==mask.shape[0] else qw+10e-6\n",
    "\n",
    "    qx = torch.unsqueeze(M[:, 2, 1] - M[:, 1, 2],-1) / (4. * qw)\n",
    "    qy = torch.unsqueeze(M[:, 0, 2] - M[:, 2, 0],-1) / (4. * qw)\n",
    "    qz = torch.unsqueeze(M[:, 1, 0] - M[:, 0, 1],-1) / (4. * qw)\n",
    "    q = torch.cat([tx, ty, tz, qx, qy, qz, qw], dim=-1)\n",
    "    return q\n",
    "\n",
    "def get_relative_pose(Q_a,Q_b):\n",
    "    M_a = quanternion2matrix(Q_a)\n",
    "    M_b = quanternion2matrix(Q_b)\n",
    "\n",
    "    try:\n",
    "        M_delta = torch.matmul(M_a.inverse(),M_b)\n",
    "    except ValueError:\n",
    "        print(\"matrix is not invertiable\")\n",
    "        M_delta = torch.eye(4).repeat(M_a.shape[0],1,1)\n",
    "\n",
    "    Q_delta = matrix2quternion(M_delta)\n",
    "\n",
    "    return Q_delta\n",
    "\n",
    "def normalize(target, norm_mean, norm_std):\n",
    "    target_trans = target[:,:3]\n",
    "    target_trans = torch.div(torch.sub(target_trans,norm_mean),norm_std)\n",
    "    target_normed = torch.cat([target_trans,target[:,3:]],dim=1)\n",
    "    return target_normed \n",
    "\n",
    "def translational_rotational_loss(pred=None, gt=None, lamda=None):\n",
    "    trans_pred, rot_pred = torch.split(pred, [3,4], dim=1)\n",
    "    trans_gt, rot_gt = torch.split(gt, [3, 4], dim=1)\n",
    "    \n",
    "    trans_loss = nn.functional.mse_loss(input=trans_pred, target=trans_gt)\n",
    "    rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_gt),dim=1)))\n",
    "    \n",
    "    loss = trans_loss + lamda * rot_loss\n",
    "\n",
    "    return loss#, trans_loss, rot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:55:59.214737Z",
     "start_time": "2020-06-08T20:55:59.196566Z"
    },
    "code_folding": [
     0,
     52
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def _quanternion2matrix(q):\n",
    "    tx, ty, tz, qx, qy, qz, qw = torch.split(q,[1, 1, 1, 1, 1, 1, 1], dim=-1)\n",
    "    M11 = 1.0 - 2 * (torch.square(qy) + torch.square(qz))\n",
    "    M12 = 2. * qx * qy - 2. * qw * qz\n",
    "    M13 = 2. * qw * qy + 2. * qx * qz\n",
    "    M14 = tx\n",
    "\n",
    "    M21 = 2. * qx * qy + 2. * qw * qz\n",
    "    M22 = 1. - 2. * (torch.square(qx) + torch.square(qz))\n",
    "    M23 = -2. * qw * qx + 2. * qy * qz\n",
    "    M24 = ty\n",
    "\n",
    "    M31 = -2. * qw * qy + 2. * qx * qz\n",
    "    M32 = 2. * qw * qx + 2. * qy * qz\n",
    "    M33 = 1. - 2. * (torch.square(qx) + torch.square(qy))\n",
    "    M34 = tz\n",
    "\n",
    "    M41 = torch.zeros_like(M11)\n",
    "    M42 = torch.zeros_like(M11)\n",
    "    M43 = torch.zeros_like(M11)\n",
    "    M44 = torch.ones_like(M11)\n",
    "\n",
    "    #M11.unsqueeze_(-1)\n",
    "    M11.unsqueeze_(axis=-1)\n",
    "    M12.unsqueeze_(axis=-1)\n",
    "    M13.unsqueeze_(axis=-1)\n",
    "    M14.unsqueeze_(axis=-1)\n",
    "\n",
    "    M21.unsqueeze_(axis=-1)\n",
    "    M22.unsqueeze_(axis=-1)\n",
    "    M23.unsqueeze_(axis=-1)\n",
    "    M24.unsqueeze_(axis=-1)\n",
    "\n",
    "    M31.unsqueeze_(axis=-1)\n",
    "    M32.unsqueeze_(axis=-1)\n",
    "    M33.unsqueeze_(axis=-1)\n",
    "    M34.unsqueeze_(axis=-1)\n",
    "\n",
    "    M41.unsqueeze_(axis=-1)\n",
    "    M42.unsqueeze_(axis=-1)\n",
    "    M43.unsqueeze_(axis=-1)\n",
    "    M44.unsqueeze_(axis=-1)\n",
    "\n",
    "    M_l1 = torch.cat([M11, M12, M13, M14], axis=2)\n",
    "    M_l2 = torch.cat([M21, M22, M23, M24], axis=2)\n",
    "    M_l3 = torch.cat([M31, M32, M33, M34], axis=2)\n",
    "    M_l4 = torch.cat([M41, M42, M43, M44], axis=2)\n",
    "\n",
    "    M = torch.cat([M_l1, M_l2, M_l3, M_l4], axis=1)\n",
    "\n",
    "    return M\n",
    "\n",
    "def _matrix2quternion(M):\n",
    "    tx = M[:, 0, 3].unsqueeze_(-1)\n",
    "    ty = M[:, 1, 3].unsqueeze_(-1)\n",
    "    tz = M[:, 2, 3].unsqueeze_(-1)\n",
    "    qw = 0.5 * torch.sqrt(M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] + M[:, 3, 3]).unsqueeze_(-1)\n",
    "\n",
    "    mask = torch.abs(qw)<10e-6\n",
    "    qw = qw if mask.sum()==mask.shape[0] else qw+10e-6\n",
    "\n",
    "    qx = torch.unsqueeze(M[:, 2, 1] - M[:, 1, 2],-1) / (4. * qw)\n",
    "    qy = torch.unsqueeze(M[:, 0, 2] - M[:, 2, 0],-1) / (4. * qw)\n",
    "    qz = torch.unsqueeze(M[:, 1, 0] - M[:, 0, 1],-1) / (4. * qw)\n",
    "    q = torch.cat([tx, ty, tz, qx, qy, qz, qw], dim=-1)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:56:01.986683Z",
     "start_time": "2020-06-08T20:55:59.247661Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# set to cpu\n",
    "#device = torch.device(\"cpu\")\n",
    "net = Model().to(device)\n",
    "args.norm_mean = args.norm_mean.to(device)\n",
    "args.norm_std = args.norm_std.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "#optimizer = optimizers.FusedAdam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "\n",
    "#net, optimizer = amp.initialize(net, optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:56:02.035275Z",
     "start_time": "2020-06-08T20:56:02.023114Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:59:38.180819Z",
     "start_time": "2020-06-08T20:56:02.086618Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/34800 (epoch 0), train_loss = 1.0399439215660096, time/batch = 0.573, learning rate = 0.000010000\n",
      "20/34800 (epoch 0), train_loss = 1.0459362059831618, time/batch = 0.577, learning rate = 0.000010000\n",
      "30/34800 (epoch 0), train_loss = 1.0152105073134103, time/batch = 0.576, learning rate = 0.000010000\n",
      "40/34800 (epoch 0), train_loss = 1.0272188991308213, time/batch = 0.576, learning rate = 0.000010000\n",
      "50/34800 (epoch 0), train_loss = 1.0174634444713593, time/batch = 0.576, learning rate = 0.000010000\n",
      "60/34800 (epoch 0), train_loss = 1.029318485657374, time/batch = 0.577, learning rate = 0.000010000\n",
      "70/34800 (epoch 0), train_loss = 1.0376757417406355, time/batch = 0.584, learning rate = 0.000010000\n",
      "80/34800 (epoch 0), train_loss = 1.0477473214268684, time/batch = 0.579, learning rate = 0.000010000\n",
      "90/34800 (epoch 0), train_loss = 1.0426623708671994, time/batch = 0.578, learning rate = 0.000010000\n",
      "100/34800 (epoch 0), train_loss = 1.0443155682086944, time/batch = 0.579, learning rate = 0.000010000\n",
      "110/34800 (epoch 0), train_loss = 1.0331639463251288, time/batch = 0.584, learning rate = 0.000010000\n",
      "120/34800 (epoch 0), train_loss = 1.0080852448940276, time/batch = 0.581, learning rate = 0.000010000\n",
      "130/34800 (epoch 0), train_loss = 0.9777698507675758, time/batch = 0.583, learning rate = 0.000010000\n",
      "140/34800 (epoch 0), train_loss = 0.9490130254200526, time/batch = 0.580, learning rate = 0.000010000\n",
      "150/34800 (epoch 0), train_loss = 0.9194190432627996, time/batch = 0.581, learning rate = 0.000010000\n",
      "160/34800 (epoch 0), train_loss = 0.8911981388926506, time/batch = 0.582, learning rate = 0.000010000\n",
      "170/34800 (epoch 0), train_loss = 0.8643901090411579, time/batch = 0.583, learning rate = 0.000010000\n",
      "184/34800 (epoch 1), train_loss = 0.4078315407037735, time/batch = 0.583, learning rate = 0.000009500\n",
      "194/34800 (epoch 1), train_loss = 0.3830097302794456, time/batch = 0.581, learning rate = 0.000009500\n",
      "204/34800 (epoch 1), train_loss = 0.3758177240689596, time/batch = 0.586, learning rate = 0.000009500\n",
      "214/34800 (epoch 1), train_loss = 0.35965944863855837, time/batch = 0.583, learning rate = 0.000009500\n",
      "224/34800 (epoch 1), train_loss = 0.34674828678369524, time/batch = 0.585, learning rate = 0.000009500\n",
      "234/34800 (epoch 1), train_loss = 0.34420616378386815, time/batch = 0.586, learning rate = 0.000009500\n",
      "244/34800 (epoch 1), train_loss = 0.3370670567665781, time/batch = 0.581, learning rate = 0.000009500\n",
      "254/34800 (epoch 1), train_loss = 0.33146592155098914, time/batch = 0.581, learning rate = 0.000009500\n",
      "264/34800 (epoch 1), train_loss = 0.32458588613404166, time/batch = 0.582, learning rate = 0.000009500\n",
      "274/34800 (epoch 1), train_loss = 0.31428307861089705, time/batch = 0.584, learning rate = 0.000009500\n",
      "284/34800 (epoch 1), train_loss = 0.30846986269409005, time/batch = 0.582, learning rate = 0.000009500\n",
      "294/34800 (epoch 1), train_loss = 0.3060584464420875, time/batch = 0.583, learning rate = 0.000009500\n",
      "304/34800 (epoch 1), train_loss = 0.30106460394767615, time/batch = 0.584, learning rate = 0.000009500\n",
      "314/34800 (epoch 1), train_loss = 0.29401775151491166, time/batch = 0.582, learning rate = 0.000009500\n",
      "324/34800 (epoch 1), train_loss = 0.28909313877423604, time/batch = 0.582, learning rate = 0.000009500\n",
      "334/34800 (epoch 1), train_loss = 0.2836343233473599, time/batch = 0.585, learning rate = 0.000009500\n",
      "344/34800 (epoch 1), train_loss = 0.27750465379041783, time/batch = 0.583, learning rate = 0.000009500\n"
     ]
    }
   ],
   "source": [
    "#for e in range(args.num_epochs):\n",
    "for e in range(2):\n",
    "    if e != 0:\n",
    "        scheduler.step()\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(dataloader, 0):\n",
    "\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x0, x1 = data['image']\n",
    "        y0, y1 = data['target']\n",
    "        x0,x1,y0,y1 = x0.to(device),x1.to(device),y0.to(device),y1.to(device)\n",
    "        # normalize targets\n",
    "        y0_norm, y1_norm = [normalize(y,args.norm_mean, args.norm_std) for y in [y0,y1]]\n",
    "        relative_target_normed = get_relative_pose(y0_norm, y1_norm)\n",
    "        \n",
    "        # Part 1: Net Forward\n",
    "        global_output0,global_output1 = net(x0, x1)\n",
    "\n",
    "        # Part 2: Loss\n",
    "        \n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, \\\n",
    "                                                    gt=y1_norm, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + 0.1 * geometry_consistent_loss\n",
    "        \n",
    "        # Part 3: Net Backward\n",
    "        #with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #del global_output0,global_output1,relative_consistence,global_loss,geometry_consistent_loss\n",
    "        end = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss += float(total_loss)\n",
    "            if ((b+1)%args.display == 0):\n",
    "                 print(\n",
    "                    \"{}/{} (epoch {}), train_loss = {}, time/batch = {:.3f}, learning rate = {:.9f}\"\n",
    "                    .format(\n",
    "                    e * len(dataloader) + (b+1),\n",
    "                    args.num_epochs * len(dataloader),\n",
    "                    e,\n",
    "                    train_loss/(b+1),\n",
    "                    end - start,\n",
    "                    optimizer.param_groups[0]['lr']))\n",
    "            if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                checkpoint_path = os.path.join(args.model_dir, 'model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "                torch.save(net.state_dict(),checkpoint_path)\n",
    "                print('saving model to model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
