{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:07.534153Z",
     "start_time": "2020-06-20T23:08:07.531739Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#from apex import amp,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:07.957243Z",
     "start_time": "2020-06-20T23:08:07.535897Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:08.129053Z",
     "start_time": "2020-06-20T23:08:07.958869Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/train_dual_cnn_torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:08.144445Z",
     "start_time": "2020-06-20T23:08:08.131019Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.9, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.1, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:08.620279Z",
     "start_time": "2020-06-20T23:08:08.145822Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:15.179148Z",
     "start_time": "2020-06-20T23:08:08.621766Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5593/5593 [00:06<00:00, 854.62it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform)\n",
    "[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:15.208685Z",
     "start_time": "2020-06-20T23:08:15.180582Z"
    },
    "code_folding": [
     0,
     22,
     74,
     86,
     100,
     106
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True) # dense_feat\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        #self.relative_context = vggnet(input_channel=4096,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self, input_data_t0, input_data_t1):\n",
    "        dense_feat0 = self.resnet(input_data_t0)\n",
    "        dense_feat1 = self.resnet(input_data_t1)\n",
    "        #dense_feat_relative = torch.cat([dense_feat0,dense_feat1],dim=1)\n",
    "        \n",
    "        global_context_feat0 = self.global_context(dense_feat0)\n",
    "        global_context_feat1 = self.global_context(dense_feat1)\n",
    "        #relative_context_feat = self.relative_context(dense_feat_relative)\n",
    "        \n",
    "        global_output0,_,_ = self.global_regressor(global_context_feat0)\n",
    "        global_output1,_,_ = self.global_regressor(global_context_feat1)\n",
    "        \n",
    "        return global_output0,global_output1#,relative_context_feat \n",
    "        \n",
    "def quanternion2matrix(q):\n",
    "    tx, ty, tz, qx, qy, qz, qw = torch.split(q,[1, 1, 1, 1, 1, 1, 1], dim=-1)\n",
    "    M11 = 1.0 - 2 * (torch.square(qy) + torch.square(qz))\n",
    "    M12 = 2. * qx * qy - 2. * qw * qz\n",
    "    M13 = 2. * qw * qy + 2. * qx * qz\n",
    "    M14 = tx\n",
    "\n",
    "    M21 = 2. * qx * qy + 2. * qw * qz\n",
    "    M22 = 1. - 2. * (torch.square(qx) + torch.square(qz))\n",
    "    M23 = -2. * qw * qx + 2. * qy * qz\n",
    "    M24 = ty\n",
    "\n",
    "    M31 = -2. * qw * qy + 2. * qx * qz\n",
    "    M32 = 2. * qw * qx + 2. * qy * qz\n",
    "    M33 = 1. - 2. * (torch.square(qx) + torch.square(qy))\n",
    "    M34 = tz\n",
    "\n",
    "    M41 = torch.zeros_like(M11)\n",
    "    M42 = torch.zeros_like(M11)\n",
    "    M43 = torch.zeros_like(M11)\n",
    "    M44 = torch.ones_like(M11)\n",
    "\n",
    "    #M11.unsqueeze_(-1)\n",
    "    M11 = torch.unsqueeze(M11, axis=-1)\n",
    "    M12 = torch.unsqueeze(M12, axis=-1)\n",
    "    M13 = torch.unsqueeze(M13, axis=-1)\n",
    "    M14 = torch.unsqueeze(M14, axis=-1)\n",
    "\n",
    "    M21 = torch.unsqueeze(M21, axis=-1)\n",
    "    M22 = torch.unsqueeze(M22, axis=-1)\n",
    "    M23 = torch.unsqueeze(M23, axis=-1)\n",
    "    M24 = torch.unsqueeze(M24, axis=-1)\n",
    "\n",
    "    M31 = torch.unsqueeze(M31, axis=-1)\n",
    "    M32 = torch.unsqueeze(M32, axis=-1)\n",
    "    M33 = torch.unsqueeze(M33, axis=-1)\n",
    "    M34 = torch.unsqueeze(M34, axis=-1)\n",
    "\n",
    "    M41 = torch.unsqueeze(M41, axis=-1)\n",
    "    M42 = torch.unsqueeze(M42, axis=-1)\n",
    "    M43 = torch.unsqueeze(M43, axis=-1)\n",
    "    M44 = torch.unsqueeze(M44, axis=-1)\n",
    "\n",
    "    M_l1 = torch.cat([M11, M12, M13, M14], axis=2)\n",
    "    M_l2 = torch.cat([M21, M22, M23, M24], axis=2)\n",
    "    M_l3 = torch.cat([M31, M32, M33, M34], axis=2)\n",
    "    M_l4 = torch.cat([M41, M42, M43, M44], axis=2)\n",
    "\n",
    "    M = torch.cat([M_l1, M_l2, M_l3, M_l4], axis=1)\n",
    "\n",
    "    return M\n",
    "\n",
    "def matrix2quternion(M):\n",
    "    eps = torch.finfo(M.dtype).eps\n",
    "    tx = M[:, 0, 3].unsqueeze(-1)\n",
    "    ty = M[:, 1, 3].unsqueeze(-1)\n",
    "    tz = M[:, 2, 3].unsqueeze(-1)\n",
    "    qw = 0.5 * torch.sqrt(M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] + M[:, 3, 3] + eps).unsqueeze(-1) # sqrt ！= 0 \n",
    "    qx = torch.unsqueeze(M[:, 2, 1] - M[:, 1, 2],-1) / (4. * qw)\n",
    "    qy = torch.unsqueeze(M[:, 0, 2] - M[:, 2, 0],-1) / (4. * qw)\n",
    "    qz = torch.unsqueeze(M[:, 1, 0] - M[:, 0, 1],-1) / (4. * qw)\n",
    "    q = torch.cat([tx, ty, tz, qx, qy, qz, qw], dim=-1)\n",
    "    return q\n",
    "\n",
    "def get_relative_pose(Q_a,Q_b):\n",
    "    M_a = quanternion2matrix(Q_a)\n",
    "    M_b = quanternion2matrix(Q_b)\n",
    "\n",
    "    try:\n",
    "        M_delta = torch.matmul(M_a.inverse(),M_b)\n",
    "    except ValueError:\n",
    "        print(\"matrix is not invertiable\")\n",
    "        M_delta = torch.eye(4).repeat(M_a.shape[0],1,1)\n",
    "\n",
    "    Q_delta = matrix2quternion(M_delta)\n",
    "\n",
    "    return Q_delta\n",
    "\n",
    "def normalize(target, norm_mean, norm_std):\n",
    "    target_trans = target[:,:3]\n",
    "    target_trans = torch.div(torch.sub(target_trans,norm_mean),norm_std)\n",
    "    target_normed = torch.cat([target_trans,target[:,3:]],dim=1)\n",
    "    return target_normed \n",
    "\n",
    "def translational_rotational_loss(pred=None, gt=None, lamda=None):\n",
    "    trans_pred, rot_pred = torch.split(pred, [3,4], dim=1)\n",
    "    trans_gt, rot_gt = torch.split(gt, [3, 4], dim=1)\n",
    "    \n",
    "    trans_loss = nn.functional.mse_loss(input=trans_pred, target=trans_gt)\n",
    "    rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_gt),dim=1)))\n",
    "    \n",
    "    loss = trans_loss + lamda * rot_loss\n",
    "\n",
    "    return loss#, trans_loss, rot_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:08:15.226880Z",
     "start_time": "2020-06-20T23:08:15.210218Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith torch.no_grad():\\n    graphs = Model()\\n    x0,x1 = next(iter(dataloader))['image']\\n    writer.add_graph(graphs, (x0,x1))\\ndel x0,x1,graphs\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    graphs = Model()\n",
    "    x0,x1 = next(iter(dataloader))['image']\n",
    "    writer.add_graph(graphs, (x0,x1))\n",
    "del x0,x1,graphs\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:15:40.395378Z",
     "start_time": "2020-06-20T23:15:38.907380Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.bn1.running_mean torch.Size([64])\n",
      "resnet.bn1.running_var torch.Size([64])\n",
      "resnet.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.bn1.running_mean torch.Size([64])\n",
      "resnet.layer1.0.bn1.running_var torch.Size([64])\n",
      "resnet.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.bn2.running_mean torch.Size([64])\n",
      "resnet.layer1.0.bn2.running_var torch.Size([64])\n",
      "resnet.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.bn3.running_mean torch.Size([256])\n",
      "resnet.layer1.0.bn3.running_var torch.Size([256])\n",
      "resnet.layer1.0.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.running_mean torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.running_var torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.bn1.running_mean torch.Size([64])\n",
      "resnet.layer1.1.bn1.running_var torch.Size([64])\n",
      "resnet.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.bn2.running_mean torch.Size([64])\n",
      "resnet.layer1.1.bn2.running_var torch.Size([64])\n",
      "resnet.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.1.bn3.running_mean torch.Size([256])\n",
      "resnet.layer1.1.bn3.running_var torch.Size([256])\n",
      "resnet.layer1.1.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.bn1.running_mean torch.Size([64])\n",
      "resnet.layer1.2.bn1.running_var torch.Size([64])\n",
      "resnet.layer1.2.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.bn2.running_mean torch.Size([64])\n",
      "resnet.layer1.2.bn2.running_var torch.Size([64])\n",
      "resnet.layer1.2.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.bn3.running_mean torch.Size([256])\n",
      "resnet.layer1.2.bn3.running_var torch.Size([256])\n",
      "resnet.layer1.2.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.bn1.running_mean torch.Size([128])\n",
      "resnet.layer2.0.bn1.running_var torch.Size([128])\n",
      "resnet.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.bn2.running_mean torch.Size([128])\n",
      "resnet.layer2.0.bn2.running_var torch.Size([128])\n",
      "resnet.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.bn3.running_mean torch.Size([512])\n",
      "resnet.layer2.0.bn3.running_var torch.Size([512])\n",
      "resnet.layer2.0.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.running_mean torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.running_var torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.bn1.running_mean torch.Size([128])\n",
      "resnet.layer2.1.bn1.running_var torch.Size([128])\n",
      "resnet.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.bn2.running_mean torch.Size([128])\n",
      "resnet.layer2.1.bn2.running_var torch.Size([128])\n",
      "resnet.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.1.bn3.running_mean torch.Size([512])\n",
      "resnet.layer2.1.bn3.running_var torch.Size([512])\n",
      "resnet.layer2.1.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.bn1.running_mean torch.Size([128])\n",
      "resnet.layer2.2.bn1.running_var torch.Size([128])\n",
      "resnet.layer2.2.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.bn2.running_mean torch.Size([128])\n",
      "resnet.layer2.2.bn2.running_var torch.Size([128])\n",
      "resnet.layer2.2.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.bn3.running_mean torch.Size([512])\n",
      "resnet.layer2.2.bn3.running_var torch.Size([512])\n",
      "resnet.layer2.2.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.bn1.running_mean torch.Size([128])\n",
      "resnet.layer2.3.bn1.running_var torch.Size([128])\n",
      "resnet.layer2.3.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.bn2.running_mean torch.Size([128])\n",
      "resnet.layer2.3.bn2.running_var torch.Size([128])\n",
      "resnet.layer2.3.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.bn3.running_mean torch.Size([512])\n",
      "resnet.layer2.3.bn3.running_var torch.Size([512])\n",
      "resnet.layer2.3.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.0.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.0.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.0.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.0.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.running_mean torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.running_var torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.1.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.1.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.1.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.1.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.1.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.2.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.2.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.2.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.2.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.2.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.2.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.3.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.3.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.3.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.3.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.3.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.3.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.4.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.4.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.4.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.4.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.4.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.4.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.bn1.running_mean torch.Size([256])\n",
      "resnet.layer3.5.bn1.running_var torch.Size([256])\n",
      "resnet.layer3.5.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.bn2.running_mean torch.Size([256])\n",
      "resnet.layer3.5.bn2.running_var torch.Size([256])\n",
      "resnet.layer3.5.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.bn3.running_mean torch.Size([1024])\n",
      "resnet.layer3.5.bn3.running_var torch.Size([1024])\n",
      "resnet.layer3.5.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.bn1.running_mean torch.Size([512])\n",
      "resnet.layer4.0.bn1.running_var torch.Size([512])\n",
      "resnet.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.bn2.running_mean torch.Size([512])\n",
      "resnet.layer4.0.bn2.running_var torch.Size([512])\n",
      "resnet.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.bn3.running_mean torch.Size([2048])\n",
      "resnet.layer4.0.bn3.running_var torch.Size([2048])\n",
      "resnet.layer4.0.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.running_mean torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.running_var torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.bn1.running_mean torch.Size([512])\n",
      "resnet.layer4.1.bn1.running_var torch.Size([512])\n",
      "resnet.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.bn2.running_mean torch.Size([512])\n",
      "resnet.layer4.1.bn2.running_var torch.Size([512])\n",
      "resnet.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.1.bn3.running_mean torch.Size([2048])\n",
      "resnet.layer4.1.bn3.running_var torch.Size([2048])\n",
      "resnet.layer4.1.bn3.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.bn1.running_mean torch.Size([512])\n",
      "resnet.layer4.2.bn1.running_var torch.Size([512])\n",
      "resnet.layer4.2.bn1.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.bn2.running_mean torch.Size([512])\n",
      "resnet.layer4.2.bn2.running_var torch.Size([512])\n",
      "resnet.layer4.2.bn2.num_batches_tracked torch.Size([])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.bn3.running_mean torch.Size([2048])\n",
      "resnet.layer4.2.bn3.running_var torch.Size([2048])\n",
      "resnet.layer4.2.bn3.num_batches_tracked torch.Size([])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n",
      "Parameters layer: 346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# set to cpu\n",
    "#device = torch.device(\"cpu\")\n",
    "net = Model().to(device)\n",
    "state_dict = torch.load(os.path.join(args.model_dir,'pretrained.pth'))\n",
    "# pretrained\n",
    "for name,param in state_dict.items():\n",
    "    print(name, param.shape)\n",
    "print('Parameters layer:',len(state_dict.keys()))\n",
    "\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T23:15:19.596821Z",
     "start_time": "2020-06-20T23:15:19.555413Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n",
      "Parameters layer: 346\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "print('Parameters layer:',len(net.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T22:23:59.886325Z",
     "start_time": "2020-06-20T22:23:59.877209Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "args.norm_mean = args.norm_mean.to(device)\n",
    "args.norm_std = args.norm_std.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "#optimizer = optimizers.FusedAdam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "\n",
    "#net, optimizer = amp.initialize(net, optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T22:23:59.899177Z",
     "start_time": "2020-06-20T22:23:59.887586Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    net.train()\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(dataloader, 0):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            x0, x1 = data['image']\n",
    "            y0, y1 = data['target']\n",
    "            x0,x1,y0,y1 = x0.to(device),x1.to(device),y0.to(device),y1.to(device)\n",
    "            # normalize targets\n",
    "            y0_norm, y1_norm = [normalize(y,args.norm_mean, args.norm_std) for y in [y0,y1]]\n",
    "            relative_target_normed = get_relative_pose(y0_norm, y1_norm)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Part 1: Net Forward\n",
    "        global_output0,global_output1 = net(x0, x1)\n",
    "        # Part 2: Loss\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, \\\n",
    "                                                    gt=y1_norm, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + geometry_consistent_loss        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        with torch.no_grad():\n",
    "            train_loss += float(total_loss)\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            writer.add_scalars('training loss',\n",
    "                  {'item loss':float(total_loss),\n",
    "                  'batch loss':train_loss/(b+1)},\n",
    "                  e * len(dataloader) + (b+1))\n",
    "            if ((b+1)%args.display == 0):\n",
    "                 print(\n",
    "                    \"{}/{} (epoch {}), train_loss = {}, time/batch = {:.3f}, learning rate = {:.9f}\"\n",
    "                    .format(\n",
    "                    e * len(dataloader) + (b+1),\n",
    "                    args.num_epochs * len(dataloader),\n",
    "                    e,\n",
    "                    train_loss/(b+1),\n",
    "                    end - start,\n",
    "                    lr))            \n",
    "            if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                checkpoint_path = os.path.join(args.model_dir, 'model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "                torch.save(net.state_dict(),checkpoint_path)\n",
    "                print('saving model to model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T22:29:10.751155Z",
     "start_time": "2020-06-20T22:23:59.900430Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "for e in range(args.num_epochs):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
