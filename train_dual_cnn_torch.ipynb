{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:45:58.615454Z",
     "start_time": "2020-06-11T14:45:58.610466Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#from apex import amp,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:45:59.348936Z",
     "start_time": "2020-06-11T14:45:58.625171Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:45:59.452046Z",
     "start_time": "2020-06-11T14:45:59.448127Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:45:59.493243Z",
     "start_time": "2020-06-11T14:45:59.471329Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''Training Parameters'''\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of epochs')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='learning rate')\n",
    "parser.add_argument('--learning_rate_clip', type=float, default=0.0000001, help='learning rate clip')\n",
    "parser.add_argument('--decay_rate', type=float, default=.95, help='decay rate for rmsprop')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--batch_norm_decay', type=float, default=.999, help='decay rate for rmsprop')\n",
    "parser.add_argument('--keep_prob', type=float, default=1.0, help='dropout keep probability')\n",
    "parser.add_argument('--lamda_weights', type=float, default=0.1, help='lamda weight')\n",
    "parser.add_argument('--data_argumentation', type=bool, default=True, help='whether do data argument')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data nomalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "\n",
    "'''Configure'''\n",
    "parser.add_argument('--network', type=str, default='vggnet_localization')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/dual_resnet_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                            '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "'''\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "'''\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--save_every', type=int, default=2000, help='save frequency')\n",
    "parser.add_argument('--display', type=int, default=10, help='display frequency')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:46:00.105848Z",
     "start_time": "2020-06-11T14:45:59.503778Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet\n",
    "from torchlib.utils import LocalizationDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:49:07.009840Z",
     "start_time": "2020-06-11T14:46:00.174239Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16446/16446 [00:22<00:00, 718.04it/s]\n",
      "100%|██████████| 22584/22584 [00:31<00:00, 716.58it/s]\n",
      "100%|██████████| 18655/18655 [00:25<00:00, 723.40it/s]\n",
      "100%|██████████| 17310/17310 [00:23<00:00, 722.94it/s]\n",
      "100%|██████████| 10766/10766 [00:14<00:00, 726.64it/s]\n",
      "100%|██████████| 14878/14878 [00:21<00:00, 681.26it/s]\n",
      "100%|██████████| 13452/13452 [00:20<00:00, 656.01it/s]\n",
      "100%|██████████| 14037/14037 [00:24<00:00, 564.31it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform)\n",
    "[args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=True, num_workers=0, \\\n",
    "                        drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:49:07.154276Z",
     "start_time": "2020-06-11T14:49:07.051270Z"
    },
    "code_folding": [
     22,
     74,
     89,
     103,
     109,
     120,
     172
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True) # dense_feat\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        #self.relative_context = vggnet(input_channel=4096,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self, input_data_t0, input_data_t1):\n",
    "        dense_feat0 = self.resnet(input_data_t0)\n",
    "        dense_feat1 = self.resnet(input_data_t1)\n",
    "        #dense_feat_relative = torch.cat([dense_feat0,dense_feat1],dim=1)\n",
    "        \n",
    "        global_context_feat0 = self.global_context(dense_feat0)\n",
    "        global_context_feat1 = self.global_context(dense_feat1)\n",
    "        #relative_context_feat = self.relative_context(dense_feat_relative)\n",
    "        \n",
    "        global_output0,_,_ = self.global_regressor(global_context_feat0)\n",
    "        global_output1,_,_ = self.global_regressor(global_context_feat1)\n",
    "        \n",
    "        return global_output0,global_output1#,relative_context_feat \n",
    "        \n",
    "def quanternion2matrix(q):\n",
    "    tx, ty, tz, qx, qy, qz, qw = torch.split(q,[1, 1, 1, 1, 1, 1, 1], dim=-1)\n",
    "    M11 = 1.0 - 2 * (torch.square(qy) + torch.square(qz))\n",
    "    M12 = 2. * qx * qy - 2. * qw * qz\n",
    "    M13 = 2. * qw * qy + 2. * qx * qz\n",
    "    M14 = tx\n",
    "\n",
    "    M21 = 2. * qx * qy + 2. * qw * qz\n",
    "    M22 = 1. - 2. * (torch.square(qx) + torch.square(qz))\n",
    "    M23 = -2. * qw * qx + 2. * qy * qz\n",
    "    M24 = ty\n",
    "\n",
    "    M31 = -2. * qw * qy + 2. * qx * qz\n",
    "    M32 = 2. * qw * qx + 2. * qy * qz\n",
    "    M33 = 1. - 2. * (torch.square(qx) + torch.square(qy))\n",
    "    M34 = tz\n",
    "\n",
    "    M41 = torch.zeros_like(M11)\n",
    "    M42 = torch.zeros_like(M11)\n",
    "    M43 = torch.zeros_like(M11)\n",
    "    M44 = torch.ones_like(M11)\n",
    "\n",
    "    #M11.unsqueeze_(-1)\n",
    "    M11 = torch.unsqueeze(M11, axis=-1)\n",
    "    M12 = torch.unsqueeze(M12, axis=-1)\n",
    "    M13 = torch.unsqueeze(M13, axis=-1)\n",
    "    M14 = torch.unsqueeze(M14, axis=-1)\n",
    "\n",
    "    M21 = torch.unsqueeze(M21, axis=-1)\n",
    "    M22 = torch.unsqueeze(M22, axis=-1)\n",
    "    M23 = torch.unsqueeze(M23, axis=-1)\n",
    "    M24 = torch.unsqueeze(M24, axis=-1)\n",
    "\n",
    "    M31 = torch.unsqueeze(M31, axis=-1)\n",
    "    M32 = torch.unsqueeze(M32, axis=-1)\n",
    "    M33 = torch.unsqueeze(M33, axis=-1)\n",
    "    M34 = torch.unsqueeze(M34, axis=-1)\n",
    "\n",
    "    M41 = torch.unsqueeze(M41, axis=-1)\n",
    "    M42 = torch.unsqueeze(M42, axis=-1)\n",
    "    M43 = torch.unsqueeze(M43, axis=-1)\n",
    "    M44 = torch.unsqueeze(M44, axis=-1)\n",
    "\n",
    "    M_l1 = torch.cat([M11, M12, M13, M14], axis=2)\n",
    "    M_l2 = torch.cat([M21, M22, M23, M24], axis=2)\n",
    "    M_l3 = torch.cat([M31, M32, M33, M34], axis=2)\n",
    "    M_l4 = torch.cat([M41, M42, M43, M44], axis=2)\n",
    "\n",
    "    M = torch.cat([M_l1, M_l2, M_l3, M_l4], axis=1)\n",
    "\n",
    "    return M\n",
    "\n",
    "def matrix2quternion(M):\n",
    "    tx = M[:, 0, 3].unsqueeze(-1)\n",
    "    ty = M[:, 1, 3].unsqueeze(-1)\n",
    "    tz = M[:, 2, 3].unsqueeze(-1)\n",
    "    qw = 0.5 * torch.sqrt(M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] + M[:, 3, 3]).unsqueeze(-1)\n",
    "\n",
    "    mask = torch.abs(qw)<10e-6\n",
    "    qw = qw if mask.sum()==mask.shape[0] else qw+10e-6\n",
    "\n",
    "    qx = torch.unsqueeze(M[:, 2, 1] - M[:, 1, 2],-1) / (4. * qw)\n",
    "    qy = torch.unsqueeze(M[:, 0, 2] - M[:, 2, 0],-1) / (4. * qw)\n",
    "    qz = torch.unsqueeze(M[:, 1, 0] - M[:, 0, 1],-1) / (4. * qw)\n",
    "    q = torch.cat([tx, ty, tz, qx, qy, qz, qw], dim=-1)\n",
    "    return q\n",
    "\n",
    "def get_relative_pose(Q_a,Q_b):\n",
    "    M_a = quanternion2matrix(Q_a)\n",
    "    M_b = quanternion2matrix(Q_b)\n",
    "\n",
    "    try:\n",
    "        M_delta = torch.matmul(M_a.inverse(),M_b)\n",
    "    except ValueError:\n",
    "        print(\"matrix is not invertiable\")\n",
    "        M_delta = torch.eye(4).repeat(M_a.shape[0],1,1)\n",
    "\n",
    "    Q_delta = matrix2quternion(M_delta)\n",
    "\n",
    "    return Q_delta\n",
    "\n",
    "def normalize(target, norm_mean, norm_std):\n",
    "    target_trans = target[:,:3]\n",
    "    target_trans = torch.div(torch.sub(target_trans,norm_mean),norm_std)\n",
    "    target_normed = torch.cat([target_trans,target[:,3:]],dim=1)\n",
    "    return target_normed \n",
    "\n",
    "def translational_rotational_loss(pred=None, gt=None, lamda=None):\n",
    "    trans_pred, rot_pred = torch.split(pred, [3,4], dim=1)\n",
    "    trans_gt, rot_gt = torch.split(gt, [3, 4], dim=1)\n",
    "    \n",
    "    trans_loss = nn.functional.mse_loss(input=trans_pred, target=trans_gt)\n",
    "    rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_gt),dim=1)))\n",
    "    \n",
    "    loss = trans_loss + lamda * rot_loss\n",
    "\n",
    "    return loss#, trans_loss, rot_loss\n",
    "\n",
    "def _quanternion2matrix(q):\n",
    "    tx, ty, tz, qx, qy, qz, qw = torch.split(q,[1, 1, 1, 1, 1, 1, 1], dim=-1)\n",
    "    M11 = 1.0 - 2 * (torch.square(qy) + torch.square(qz))\n",
    "    M12 = 2. * qx * qy - 2. * qw * qz\n",
    "    M13 = 2. * qw * qy + 2. * qx * qz\n",
    "    M14 = tx\n",
    "\n",
    "    M21 = 2. * qx * qy + 2. * qw * qz\n",
    "    M22 = 1. - 2. * (torch.square(qx) + torch.square(qz))\n",
    "    M23 = -2. * qw * qx + 2. * qy * qz\n",
    "    M24 = ty\n",
    "\n",
    "    M31 = -2. * qw * qy + 2. * qx * qz\n",
    "    M32 = 2. * qw * qx + 2. * qy * qz\n",
    "    M33 = 1. - 2. * (torch.square(qx) + torch.square(qy))\n",
    "    M34 = tz\n",
    "\n",
    "    M41 = torch.zeros_like(M11)\n",
    "    M42 = torch.zeros_like(M11)\n",
    "    M43 = torch.zeros_like(M11)\n",
    "    M44 = torch.ones_like(M11)\n",
    "\n",
    "    #M11.unsqueeze_(-1)\n",
    "    M11.unsqueeze_(axis=-1)\n",
    "    M12.unsqueeze_(axis=-1)\n",
    "    M13.unsqueeze_(axis=-1)\n",
    "    M14.unsqueeze_(axis=-1)\n",
    "\n",
    "    M21.unsqueeze_(axis=-1)\n",
    "    M22.unsqueeze_(axis=-1)\n",
    "    M23.unsqueeze_(axis=-1)\n",
    "    M24.unsqueeze_(axis=-1)\n",
    "\n",
    "    M31.unsqueeze_(axis=-1)\n",
    "    M32.unsqueeze_(axis=-1)\n",
    "    M33.unsqueeze_(axis=-1)\n",
    "    M34.unsqueeze_(axis=-1)\n",
    "\n",
    "    M41.unsqueeze_(axis=-1)\n",
    "    M42.unsqueeze_(axis=-1)\n",
    "    M43.unsqueeze_(axis=-1)\n",
    "    M44.unsqueeze_(axis=-1)\n",
    "\n",
    "    M_l1 = torch.cat([M11, M12, M13, M14], axis=2)\n",
    "    M_l2 = torch.cat([M21, M22, M23, M24], axis=2)\n",
    "    M_l3 = torch.cat([M31, M32, M33, M34], axis=2)\n",
    "    M_l4 = torch.cat([M41, M42, M43, M44], axis=2)\n",
    "\n",
    "    M = torch.cat([M_l1, M_l2, M_l3, M_l4], axis=1)\n",
    "\n",
    "    return M\n",
    "\n",
    "def _matrix2quternion(M):\n",
    "    tx = M[:, 0, 3].unsqueeze_(-1)\n",
    "    ty = M[:, 1, 3].unsqueeze_(-1)\n",
    "    tz = M[:, 2, 3].unsqueeze_(-1)\n",
    "    qw = 0.5 * torch.sqrt(M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] + M[:, 3, 3]).unsqueeze_(-1)\n",
    "\n",
    "    mask = torch.abs(qw)<10e-6\n",
    "    qw = qw if mask.sum()==mask.shape[0] else qw+10e-6\n",
    "\n",
    "    qx = torch.unsqueeze(M[:, 2, 1] - M[:, 1, 2],-1) / (4. * qw)\n",
    "    qy = torch.unsqueeze(M[:, 0, 2] - M[:, 2, 0],-1) / (4. * qw)\n",
    "    qz = torch.unsqueeze(M[:, 1, 0] - M[:, 0, 1],-1) / (4. * qw)\n",
    "    q = torch.cat([tx, ty, tz, qx, qy, qz, qw], dim=-1)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:49:13.185146Z",
     "start_time": "2020-06-11T14:49:07.172352Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# set to cpu\n",
    "#device = torch.device(\"cpu\")\n",
    "net = Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:49:13.246005Z",
     "start_time": "2020-06-11T14:49:13.214077Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight torch.Size([64, 1, 7, 7])\n",
      "resnet.bn1.weight torch.Size([64])\n",
      "resnet.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "resnet.layer1.0.bn1.weight torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias torch.Size([64])\n",
      "resnet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias torch.Size([64])\n",
      "resnet.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.bn3.weight torch.Size([256])\n",
      "resnet.layer1.0.bn3.bias torch.Size([256])\n",
      "resnet.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.0.downsample.1.weight torch.Size([256])\n",
      "resnet.layer1.0.downsample.1.bias torch.Size([256])\n",
      "resnet.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.1.bn1.weight torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias torch.Size([64])\n",
      "resnet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias torch.Size([64])\n",
      "resnet.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.1.bn3.weight torch.Size([256])\n",
      "resnet.layer1.1.bn3.bias torch.Size([256])\n",
      "resnet.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "resnet.layer1.2.bn1.weight torch.Size([64])\n",
      "resnet.layer1.2.bn1.bias torch.Size([64])\n",
      "resnet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.2.bn2.weight torch.Size([64])\n",
      "resnet.layer1.2.bn2.bias torch.Size([64])\n",
      "resnet.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "resnet.layer1.2.bn3.weight torch.Size([256])\n",
      "resnet.layer1.2.bn3.bias torch.Size([256])\n",
      "resnet.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "resnet.layer2.0.bn1.weight torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias torch.Size([128])\n",
      "resnet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias torch.Size([128])\n",
      "resnet.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.0.bn3.weight torch.Size([512])\n",
      "resnet.layer2.0.bn3.bias torch.Size([512])\n",
      "resnet.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight torch.Size([512])\n",
      "resnet.layer2.0.downsample.1.bias torch.Size([512])\n",
      "resnet.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.1.bn1.weight torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias torch.Size([128])\n",
      "resnet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias torch.Size([128])\n",
      "resnet.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.1.bn3.weight torch.Size([512])\n",
      "resnet.layer2.1.bn3.bias torch.Size([512])\n",
      "resnet.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.2.bn1.weight torch.Size([128])\n",
      "resnet.layer2.2.bn1.bias torch.Size([128])\n",
      "resnet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.2.bn2.weight torch.Size([128])\n",
      "resnet.layer2.2.bn2.bias torch.Size([128])\n",
      "resnet.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.2.bn3.weight torch.Size([512])\n",
      "resnet.layer2.2.bn3.bias torch.Size([512])\n",
      "resnet.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "resnet.layer2.3.bn1.weight torch.Size([128])\n",
      "resnet.layer2.3.bn1.bias torch.Size([128])\n",
      "resnet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.3.bn2.weight torch.Size([128])\n",
      "resnet.layer2.3.bn2.bias torch.Size([128])\n",
      "resnet.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "resnet.layer2.3.bn3.weight torch.Size([512])\n",
      "resnet.layer2.3.bn3.bias torch.Size([512])\n",
      "resnet.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "resnet.layer3.0.bn1.weight torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias torch.Size([256])\n",
      "resnet.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias torch.Size([256])\n",
      "resnet.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.0.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.0.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "resnet.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "resnet.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.1.bn1.weight torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias torch.Size([256])\n",
      "resnet.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias torch.Size([256])\n",
      "resnet.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.1.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.1.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.2.bn1.weight torch.Size([256])\n",
      "resnet.layer3.2.bn1.bias torch.Size([256])\n",
      "resnet.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.2.bn2.weight torch.Size([256])\n",
      "resnet.layer3.2.bn2.bias torch.Size([256])\n",
      "resnet.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.2.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.2.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.3.bn1.weight torch.Size([256])\n",
      "resnet.layer3.3.bn1.bias torch.Size([256])\n",
      "resnet.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.3.bn2.weight torch.Size([256])\n",
      "resnet.layer3.3.bn2.bias torch.Size([256])\n",
      "resnet.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.3.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.3.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.4.bn1.weight torch.Size([256])\n",
      "resnet.layer3.4.bn1.bias torch.Size([256])\n",
      "resnet.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.4.bn2.weight torch.Size([256])\n",
      "resnet.layer3.4.bn2.bias torch.Size([256])\n",
      "resnet.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.4.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.4.bn3.bias torch.Size([1024])\n",
      "resnet.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "resnet.layer3.5.bn1.weight torch.Size([256])\n",
      "resnet.layer3.5.bn1.bias torch.Size([256])\n",
      "resnet.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.5.bn2.weight torch.Size([256])\n",
      "resnet.layer3.5.bn2.bias torch.Size([256])\n",
      "resnet.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "resnet.layer3.5.bn3.weight torch.Size([1024])\n",
      "resnet.layer3.5.bn3.bias torch.Size([1024])\n",
      "resnet.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "resnet.layer4.0.bn1.weight torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias torch.Size([512])\n",
      "resnet.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias torch.Size([512])\n",
      "resnet.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.0.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.0.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "resnet.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "resnet.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.1.bn1.weight torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias torch.Size([512])\n",
      "resnet.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias torch.Size([512])\n",
      "resnet.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.1.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.1.bn3.bias torch.Size([2048])\n",
      "resnet.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "resnet.layer4.2.bn1.weight torch.Size([512])\n",
      "resnet.layer4.2.bn1.bias torch.Size([512])\n",
      "resnet.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.2.bn2.weight torch.Size([512])\n",
      "resnet.layer4.2.bn2.bias torch.Size([512])\n",
      "resnet.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "resnet.layer4.2.bn3.weight torch.Size([2048])\n",
      "resnet.layer4.2.bn3.bias torch.Size([2048])\n",
      "global_context.context.squeeze.0.weight torch.Size([128, 2048, 1, 1])\n",
      "global_context.context.squeeze.0.bias torch.Size([128])\n",
      "global_context.context.context5_1.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_1.0.bias torch.Size([128])\n",
      "global_context.context.context5_2.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_2.0.bias torch.Size([128])\n",
      "global_context.context.context5_3.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_3.0.bias torch.Size([128])\n",
      "global_context.context.context5_4.0.weight torch.Size([128, 128, 3, 3])\n",
      "global_context.context.context5_4.0.bias torch.Size([128])\n",
      "global_context.context.squeeze2.0.weight torch.Size([64, 128, 1, 1])\n",
      "global_context.context.squeeze2.0.bias torch.Size([64])\n",
      "global_regressor.regressor.fc1_trans.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_trans.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_trans.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_trans.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_trans.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_t.weight torch.Size([3, 128])\n",
      "global_regressor.regressor.logits_t.bias torch.Size([3])\n",
      "global_regressor.regressor.fc1_rot.0.weight torch.Size([4096, 6400])\n",
      "global_regressor.regressor.fc1_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc2_rot.0.weight torch.Size([4096, 4096])\n",
      "global_regressor.regressor.fc2_rot.0.bias torch.Size([4096])\n",
      "global_regressor.regressor.fc3_rot.0.weight torch.Size([128, 4096])\n",
      "global_regressor.regressor.fc3_rot.0.bias torch.Size([128])\n",
      "global_regressor.regressor.logits_r.weight torch.Size([4, 128])\n",
      "global_regressor.regressor.logits_r.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:49:13.297044Z",
     "start_time": "2020-06-11T14:49:13.274725Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "args.norm_mean = args.norm_mean.to(device)\n",
    "args.norm_std = args.norm_std.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "#optimizer = optimizers.FusedAdam(net.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "\n",
    "#net, optimizer = amp.initialize(net, optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:54:38.141867Z",
     "start_time": "2020-06-11T14:49:13.336286Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/800600 (epoch 0), train_loss = 1.206473696231842, time/batch = 0.574, learning rate = 0.000010000\n",
      "20/800600 (epoch 0), train_loss = 1.0977530211210251, time/batch = 0.576, learning rate = 0.000010000\n",
      "30/800600 (epoch 0), train_loss = 1.0649363617102305, time/batch = 0.574, learning rate = 0.000010000\n",
      "40/800600 (epoch 0), train_loss = 1.0680266454815865, time/batch = 0.578, learning rate = 0.000010000\n",
      "50/800600 (epoch 0), train_loss = 1.0786670863628387, time/batch = 0.577, learning rate = 0.000010000\n",
      "60/800600 (epoch 0), train_loss = 1.0869983742634455, time/batch = 0.578, learning rate = 0.000010000\n",
      "70/800600 (epoch 0), train_loss = 1.0868839817387717, time/batch = 0.578, learning rate = 0.000010000\n",
      "80/800600 (epoch 0), train_loss = 1.0829214081168175, time/batch = 0.585, learning rate = 0.000010000\n",
      "90/800600 (epoch 0), train_loss = 1.0786697102917566, time/batch = 0.581, learning rate = 0.000010000\n",
      "100/800600 (epoch 0), train_loss = 1.0791044741868974, time/batch = 0.585, learning rate = 0.000010000\n",
      "110/800600 (epoch 0), train_loss = 1.0763627691702409, time/batch = 0.581, learning rate = 0.000010000\n",
      "120/800600 (epoch 0), train_loss = 1.0790241315960885, time/batch = 0.587, learning rate = 0.000010000\n",
      "130/800600 (epoch 0), train_loss = 1.0743340368454273, time/batch = 0.583, learning rate = 0.000010000\n",
      "140/800600 (epoch 0), train_loss = 1.0734490956578935, time/batch = 0.583, learning rate = 0.000010000\n",
      "150/800600 (epoch 0), train_loss = 1.0672832067807516, time/batch = 0.583, learning rate = 0.000010000\n",
      "160/800600 (epoch 0), train_loss = 1.0638394292443991, time/batch = 0.584, learning rate = 0.000010000\n",
      "170/800600 (epoch 0), train_loss = 1.0581566589720108, time/batch = 0.585, learning rate = 0.000010000\n",
      "180/800600 (epoch 0), train_loss = 1.0442931519614325, time/batch = 0.582, learning rate = 0.000010000\n",
      "190/800600 (epoch 0), train_loss = 1.0363981497915167, time/batch = 0.583, learning rate = 0.000010000\n",
      "200/800600 (epoch 0), train_loss = 1.0269162884354592, time/batch = 0.586, learning rate = 0.000010000\n",
      "210/800600 (epoch 0), train_loss = 1.0198729004178728, time/batch = 0.585, learning rate = 0.000010000\n",
      "220/800600 (epoch 0), train_loss = 1.0087663238698785, time/batch = 0.584, learning rate = 0.000010000\n",
      "230/800600 (epoch 0), train_loss = 0.9954578329687533, time/batch = 0.585, learning rate = 0.000010000\n",
      "240/800600 (epoch 0), train_loss = 0.9818059450636307, time/batch = 0.587, learning rate = 0.000010000\n",
      "250/800600 (epoch 0), train_loss = 0.9710392163991928, time/batch = 0.585, learning rate = 0.000010000\n",
      "260/800600 (epoch 0), train_loss = 0.9618332944237269, time/batch = 0.587, learning rate = 0.000010000\n",
      "270/800600 (epoch 0), train_loss = 0.9512505788494039, time/batch = 0.595, learning rate = 0.000010000\n",
      "280/800600 (epoch 0), train_loss = 0.9408263135169234, time/batch = 0.589, learning rate = 0.000010000\n",
      "290/800600 (epoch 0), train_loss = 0.9288996892756429, time/batch = 0.587, learning rate = 0.000010000\n",
      "300/800600 (epoch 0), train_loss = 0.9191447558005651, time/batch = 0.586, learning rate = 0.000010000\n",
      "310/800600 (epoch 0), train_loss = 0.9079776977339099, time/batch = 0.588, learning rate = 0.000010000\n",
      "320/800600 (epoch 0), train_loss = 0.8980696485377848, time/batch = 0.586, learning rate = 0.000010000\n",
      "330/800600 (epoch 0), train_loss = 0.8869511372212208, time/batch = 0.584, learning rate = 0.000010000\n",
      "340/800600 (epoch 0), train_loss = 0.8766454343410099, time/batch = 0.592, learning rate = 0.000010000\n",
      "350/800600 (epoch 0), train_loss = 0.8675448806796755, time/batch = 0.586, learning rate = 0.000010000\n",
      "360/800600 (epoch 0), train_loss = 0.8577398445871142, time/batch = 0.586, learning rate = 0.000010000\n",
      "370/800600 (epoch 0), train_loss = 0.8488642218950633, time/batch = 0.587, learning rate = 0.000010000\n",
      "380/800600 (epoch 0), train_loss = 0.8403116230117648, time/batch = 0.586, learning rate = 0.000010000\n",
      "390/800600 (epoch 0), train_loss = 0.8325293835921165, time/batch = 0.588, learning rate = 0.000010000\n",
      "400/800600 (epoch 0), train_loss = 0.825068189278245, time/batch = 0.586, learning rate = 0.000010000\n",
      "410/800600 (epoch 0), train_loss = 0.8199093106316357, time/batch = 0.590, learning rate = 0.000010000\n",
      "420/800600 (epoch 0), train_loss = 0.8115938496021997, time/batch = 0.588, learning rate = 0.000010000\n",
      "430/800600 (epoch 0), train_loss = 0.8031397202680277, time/batch = 0.590, learning rate = 0.000010000\n",
      "440/800600 (epoch 0), train_loss = 0.7959944562478499, time/batch = 0.587, learning rate = 0.000010000\n",
      "450/800600 (epoch 0), train_loss = 0.7877181991603639, time/batch = 0.588, learning rate = 0.000010000\n",
      "460/800600 (epoch 0), train_loss = 0.780959410706292, time/batch = 0.587, learning rate = 0.000010000\n",
      "470/800600 (epoch 0), train_loss = 0.7745304747464808, time/batch = 0.588, learning rate = 0.000010000\n",
      "480/800600 (epoch 0), train_loss = 0.7694329347461462, time/batch = 0.587, learning rate = 0.000010000\n",
      "490/800600 (epoch 0), train_loss = 0.7630762787497773, time/batch = 0.587, learning rate = 0.000010000\n",
      "500/800600 (epoch 0), train_loss = 0.7567890894412994, time/batch = 0.587, learning rate = 0.000010000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4bd207bc7985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Part 2: Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mrelative_consistence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relative_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_output0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_output1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         global_loss = translational_rotational_loss(pred=global_output1, \\\n",
      "\u001b[0;32m<ipython-input-7-8df2a83ad699>\u001b[0m in \u001b[0;36mget_relative_pose\u001b[0;34m(Q_a, Q_b)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mM_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix is not invertiable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(args.num_epochs):\n",
    "#for e in range(2):\n",
    "    if e != 0:\n",
    "        scheduler.step()\n",
    "    train_loss = 0.\n",
    "    for b, data in enumerate(dataloader, 0):\n",
    "\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x0, x1 = data['image']\n",
    "        y0, y1 = data['target']\n",
    "        x0,x1,y0,y1 = x0.to(device),x1.to(device),y0.to(device),y1.to(device)\n",
    "        # normalize targets\n",
    "        y0_norm, y1_norm = [normalize(y,args.norm_mean, args.norm_std) for y in [y0,y1]]\n",
    "        relative_target_normed = get_relative_pose(y0_norm, y1_norm)\n",
    "        \n",
    "        # Part 1: Net Forward\n",
    "        global_output0,global_output1 = net(x0, x1)\n",
    "\n",
    "        # Part 2: Loss\n",
    "        \n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "\n",
    "        global_loss = translational_rotational_loss(pred=global_output1, \\\n",
    "                                                    gt=y1_norm, \\\n",
    "                                                    lamda=args.lamda_weights)\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed, \\\n",
    "                                                                 lamda=args.lamda_weights)\n",
    "        total_loss = global_loss + 0.1 * geometry_consistent_loss\n",
    "        \n",
    "        # Part 3: Net Backward\n",
    "        #with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #del global_output0,global_output1,relative_consistence,global_loss,geometry_consistent_loss\n",
    "        end = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss += float(total_loss)\n",
    "            if ((b+1)%args.display == 0):\n",
    "                 print(\n",
    "                    \"{}/{} (epoch {}), train_loss = {}, time/batch = {:.3f}, learning rate = {:.9f}\"\n",
    "                    .format(\n",
    "                    e * len(dataloader) + (b+1),\n",
    "                    args.num_epochs * len(dataloader),\n",
    "                    e,\n",
    "                    train_loss/(b+1),\n",
    "                    end - start,\n",
    "                    optimizer.param_groups[0]['lr']))\n",
    "            if (e * len(dataloader) + (b+1)) % args.save_every == 0:\n",
    "                checkpoint_path = os.path.join(args.model_dir, 'model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))\n",
    "                torch.save(net.state_dict(),checkpoint_path)\n",
    "                print('saving model to model-{}-{}.pth'.format(e, e * len(dataloader) + (b+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
