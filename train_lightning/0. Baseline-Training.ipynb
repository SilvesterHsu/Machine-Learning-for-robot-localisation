{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and Basic setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:00:17.084041Z",
     "start_time": "2020-10-24T04:00:15.911317Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# default setting\n",
    "np.set_printoptions(precision=2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:00:17.094910Z",
     "start_time": "2020-10-24T04:00:17.086031Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=130,\n",
    "                    help='size of mini batch')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0002, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--lamda_weights', type=float, default=.01, help='lamda weight')\n",
    "parser.add_argument('--is_normalization', type=bool,\n",
    "                    default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int,\n",
    "                    help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--model_dir', type=str,\n",
    "                    default='/notebooks/global_localization/gp_net_torch', help='rnn, gru, or lstm')\n",
    "parser.add_argument('--test_dataset', type=str, default=['/notebooks/michigan_nn_data/2012_02_12',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_04_29',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_05_11',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_06_15',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_08_04',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_10_28',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_11_16',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_12_01'])\n",
    "parser.add_argument('--train_dataset', type=str, default=['/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                          '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "parser.add_argument('--norm_tensor', type=str,\n",
    "                    default=['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:03:25.674313Z",
     "start_time": "2020-10-24T04:00:17.098894Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n",
      "100%|██████████| 16446/16446 [00:22<00:00, 715.16it/s]\n",
      "100%|██████████| 22584/22584 [00:31<00:00, 709.67it/s]\n",
      "100%|██████████| 18655/18655 [00:26<00:00, 716.58it/s]\n",
      "100%|██████████| 17310/17310 [00:24<00:00, 715.67it/s]\n",
      "100%|██████████| 10766/10766 [00:14<00:00, 720.02it/s]\n",
      "100%|██████████| 14878/14878 [00:22<00:00, 660.79it/s]\n",
      "100%|██████████| 13452/13452 [00:20<00:00, 662.76it/s]\n",
      "100%|██████████| 14037/14037 [00:24<00:00, 569.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import LocalizationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.train_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform, get_pair = False, sampling_rate=2)\n",
    "num_data = len(dataset)\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [round(num_data*0.7), round(num_data*0.3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T13:39:34.876300Z",
     "start_time": "2020-10-24T04:03:25.693654Z"
    },
    "code_folding": [
     9,
     30,
     32,
     61,
     68,
     82,
     100,
     124,
     130,
     135,
     149,
     154,
     158,
     164,
     169
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from torch.cuda.amp import autocast, GradScaler\n",
    "import gpytorch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.GPs import Backbone, NN\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize_navie\n",
    "\n",
    "\n",
    "# Gaussian Process Model\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([1]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([1])),\n",
    "            batch_shape=torch.Size([1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Baseline(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        inducing_points = torch.zeros(3, 300, 128)\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks=3)\n",
    "        [norm_mean, norm_std] = torch.load(*args.norm_tensor)\n",
    "        self.norm_mean = torch.nn.parameter.Parameter(norm_mean,requires_grad=False)\n",
    "        self.norm_std = torch.nn.parameter.Parameter(norm_std,requires_grad=False) \n",
    "        \n",
    "        '''\n",
    "        # load pre-trained model\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, 'pretrained.pth'),map_location=self.device)\n",
    "        for key in list(state_dict):\n",
    "            if 'net.resnet.' in key:\n",
    "                state_dict[key.replace('net.resnet.','backbone.resnet.')] = state_dict.pop(key)\n",
    "            if 'net.global_regressor.' in key:\n",
    "                state_dict[key.replace('net.global_regressor.','nn.global_regressor.')] = state_dict.pop(key)\n",
    "            elif 'net.global_context.' in key:\n",
    "                state_dict[key.replace('net.global_context.','nn.global_context.')] = state_dict.pop(key)\n",
    "        self.load_state_dict(state_dict,strict = False)\n",
    "        '''\n",
    "        \n",
    "        # shut down backbone learning\n",
    "        self.__disable_grad(self.backbone)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_feat = self.backbone(x)\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        _, rot_pred = torch.split(output, [3, 4], dim=1)\n",
    "        trans_pred = self.gp(feature_t)\n",
    "        return trans_pred, rot_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch.values()\n",
    "        y = normalize(y, self.norm_mean, self.norm_std)\n",
    "        train_loss,trans_loss,rot_loss = self.__loss(x, y)\n",
    "        #self.log('train_loss', train_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        tensorboard = self.logger.experiment\n",
    "        #tensorboard.add_scalar('train_loss',float(train_loss),self.global_step)\n",
    "        tensorboard.add_scalars('train_loss',\n",
    "                                {'total_loss':float(train_loss),\n",
    "                                'trans_loss':float(trans_loss),\n",
    "                                'rot_loss':float(rot_loss)},\n",
    "                                self.global_step)\n",
    "        return train_loss\n",
    "\n",
    "    def __loss(self, x, y):\n",
    "        # target\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        # predict\n",
    "        trans_pred, rot_pred = self.forward(x)\n",
    "\n",
    "        # trans loss\n",
    "        mll = gpytorch.mlls.PredictiveLogLikelihood(self.likelihood, self.gp, num_data=num_data)\n",
    "        trans_loss = -1.*mll(trans_pred, trans_target)\n",
    "        # rot loss\n",
    "        rot_loss = 1. - \\\n",
    "            torch.mean(torch.square(\n",
    "                torch.sum(torch.mul(rot_pred, rot_target), dim=1)))\n",
    "\n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss\n",
    "\n",
    "        return total_loss, trans_loss, rot_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch.values()\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        trans_pred, rot_pred = self.forward(x)\n",
    "        \n",
    "        trans_pred, trans_mean, trans_var = self._eval_gp(trans_pred)\n",
    "        trans_pred = denormalize_navie(trans_pred, self.norm_mean, self.norm_std)\n",
    "        trans_mean = denormalize_navie(trans_mean, self.norm_mean, self.norm_std)\n",
    "        trans_var = trans_var.mul(self.norm_std)\n",
    "        samples = self._sample(trans_mean, trans_var, 100)\n",
    "        \n",
    "        trans_loss = torch.sqrt(torch.sum((trans_pred - trans_target)**2,dim=1)).mean()\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        #return trans_pred, rot_pred, trans_target, rot_target, samples\n",
    "        \n",
    "        val_loss = trans_loss\n",
    "        self.log('val_loss', val_loss, on_step=True, on_epoch=True, prog_bar=False, logger=False)\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_scalars('val_loss',\n",
    "                                {'trans_loss':float(trans_loss),\n",
    "                                'rot_loss':float(rot_loss)},\n",
    "                                self.current_epoch*self.trainer.num_val_batches[0]+batch_idx)\n",
    "        return val_loss\n",
    "    \n",
    "    def _eval_gp(self, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = self.likelihood(trans_pred).mean, self.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = torch.distributions.Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr,weight_decay = args.learning_rate,args.weight_decay\n",
    "        optimizer_args = [\n",
    "            {'params': self.gp.parameters(), 'lr': lr, 'weight_decay': weight_decay},\n",
    "            {'params': self.likelihood.parameters(), 'lr': lr,\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': self.nn.global_regressor.parameters(), 'lr': lr * 0.01,\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': self.nn.global_context.parameters(), 'lr': lr * 0.001, 'weight_decay': weight_decay}]\n",
    "        \n",
    "        optimizer = torch.optim.Adam(optimizer_args)\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    def show_require_grad(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print (name, param.shape)\n",
    "                \n",
    "    def __disable_grad(self,model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def get_progress_bar_dict(self):\n",
    "        tqdm_dict = super().get_progress_bar_dict()\n",
    "        if 'v_num' in tqdm_dict:\n",
    "            del tqdm_dict['v_num']\n",
    "        return tqdm_dict\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                  shuffle=True, num_workers=os.cpu_count(),drop_last=True)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                shuffle=False, num_workers=os.cpu_count(),drop_last=True)\n",
    "        return val_loader\n",
    "    \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.system('rm -rf lightning_logs')\n",
    "logger = TensorBoardLogger('lightning_logs')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filepath='model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2,\n",
    "    mode='min',\n",
    "    save_weights_only = True)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,precision=32,\n",
    "                     limit_train_batches=0.8,\n",
    "                     limit_val_batches=0.2,\n",
    "                     accumulate_grad_batches=1,\n",
    "                     reload_dataloaders_every_epoch = True,\n",
    "                     logger=logger,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "model = Baseline.load_from_checkpoint('pretrained-model-epoch=98-val_loss=0.32.ckpt')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
