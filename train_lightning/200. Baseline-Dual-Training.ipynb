{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and Basic setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:15:23.334293Z",
     "start_time": "2020-11-28T10:15:21.845297Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# default setting\n",
    "np.set_printoptions(precision=2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:15:23.340825Z",
     "start_time": "2020-11-28T10:15:23.335927Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=20, help='size of mini batch')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=.0001, help='decay rate for rmsprop')\n",
    "parser.add_argument('--lamda_weights', type=float, default=.01, help='lamda weight')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/lightning/baseline-dual')\n",
    "parser.add_argument('--train_dataset', type=str, default='/dataset/train.lmdb')\n",
    "parser.add_argument('--norm_tensor', type=str, default=['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:15:25.523674Z",
     "start_time": "2020-11-28T10:15:24.877072Z"
    },
    "code_folding": [
     9,
     15,
     16,
     33,
     39,
     88
    ]
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "import tf.transformations as tf_tran\n",
    "\n",
    "class Michigan_Image:\n",
    "    def __init__(self, img, pose, sparse=False):\n",
    "        img = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n",
    "        self.img = sp.coo_matrix(img) if sparse else img\n",
    "        self.pose = np.loadtxt(pose,dtype=np.float32)\n",
    "\n",
    "class MichiganDatasetLMDB(Dataset):\n",
    "    def __init__(self, db_path, transform=None, offset=False, \n",
    "                 target_image_size=[300, 300],\n",
    "                 pair=False, num_connected_frames=10):\n",
    "        self.db_path = db_path\n",
    "        self.env = lmdb.open(db_path,readonly=True, lock=False,\n",
    "                             readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = txn.stat()['entries']\n",
    "            self.keys= [str(i).encode('ascii') for i in range(self.length)]\n",
    "            assert self.length >= 1\n",
    "\n",
    "        self.transform = transform\n",
    "        self.offset = offset\n",
    "        self.target_image_size=target_image_size\n",
    "        self.pair = pair\n",
    "        self.num_connected_frames = num_connected_frames\n",
    "        \n",
    "    def _quaternized_pose(self, pose):\n",
    "        [px, py, pz, ex, ey, ez] = pose\n",
    "        [qx, qy, qz, qw] = tf_tran.quaternion_from_euler(ex, ey, ez, 'rxyz')\n",
    "        pose = torch.Tensor([px, py, pz, qx, qy, qz, qw])\n",
    "        return pose\n",
    "        \n",
    "    def _image_argumentation(self, img, target, rot_angle=None):\n",
    "        RES = 1.0\n",
    "\n",
    "        margin_row = img.shape[0] - self.target_image_size[0]\n",
    "        margin_col = img.shape[1] - self.target_image_size[1]\n",
    "\n",
    "        [px, py, pz, qx, qy, qz, qw] = target\n",
    "\n",
    "        if self.offset:\n",
    "            #np.random.seed(0) # fixed\n",
    "            offset_row = int(max(0.0, min(1.0, np.random.normal(0.5, 0.1))) * margin_row)\n",
    "            offset_col = int(max(0.0, min(1.0, np.random.normal(0.5, 0.1))) * margin_col)\n",
    "            \n",
    "            offset_x = (offset_row - int(margin_row / 2)) * RES\n",
    "            offset_y = -(offset_col - int(margin_col / 2)) * RES\n",
    "\n",
    "            if rot_angle is None:\n",
    "                # not rotate\n",
    "                deltaT = tf_tran.identity_matrix()\n",
    "            else:\n",
    "                # rotate image 180 with noise\n",
    "                angle = rot_angle\n",
    "\n",
    "                deltaT = tf_tran.rotation_matrix(angle, (0,0,1))\n",
    "                img = imutils.rotate(img[:, :, 0], angle/math.pi*180)\n",
    "                img = np.array(img)[:, :, np.newaxis]\n",
    "\n",
    "            deltaT[0:3, 3] = [offset_x, offset_y, 0.]\n",
    "            \n",
    "            '''gt'''\n",
    "            T = tf_tran.quaternion_matrix([qx, qy, qz, qw])\n",
    "            T[0:3, 3] = [px, py, pz]\n",
    "\n",
    "            # apply on global pose\n",
    "            T = np.matmul(deltaT, T)\n",
    "\n",
    "            position = np.array(tf_tran.translation_from_matrix(T), dtype=np.single)\n",
    "            quaternion = np.array(tf_tran.quaternion_from_matrix(T), dtype=np.single)\n",
    "\n",
    "            target = np.concatenate((position, quaternion), axis=0)\n",
    "        else:\n",
    "            offset_row = int(margin_row / 2)\n",
    "            offset_col = int(margin_col / 2)\n",
    "\n",
    "        img = img[offset_row:offset_row + self.target_image_size[0], \n",
    "                  offset_col:offset_col + self.target_image_size[1], :]\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def _request_data(self, index):\n",
    "        env = self.env\n",
    "        with env.begin(write=False, buffers=True) as txn:\n",
    "            byteflow = txn.get(self.keys[index])\n",
    "            data = pickle.loads(byteflow)\n",
    "        img, pose = data.img, data.pose\n",
    "        img = img[:, :, np.newaxis]\n",
    "        pose = self._quaternized_pose(pose)\n",
    "        #img, pose = self._image_argumentation(img, pose)\n",
    "        \n",
    "        return img, pose   \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.pair:\n",
    "            length = self.length - self.num_connected_frames\n",
    "        else:\n",
    "            length = self.length\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not self.pair:\n",
    "            img, pose = self._request_data(index)\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)   \n",
    "            return img, pose\n",
    "        else:\n",
    "            paired_frame_offset = np.random.randint(1, self.num_connected_frames)\n",
    "            index_offset = index + paired_frame_offset\n",
    "            img_1, pose_1 = self._request_data(index)\n",
    "            img_2, pose_2 = self._request_data(index_offset)\n",
    "            \n",
    "            # check continuity\n",
    "            # if difference is high (not continue), change to another image\n",
    "            while torch.abs(pose_1-pose_2)[:3].max().item() > 15:\n",
    "                #print('not continue.')\n",
    "                index_offset = index - paired_frame_offset #invert index direction\n",
    "                img_2, pose_2 = self._request_data(index_offset)\n",
    "                paired_frame_offset = np.random.randint(1, self.num_connected_frames)\n",
    "            \n",
    "            if self.transform:\n",
    "                img_1 = self.transform(img_1)\n",
    "                img_2 = self.transform(img_2)\n",
    "            return [img_1,img_2],[pose_1,pose_2]\n",
    "            \n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize(300),\n",
    "                                transforms.RandomRotation(180),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = MichiganDatasetLMDB(args.train_dataset, transform, pair=True, num_connected_frames=40)\n",
    "  \n",
    "num_data = len(dataset)\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [round(num_data*0.7), round(num_data*0.3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:15:26.366347Z",
     "start_time": "2020-11-28T10:15:26.348916Z"
    },
    "code_folding": [
     3,
     40,
     101
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "class Context(nn.Module):\n",
    "    def __init__(self, input_channel=2048):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channel,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.context5_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.context5_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.context5_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.context5_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.squeeze2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=64,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.context5_1(x) #context5_1 \n",
    "        x = self.context5_2(x) #context5_2\n",
    "        x = self.context5_3(x) #context5_3\n",
    "        x = self.context5_4(x) #context5_4\n",
    "        x = self.squeeze2(x)\n",
    "        return x\n",
    "    \n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_features=6400):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Part 1: trans\n",
    "        self.fc1_trans = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features,out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc2_trans = nn.Sequential(\n",
    "            nn.Linear(in_features=4096,out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc3_trans = nn.Sequential(\n",
    "            nn.Linear(in_features=4096,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2)\n",
    "        )\n",
    "        self.logits_t = nn.Linear(in_features=128,out_features=3)\n",
    "        \n",
    "        # Part 2: rot\n",
    "        '''\n",
    "        self.fc1_rot = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features,out_features=4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2_rot = nn.Sequential(\n",
    "            nn.Linear(in_features=4096,out_features=4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3_rot = nn.Sequential(\n",
    "            nn.Linear(in_features=4096,out_features=128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.logits_r = nn.Linear(in_features=128,out_features=4)\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # Part 1: trans\n",
    "        net_t = self.fc1_trans(x)\n",
    "        net_t = self.fc2_trans(net_t)\n",
    "        feature_t = self.fc3_trans(net_t)        \n",
    "        # Part 2: rot\n",
    "        '''\n",
    "        net_r = self.fc1_rot(x)\n",
    "        net_r = self.fc2_rot(net_r)\n",
    "        feature_r = self.fc3_rot(net_r)\n",
    "        '''\n",
    "        # Part 3: FC layer\n",
    "        logits_t = self.logits_t(feature_t)\n",
    "        '''\n",
    "        logits_r = self.logits_r(feature_r)\n",
    "        \n",
    "        logits_r = nn.functional.normalize(logits_r, p=2, dim=1)\n",
    "        \n",
    "        logits = torch.cat([logits_t, logits_r],dim=1)\n",
    "        '''\n",
    "        return logits_t, feature_t\n",
    "    \n",
    "class vggnet(nn.Module):\n",
    "    def __init__(self, opt=\"context\", input_channel = 2048):\n",
    "        super().__init__()\n",
    "        self.opt = opt\n",
    "        if opt == \"context\":\n",
    "            self.context = Context(input_channel)\n",
    "        elif opt == \"regressor\":\n",
    "            self.regressor = Regressor()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.opt == \"context\":\n",
    "            return self.context(x)\n",
    "        elif self.opt == \"regressor\":\n",
    "            return self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:15:32.112299Z",
     "start_time": "2020-11-28T10:15:31.059103Z"
    },
    "code_folding": [
     5,
     9,
     18,
     26,
     37,
     38,
     47,
     52,
     70,
     93,
     110,
     121,
     126,
     132
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Checkpoint directory /notebooks/self_localization/train_lightning exists and is not empty. With save_top_k=2, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib import resnet\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize, denormalize_navie, get_relative_pose\n",
    "\n",
    "def translational_rotational_loss(pred=None, gt=None, loss_type='mse'):\n",
    "    trans_pred = pred[:,:3]\n",
    "    trans_gt = gt[:,:3]\n",
    "    \n",
    "    if loss_type=='mse':\n",
    "        trans_loss = nn.functional.mse_loss(input=trans_pred, target=trans_gt)\n",
    "    else:\n",
    "        trans_loss = torch.sum((trans_pred - trans_gt)**2,dim=1).mean()\n",
    "    \n",
    "    loss = trans_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self,pretrained=True):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=pretrained)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output, feature = self.global_regressor(context_feat)\n",
    "        return output, feature\n",
    "    \n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        \n",
    "        [norm_mean, norm_std] = torch.load(*args.norm_tensor)\n",
    "        self.norm_mean = torch.nn.parameter.Parameter(norm_mean,requires_grad=False)\n",
    "        self.norm_std = torch.nn.parameter.Parameter(norm_std,requires_grad=False) \n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_feat = self.backbone(x)\n",
    "        output, _ = self.nn(dense_feat)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        [img1,img2],[pose1,pose2] = batch\n",
    "        \n",
    "        pose1 = normalize(pose1, self.norm_mean, self.norm_std)\n",
    "        pose2 = normalize(pose2, self.norm_mean, self.norm_std)\n",
    "        \n",
    "        global_loss, consistent_loss = self._loss(img1, img2, pose1, pose2)\n",
    "        loss = global_loss + consistent_loss\n",
    "        \n",
    "        tensorboard = self.logger.experiment\n",
    "        #tensorboard.add_scalar('train_loss',float(train_loss),self.global_step)\n",
    "        tensorboard.add_scalars('train_loss',\n",
    "                                {'total_loss':float(loss),\n",
    "                                'global_loss':float(global_loss),\n",
    "                                'consistent_loss':float(consistent_loss)},\n",
    "                                self.global_step)\n",
    "        return loss\n",
    "        \n",
    "    def _loss(self,x0, x1, y0, y1):\n",
    "        \n",
    "        y0[:,3:] = 0\n",
    "        y1[:,3:] = 0\n",
    "        \n",
    "        # target relative\n",
    "        relative_target_normed = get_relative_pose(y0, y1)\n",
    "        # forward output\n",
    "        global_output0 = self.forward(x0)\n",
    "        global_output1 = self.forward(x1)\n",
    "        # output relative\n",
    "        global_output0 = torch.cat([global_output0,torch.zeros_like(y0[:,3:])], dim=1)\n",
    "        global_output1 = torch.cat([global_output1,torch.zeros_like(y1[:,3:])], dim=1)\n",
    "        relative_consistence = get_relative_pose(global_output0,global_output1)\n",
    "        \n",
    "        # target loss\n",
    "        global_loss = translational_rotational_loss(pred=global_output0, gt=y0)\n",
    "        # relative loss\n",
    "        geometry_consistent_loss = translational_rotational_loss(pred=relative_consistence, \\\n",
    "                                                                 gt=relative_target_normed)        \n",
    "        \n",
    "        return global_loss, geometry_consistent_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        [x,_],[y,_] = batch\n",
    "        \n",
    "        trans_target = y[:,:3]\n",
    "        trans_pred = self.forward(x)\n",
    "        trans_pred = denormalize_navie(trans_pred, self.norm_mean, self.norm_std)\n",
    "        \n",
    "        trans_loss = torch.sqrt(torch.sum((trans_pred - trans_target)**2,dim=1)).mean()\n",
    "        \n",
    "        val_loss = trans_loss\n",
    "        self.log('val_loss', val_loss, on_step=True, on_epoch=True, prog_bar=False, logger=False)\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_scalars('val_loss',\n",
    "                                {'trans_loss':float(trans_loss)},\n",
    "                                self.current_epoch*self.trainer.num_val_batches[0]+batch_idx)\n",
    "        return val_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr,weight_decay = args.learning_rate,args.weight_decay\n",
    "        optimizer_args = [\n",
    "            {'params': self.backbone.parameters(), 'lr': lr, 'weight_decay': weight_decay},\n",
    "            {'params': self.nn.parameters(), 'lr': lr, 'weight_decay': weight_decay}]\n",
    "        \n",
    "        optimizer = torch.optim.Adam(optimizer_args)\n",
    "        #optimizer = RAdam(optimizer_args)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def show_require_grad(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print (name, param.shape)\n",
    "    \n",
    "    def get_progress_bar_dict(self):\n",
    "        tqdm_dict = super().get_progress_bar_dict()\n",
    "        if 'v_num' in tqdm_dict:\n",
    "            del tqdm_dict['v_num']\n",
    "        return tqdm_dict\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True,\n",
    "                                  shuffle=True, num_workers=os.cpu_count(), drop_last=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, pin_memory=True,\n",
    "                                shuffle=False, num_workers=os.cpu_count(), drop_last=True)\n",
    "        return val_loader\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.system('rm -rf lightning_logs')\n",
    "logger = TensorBoardLogger('lightning_logs')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filepath='model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2,\n",
    "    mode='min',\n",
    "    save_weights_only = True)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,precision=16,\n",
    "                     limit_train_batches=1.0,\n",
    "                     limit_val_batches=1.0,\n",
    "                     accumulate_grad_batches=1,\n",
    "                     reload_dataloaders_every_epoch = True,\n",
    "                     logger=logger,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "model = Model.load_from_checkpoint(os.path.join(args.model_dir,'model-epoch=20-val_loss=2.14.ckpt'))\n",
    "#model.show_require_grad()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
