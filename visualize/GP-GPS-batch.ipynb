{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:50:05.750814Z",
     "start_time": "2020-07-28T02:50:04.143454Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import roslib\n",
    "import rospy\n",
    "import tf as tf_ros\n",
    "from nav_msgs.msg import Odometry, Path\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "from geometry_msgs.msg import PoseStamped, PoseArray, Pose\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:50:06.164688Z",
     "start_time": "2020-07-28T02:50:05.752693Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "list_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:50:06.169919Z",
     "start_time": "2020-07-28T02:50:06.166321Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "set_device(1)\n",
    "import numpy as np\n",
    "np.set_printoptions(precision = 2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=2)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:50:06.182095Z",
     "start_time": "2020-07-28T02:50:06.171231Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=300, help='size of mini batch')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "parser.add_argument('--output_dim', default=3, type=int, help='output dimention.')\n",
    "parser.add_argument('--feat_dim', default=128, type=int, help='feature dimention.')\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/gp_gps_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--test_dataset', type=str, default=[# '/notebooks/michigan_nn_data/2012_01_08',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_15',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_01_22',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_02',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_02_05',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_02_12',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_03_31',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_04_29',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_05_11',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_06_15',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_08_04',\n",
    "                                                         # '/notebooks/michigan_nn_data/2012_09_28'])\n",
    "                                                         '/notebooks/michigan_nn_data/2012_10_28',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_11_16',\n",
    "                                                         '/notebooks/michigan_nn_data/2012_12_01'\n",
    "                                                        ] )\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "#parser.add_argument('--map_dataset', type=str, default='/home/kevin/data/michigan_gt/training')\n",
    "parser.add_argument('--seed', default=1337, type=int)\n",
    "parser.add_argument('--enable_ros', type=bool, default=False, help='put data into ros')\n",
    "parser.add_argument('--num_gp', type=int, default=15, help='number of local gps')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.enable_ros:\n",
    "    rospy.init_node('global_localization_tf_broadcaster_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:09.435030Z",
     "start_time": "2020-07-28T02:50:09.364143Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#\n",
    "import gpytorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet, cnn_auxiliary\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize, denormalize_navie, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = LocalizationDataset(dataset_dirs = args.test_dataset, \\\n",
    "                              image_size = args.target_image_size, \\\n",
    "                              transform = transform, get_pair = False, mode='evaluate')\n",
    "\n",
    "if len(args.train_dataset)>7:\n",
    "    [args.norm_mean, args.norm_std] = [torch.tensor(x) for x in dataset.get_norm()]\n",
    "    torch.save([args.norm_mean, args.norm_std], *args.norm_tensor)\n",
    "    print('Save norm and std:',*args.norm_tensor)\n",
    "else:\n",
    "    [args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "    print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "    \n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:11.075603Z",
     "start_time": "2020-07-28T02:52:09.459313Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "mbk_means_cluster_centers = np.load(os.path.join(args.model_dir,'k-means_gp15.npy'))\n",
    "gps_mean_std = np.load(os.path.join(args.model_dir,'mean_std_gp15.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T03:13:40.911470Z",
     "start_time": "2020-07-28T03:13:40.647790Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "\n",
    "X = np.array(dataset.Targets)[:,:2]\n",
    "X = np.vstack([X,X+np.random.rand(91940, 2)*10])\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T18:23:29.347561Z",
     "start_time": "2020-07-28T18:23:28.380601Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "r = lambda: np.random.randint(0,255)\n",
    "colors = ['#%02X%02X%02X' % (r(),r(),r()) for _ in range(args.num_gp)]\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "#colors = ['#4EACC5', '#FF9C34', '#4E9A06']\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for k, col in zip(range(args.num_gp), colors):\n",
    "    my_members = mbk_means_labels == k\n",
    "    cluster_center = mbk_means_cluster_centers[k]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "            markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', \n",
    "            markerfacecolor=col, markeredgecolor='k', markersize=6)\n",
    "ax.set_title('MiniBatchKMeans')\n",
    "plt.savefig('gps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:16.082845Z",
     "start_time": "2020-07-28T02:52:11.077560Z"
    },
    "code_folding": [
     0,
     8,
     19,
     40,
     56,
     57,
     78,
     94,
     104,
     109,
     134,
     155,
     170,
     194,
     199
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([1]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([1])),\n",
    "            batch_shape=torch.Size([1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        inducing_points = torch.zeros(3, args.batch_size, 128)\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        dense_feat = self.backbone(input_data)\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention \n",
    "        trans_pred = self.gp(feature_t)\n",
    "        return trans_pred, rot_pred\n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self,regressor_context_rate = [0.0,0.0]):\n",
    "        # data\n",
    "        self.model = Model().cuda()\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # disable learning backbone\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        '''\n",
    "        # training tool\n",
    "        self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                         lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        '''\n",
    "        # or use `gpytorch.mlls.VariationalELBO`\n",
    "        num_data = min(len(dataloader)*args.batch_size,len(dataset))\n",
    "        self.mll = gpytorch.mlls.PredictiveLogLikelihood(self.model.likelihood, \\\n",
    "                                                         self.model.gp, num_data = num_data)\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth'):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, file_name))\n",
    "        if 'net.resnet.conv1.weight' in state_dict:\n",
    "            print('Transform from old model.')\n",
    "            state_dict = self._from_old_model(state_dict)\n",
    "        print('Parameters layer:',len(state_dict.keys()))\n",
    "        # load file to model\n",
    "        self.model.load_state_dict(state_dict,strict = True)\n",
    "        # Display model structure\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Parameters layer:',len(self.model.state_dict().keys()))\n",
    "        # check load\n",
    "        assert len(state_dict.keys()) == len(self.model.state_dict().keys())\n",
    "    \n",
    "    def _from_old_model(self, state_dict):\n",
    "        for key in list(state_dict):\n",
    "            if 'net.resnet.' in key:\n",
    "                state_dict[key.replace('net.resnet.','backbone.resnet.')] = state_dict.pop(key)\n",
    "            elif 'net.global_regressor.' in key:\n",
    "                state_dict[key.replace('net.global_regressor.','nn.global_regressor.')] = state_dict.pop(key)\n",
    "            elif 'net.global_context.' in key:\n",
    "                state_dict[key.replace('net.global_context.','nn.global_context.')] = state_dict.pop(key)\n",
    "        return state_dict\n",
    "         \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "  \n",
    "    def _optimize(self,regressor_context_rate = [0.0,0.0]):\n",
    "        optimizer = [\n",
    "                {'params': self.model.gp.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "                {'params': self.model.likelihood.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay}]\n",
    "            \n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            for param in self.model.nn.global_regressor.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "        else:\n",
    "            for param in self.model.nn.global_context.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    def train(self,x, y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        single_loss = self._loss(x, y)\n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        #Step 3: update\n",
    "        single_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "            \n",
    "    def _loss(self,x, y):\n",
    "        # target\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        # predict\n",
    "        trans_pred, rot_pred = self.model(x)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*self.mll(trans_pred, trans_target)\n",
    "        # rot loss\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        \n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss      \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,num_sample = 100,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        trans_prediction, rot_prediction = self.model(x)\n",
    "        trans_prediction, trans_mean, trans_var = self._eval_gp(trans_prediction)\n",
    "        \n",
    "        if args.is_normalization and output_denormalize:\n",
    "            trans_prediction = denormalize_navie(trans_prediction, self.norm_mean, self.norm_std)\n",
    "            trans_mean = denormalize_navie(trans_mean, self.norm_mean, self.norm_std)\n",
    "            trans_var = trans_var.mul(self.norm_std)\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_mean, trans_var, num_sample)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target, samples\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "    \n",
    "    def _eval_gp(self, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = self.model.likelihood(trans_pred).mean, self.model.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var\n",
    "    \n",
    "global_trainer = Trainer()\n",
    "global_trainer.load_model('/notebooks/global_localization/gp_net_torch/pretrained.pth')\n",
    "global_trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:16.090186Z",
     "start_time": "2020-07-28T02:52:16.085459Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "for param in global_trainer.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:16.109691Z",
     "start_time": "2020-07-28T02:52:16.091694Z"
    },
    "code_folding": [
     0,
     8,
     19,
     40,
     58
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet.resnet50(pretrained=True)\n",
    "    def forward(self,input_data):\n",
    "        dense_feat = self.resnet(input_data)\n",
    "        return dense_feat\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        context_feat = self.global_context(input_data)\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([output_dim]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([output_dim])),\n",
    "            batch_shape=torch.Size([output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class GPNode(nn.Module):\n",
    "    def __init__(self,inducing_points,mean_std):\n",
    "        super().__init__()\n",
    "        output_dim = inducing_points.shape[0]\n",
    "        feat_dim = inducing_points.shape[-1]\n",
    "        assert output_dim == args.output_dim\n",
    "        assert feat_dim == args.feat_dim\n",
    "        assert mean_std.shape == (2,3)\n",
    "        \n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=output_dim) \n",
    "        self.mean = torch.tensor(mean_std[0]).cuda()\n",
    "        self.std = torch.tensor(mean_std[1]).cuda()\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        output = self.gp(input_data)\n",
    "        return output\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, gps_mean_std):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gps = nn.ModuleList()\n",
    "        \n",
    "        self.num_gp = args.num_gp\n",
    "        \n",
    "        for i in range(self.num_gp):\n",
    "            inducing_points = torch.zeros(args.output_dim, args.batch_size, args.feat_dim)\n",
    "            gp = GPNode(inducing_points,gps_mean_std[i])\n",
    "            self.gps.append(gp)\n",
    "        \n",
    "    def forward_nn(self, input_data):\n",
    "        dense_feat = self.backbone(input_data)\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention            \n",
    "        return feature_t, rot_pred\n",
    "    \n",
    "    def forward_gp(self,gp,trans_feat):\n",
    "        trans_pred = gp(trans_feat)\n",
    "        return trans_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:41.065951Z",
     "start_time": "2020-07-28T02:52:39.076363Z"
    },
    "code_folding": [
     1,
     20,
     43,
     60,
     65,
     88,
     134,
     138,
     148,
     154
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,gps_mean_std,is_training=True,regressor_context_rate = [0.0,0.0]):\n",
    "        self.model = Model(gps_mean_std).cuda()\n",
    "        self.norm_mean = args.norm_mean.cuda()\n",
    "        self.norm_std = args.norm_std.cuda()\n",
    "        \n",
    "        # disable learning backbone\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if is_training:\n",
    "            # training tool\n",
    "            self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                             lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "        else:\n",
    "            # disable all learning\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def load_model(self, file_name = 'pretrained.pth'):\n",
    "        # load file info\n",
    "        state_dict = torch.load(os.path.join(args.model_dir, file_name))\n",
    "        if 'net.resnet.conv1.weight' in state_dict:\n",
    "            print('Transform from old model.')\n",
    "            # Part 1: backbone\n",
    "            backbone_state_dict = self._from_old_model(state_dict,'backbone')\n",
    "            print('Backbone parameters layer:',len(backbone_state_dict.keys()))\n",
    "            self.model.backbone.load_state_dict(backbone_state_dict,strict = True)\n",
    "            # Part 2: nn\n",
    "            nn_state_dict = self._from_old_model(torch.load(os.path.join(args.model_dir, file_name)),'nn')\n",
    "            print('NN parameters layer:',len(nn_state_dict.keys()))\n",
    "            self.model.nn.load_state_dict(nn_state_dict,strict = True)\n",
    "        else:\n",
    "            print('Parameters layer:',len(state_dict.keys()))\n",
    "            # load file to model\n",
    "            self.model.load_state_dict(state_dict,strict = True)\n",
    "        print('Model Structure:')\n",
    "        # Display model structure\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Parameters layer:',len(self.model.state_dict().keys()))\n",
    "    \n",
    "    def _from_old_model(self, state_dict, select = 'backbone'):\n",
    "        if select == 'backbone':\n",
    "            for key in list(state_dict):\n",
    "                if 'net.resnet.' in key:\n",
    "                    state_dict[key.replace('net.resnet.','resnet.')] = state_dict.pop(key)\n",
    "                else:\n",
    "                    state_dict.pop(key)\n",
    "        elif select == 'nn':\n",
    "            for key in list(state_dict):\n",
    "                if 'net.global_regressor.' in key:\n",
    "                    state_dict[key.replace('net.global_regressor.','global_regressor.')] = state_dict.pop(key)\n",
    "                elif 'net.global_context.' in key:\n",
    "                    state_dict[key.replace('net.global_context.','global_context.')] = state_dict.pop(key)\n",
    "                else:\n",
    "                    state_dict.pop(key)\n",
    "        return state_dict\n",
    "    \n",
    "    def save_model(self, file_name = 'model-{}-{}.pth'):\n",
    "        checkpoint_path = os.path.join(args.model_dir, file_name)\n",
    "        torch.save(self.model.state_dict(),checkpoint_path)\n",
    "        print('Saving model to ' +  file_name)\n",
    "        \n",
    "    def _optimize(self,regressor_context_rate = [0.0,0.0]):\n",
    "        optimizer = [\n",
    "                {'params': self.model.gps.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay}]\n",
    "            \n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            for param in self.model.nn.global_regressor.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "        else:\n",
    "            for param in self.model.nn.global_context.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        return optimizer\n",
    "            \n",
    "    def train(self,x,y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        #labels = torch.from_numpy(pairwise_distances_argmin(y[:,:2].numpy(), mbk_means_cluster_centers)).cuda()\n",
    "        labels = pairwise_distances_argmin(y[:,:2].numpy(), mbk_means_cluster_centers)\n",
    "        x,y_raw = x.cuda(),y.cuda()\n",
    "        \n",
    "        if args.is_normalization:\n",
    "            y = normalize(y_raw,self.norm_mean, self.norm_std) # input should be 7 dim       \n",
    "            \n",
    "        # Step 2: training\n",
    "        assert self.model.training == True\n",
    "        \n",
    "        trans_loss = torch.tensor(0.).cuda()\n",
    "        \n",
    "        _, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        rot_loss = self._nn_loss(rot_pred,rot_target)\n",
    "        \n",
    "        gps_loss = np.zeros(args.num_gp)\n",
    "        for i,gp in enumerate(self.model.gps):\n",
    "            num_data = dis[i]\n",
    "            label_mask = labels == i\n",
    "            sub_x = trans_feat[label_mask]\n",
    "            sub_y = y_raw[label_mask] # unnormalized data \n",
    "            if sub_y.shape[0]>0:\n",
    "                if args.is_normalization:\n",
    "                    sub_y = normalize(sub_y,gp.mean, gp.std) # normalize 7 dim with gp-i mean and std\n",
    "                sub_y,_ = torch.split(sub_y, [3, 4], dim=1) # filter out position\n",
    "                gp_loss = self._gp_loss(gp,num_data,sub_x,sub_y)\n",
    "                gps_loss[i] = float(gp_loss)\n",
    "                trans_loss += gp_loss\n",
    "        trans_loss = trans_loss/self.model.num_gp\n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss\n",
    "        \n",
    "        #Step 3: update\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - start\n",
    "        return float(total_loss), batch_time, gps_loss\n",
    "    \n",
    "    def _nn_loss(self,rot_pred,rot_target):\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        return rot_loss\n",
    "        \n",
    "    def _gp_loss(self,gp,num_data,trans_feat,trans_target):\n",
    "        # predict\n",
    "        trans_pred = self.model.forward_gp(gp,trans_feat)\n",
    "        mll = gpytorch.mlls.PredictiveLogLikelihood(gp.likelihood, gp.gp, num_data = num_data)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*mll(trans_pred, trans_target)\n",
    "        \n",
    "        return trans_loss\n",
    "    \n",
    "    def _eval_gp(self, gp, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = gp.likelihood(trans_pred).mean, gp.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "\n",
    "    def eval_forward(self,x,y,num_sample = 100,output_denormalize = True,label_from_gp = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "        if not label_from_gp:\n",
    "            labels = pairwise_distances_argmin(y[:,:2].cpu().numpy(), mbk_means_cluster_centers)\n",
    "        else:\n",
    "            global_trans_pred,_ = global_trainer.model(x)\n",
    "            global_trans_pred, _, _ = global_trainer._eval_gp(global_trans_pred)\n",
    "            global_trans_pred = denormalize_navie(global_trans_pred, global_trainer.norm_mean, global_trainer.norm_std)\n",
    "            labels = pairwise_distances_argmin(global_trans_pred[:,:2].cpu().numpy(), mbk_means_cluster_centers)\n",
    "        \n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert self.model.training == False\n",
    "        trans_feat, rot_pred = self.model.forward_nn(x)\n",
    "        \n",
    "        trans_pred = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "        trans_mean = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "        trans_var = torch.zeros(y.shape[0],args.output_dim).cuda()\n",
    "\n",
    "        for i,gp in enumerate(trainer.model.gps):\n",
    "            label_mask = labels == i\n",
    "            sub_x = trans_feat[label_mask]\n",
    "            if sub_x.shape[0]>0:\n",
    "                sub_trans_pred = self.model.forward_gp(gp,sub_x)\n",
    "                sub_trans_pred, sub_trans_mean, sub_trans_var = self._eval_gp(gp, sub_trans_pred)\n",
    "                if args.is_normalization and output_denormalize:\n",
    "                    trans_pred[label_mask] = denormalize_navie(sub_trans_pred,gp.mean,gp.std)\n",
    "                    trans_mean[label_mask] = denormalize_navie(sub_trans_mean,gp.mean,gp.std)\n",
    "                    trans_var[label_mask] = sub_trans_var.mul(gp.std)\n",
    "                else:\n",
    "                    trans_pred[label_mask] = sub_trans_pred\n",
    "                    trans_mean[label_mask] = sub_trans_mean\n",
    "                    trans_var[label_mask] = sub_trans_var\n",
    "        \n",
    "        if args.is_normalization and output_denormalize:\n",
    "            y = denormalize(y, self.norm_mean, self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_mean, trans_var, num_sample)\n",
    "        \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_pred, rot_pred, trans_target, rot_target, samples\n",
    "\n",
    "trainer = Trainer(gps_mean_std,is_training=False)\n",
    "#trainer = Trainer(regressor_context_rate = [0.1,0.01])\n",
    "\n",
    "trainer.load_model('model-2-1000.pth')\n",
    "#trainer.load_model('pretrained_gp30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:52:41.074056Z",
     "start_time": "2020-07-28T02:52:41.068645Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "for param in trainer.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T04:02:00.950134Z",
     "start_time": "2020-07-28T04:02:00.841489Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "trans_errors = []\n",
    "rot_errors = []\n",
    "uncertainties = []\n",
    "pose_map = []\n",
    "\n",
    "total_trans_error = 0.\n",
    "total_rot_error = 0.\n",
    "\n",
    "count = 0.\n",
    "\n",
    "is_save_map = False\n",
    "is_read_map = False\n",
    "\n",
    "trans_preds = []\n",
    "trans_gts = []\n",
    "\n",
    "rot_preds = []\n",
    "rot_gts = []\n",
    "\n",
    "pred_uncertainties = []\n",
    "\n",
    "pred_time = []\n",
    "\n",
    "br = tf_ros.TransformBroadcaster()\n",
    "\n",
    "GT_POSE_TOPIC = '/gt_pose'\n",
    "BIRDVIEW_TOPIC_PUB = '/bird_view'\n",
    "MAP_TOPIC_PUB = '/pose_map'\n",
    "PARTICLES_PUB = '/particles'\n",
    "NN_LOCALIZASION_PUB = '/nn_pose'\n",
    "gt_pose_pub = rospy.Publisher(GT_POSE_TOPIC, Odometry, queue_size=1)\n",
    "bird_view_pub = rospy.Publisher(BIRDVIEW_TOPIC_PUB, Image, queue_size=1)\n",
    "map_pub = rospy.Publisher(MAP_TOPIC_PUB, Path, queue_size=1)\n",
    "particles_pub = rospy.Publisher(PARTICLES_PUB, PoseArray, queue_size=1)\n",
    "nn_pose_pub = rospy.Publisher(NN_LOCALIZASION_PUB, Odometry, queue_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T04:09:50.110750Z",
     "start_time": "2020-07-28T04:02:02.376926Z"
    },
    "code_folding": [
     15
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data.values()\n",
    "    trans_pred, rot_pred, trans_gt, rot_gt, samples = trainer.eval_forward(x,y)\n",
    "    \n",
    "    # transform data\n",
    "    trans_pred = trans_pred.cpu().numpy()\n",
    "    rot_pred = rot_pred.cpu().numpy()\n",
    "    trans_gt = trans_gt.cpu().numpy()\n",
    "    rot_gt = rot_gt.cpu().numpy()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    if args.enable_ros:\n",
    "        particles = PoseArray()\n",
    "        particles.header.stamp = rospy.Time.now()\n",
    "        particles.header.frame_id = 'world'\n",
    "        for s in samples:\n",
    "            pose = Pose()\n",
    "            [pose.position.x, pose.position.y, pose.position.z] = s\n",
    "            [pose.orientation.x, pose.orientation.y, pose.orientation.z, pose.orientation.w] = rot_pred[0]\n",
    "            particles.poses.append(pose)\n",
    "        particles_pub.publish(particles)\n",
    "\n",
    "        [px_pred, py_pred, pz_pred] = trans_pred[0]\n",
    "        [qx_pred, qy_pred, qz_pred, qw_pred] = rot_pred[0]\n",
    "\n",
    "        br.sendTransform((px_pred, py_pred, pz_pred),\n",
    "                         (qx_pred, qy_pred, qz_pred, qw_pred), rospy.Time.now(),\n",
    "                         \"estimation\", \"world\")\n",
    "\n",
    "        [px_gt, py_gt, pz_gt] = trans_gt[0]\n",
    "        [qx_gt, qy_gt, qz_gt, qw_gt] = rot_gt[0]\n",
    "\n",
    "        br.sendTransform((px_gt, py_gt, pz_gt),\n",
    "                         (qx_gt, qy_gt, qz_gt, qw_gt),\n",
    "                         rospy.Time.now(), \"gt\", \"world\")\n",
    "\n",
    "        timestamp = rospy.Time.now()\n",
    "\n",
    "        nn_pose_msg = Odometry()\n",
    "        nn_pose_msg.header.frame_id = 'world'\n",
    "        nn_pose_msg.header.stamp = timestamp\n",
    "        nn_pose_msg.child_frame_id = 'base_link'\n",
    "        nn_pose_msg.pose.pose.position.x = px_pred\n",
    "        nn_pose_msg.pose.pose.position.y = py_pred\n",
    "        nn_pose_msg.pose.pose.position.z = pz_pred\n",
    "        [nn_pose_msg.pose.pose.orientation.x, nn_pose_msg.pose.pose.orientation.y, nn_pose_msg.pose.pose.orientation.z, nn_pose_msg.pose.pose.orientation.w] = [qx_pred, qy_pred, qz_pred, qw_pred]\n",
    "\n",
    "        conv = np.zeros((6,6), dtype=np.float32)\n",
    "        [conv[0][0], conv[1][1], conv[2][2]] = trans_cov[0]\n",
    "        nn_pose_msg.pose.covariance = conv.flatten().tolist()\n",
    "        nn_pose_pub.publish(nn_pose_msg)\n",
    "\n",
    "        bridge = CvBridge()\n",
    "\n",
    "        bird_view_img_msg = bridge.cv2_to_imgmsg(np.asarray(x[0].cpu(), dtype=np.float32), encoding=\"passthrough\")\n",
    "        stamp_now = rospy.Time.now()\n",
    "        bird_view_img_msg.header.stamp = stamp_now\n",
    "\n",
    "        bird_view_pub.publish(bird_view_img_msg)\n",
    "\n",
    "        rospy.sleep(.0)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        count += 1\n",
    "    else:\n",
    "        count += y.shape[0]\n",
    "    \n",
    "    trans_preds += [x for x in trans_pred]\n",
    "    rot_preds += [x for x in rot_pred]\n",
    "    trans_gts += [x for x in trans_gt]\n",
    "    rot_gts += [x for x in rot_gt]\n",
    "\n",
    "    trans_error = np.sqrt(np.sum((trans_pred - trans_gt)**2,axis=1))\n",
    "    rot_error_1 = np.arccos(np.sum(np.multiply(rot_pred,rot_gt),axis=1))/math.pi*180\n",
    "    rot_error_2 = np.arccos(np.sum(np.multiply(rot_pred,-rot_gt),axis=1))/math.pi*180\n",
    "    rot_error = np.minimum(rot_error_1,rot_error_2)\n",
    "\n",
    "    trans_errors += [x for x in trans_error]\n",
    "    rot_errors += [x for x in rot_error]\n",
    "\n",
    "    total_trans_error += np.sum(trans_error)\n",
    "    total_rot_error += np.sum(rot_error)\n",
    "    \n",
    "    display = 1\n",
    "\n",
    "    if b % display == 0:\n",
    "        print(\n",
    "            \"{}/{}, translation error = {:.3f}, rotation error = {:.3f}, time/batch = {:.3f}\"\n",
    "            .format(\n",
    "             (b+1)*args.batch_size,\n",
    "            len(dataloader)*args.batch_size,\n",
    "            total_trans_error / count,\n",
    "            total_rot_error / count,\n",
    "            end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T04:10:15.537207Z",
     "start_time": "2020-07-28T04:09:50.112365Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sio.savemat('results.mat', {'trans_pred': np.array(trans_preds), 'trans_gt': np.array(trans_gts), 'uncertainty': np.array(pred_uncertainties)})\n",
    "\n",
    "if len(pose_map):\n",
    "    np.savetxt(os.path.join(args.map_dataset, 'map.txt'), np.asarray(pose_map, dtype=np.float32))\n",
    "    print(\"map is saved!\")\n",
    "\n",
    "plt.hist(trans_errors, bins='auto')\n",
    "plt.title(\"Translation errors\")\n",
    "plt.xlabel(\"translational error in meters\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('terror.png', bbox_inches='tight')\n",
    "\n",
    "plt.hist(rot_errors, bins='auto')\n",
    "plt.title(\"Rotation errors\")\n",
    "plt.xlabel(\"rotational error in degree\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('rerror.png', bbox_inches='tight')\n",
    "\n",
    "median_trans_errors = np.median(trans_errors)\n",
    "median_rot_errors = np.median(rot_errors)\n",
    "mean_trans_errors = np.mean(trans_errors)\n",
    "mean_rot_errors = np.mean(rot_errors)\n",
    "\n",
    "print(\"median translation error = {:.3f}\".format(median_trans_errors))\n",
    "print(\"median rotation error = {:.3f}\".format(median_rot_errors))\n",
    "print(\"mean translation error = {:.3f}\".format(mean_trans_errors))\n",
    "print(\"mean rotation error = {:.3f}\".format(mean_rot_errors))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T04:10:15.611591Z",
     "start_time": "2020-07-28T04:10:15.538943Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(trans_errors,rot_errors):\n",
    "    t = [14301,7008,12852,9567,13580,14835,7114,12683]\n",
    "    for i in range(len(t)):\n",
    "        if i >0:\n",
    "            t[i] += t[i-1]\n",
    "    trans_errors_month = list()\n",
    "    trans_errors_month.append(trans_errors[:t[0]])\n",
    "    trans_errors_month.append(trans_errors[t[0]:t[1]])\n",
    "    trans_errors_month.append(trans_errors[t[1]:t[2]])\n",
    "    trans_errors_month.append(trans_errors[t[2]:t[3]])\n",
    "    trans_errors_month.append(trans_errors[t[3]:t[4]])\n",
    "    trans_errors_month.append(trans_errors[t[4]:t[5]])\n",
    "    trans_errors_month.append(trans_errors[t[5]:t[6]])\n",
    "    trans_errors_month.append(trans_errors[t[6]:])\n",
    "\n",
    "    rot_errors_month = list()\n",
    "    rot_errors_month.append(rot_errors[:t[0]])\n",
    "    rot_errors_month.append(rot_errors[t[0]:t[1]])\n",
    "    rot_errors_month.append(rot_errors[t[1]:t[2]])\n",
    "    rot_errors_month.append(rot_errors[t[2]:t[3]])\n",
    "    rot_errors_month.append(rot_errors[t[3]:t[4]])\n",
    "    rot_errors_month.append(rot_errors[t[4]:t[5]])\n",
    "    rot_errors_month.append(rot_errors[t[5]:t[6]])\n",
    "    rot_errors_month.append(rot_errors[t[6]:])\n",
    "    \n",
    "    print('================== var translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"var translation error = {:.3f}\".format(np.var(trans_errors_i)))\n",
    "    \n",
    "    print('================== median translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"median translation error = {:.3f}\".format(np.median(trans_errors_i)))\n",
    "        \n",
    "    print('================== median rotation error ==================')\n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"median rotation error = {:.3f}\".format(np.median(rot_errors_i)))\n",
    "    \n",
    "    print('================== mean translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"mean translation error = {:.3f}\".format(np.mean(trans_errors_i)))\n",
    "        \n",
    "    print('================== mean rotation error ==================')  \n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"mean rotation error = {:.3f}\".format(np.mean(rot_errors_i)))\n",
    "        \n",
    "evaluate(trans_errors,rot_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
