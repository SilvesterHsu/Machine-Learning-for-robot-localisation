{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:09.838331Z",
     "start_time": "2020-08-21T15:40:08.588320Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import roslib\n",
    "import rospy\n",
    "import tf as tf_ros\n",
    "from nav_msgs.msg import Odometry, Path\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "from geometry_msgs.msg import PoseStamped, PoseArray, Pose\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:10.225765Z",
     "start_time": "2020-08-21T15:40:09.839827Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device 1 : TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from torchlib.utils import list_device,set_device\n",
    "\n",
    "# S1: check GPU\n",
    "#list_device()\n",
    "\n",
    "# S2: default parameters\n",
    "set_device(1)\n",
    "np.set_printoptions(precision = 2)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:10.235217Z",
     "start_time": "2020-08-21T15:40:10.227402Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=420, help='size of mini batch')\n",
    "parser.add_argument('--is_normalization', type=bool, default=True, help='whether do data normalization')\n",
    "parser.add_argument('--target_image_size', default=[300, 300], nargs=2, type=int, help='Input images will be resized to this for data argumentation.')\n",
    "\n",
    "parser.add_argument('--model_dir', type=str, default='/notebooks/global_localization/gp_net_torch', help='rnn, gru, or lstm')\n",
    "\n",
    "parser.add_argument('--test_dataset', type=str, default = ['/notebooks/michigan_nn_data/test_dense_old'])\n",
    "\n",
    "parser.add_argument('--train_dataset', type=str, default = ['/notebooks/michigan_nn_data/test'])\n",
    "parser.add_argument('--norm_tensor', type=str, default = ['/notebooks/global_localization/norm_mean_std.pt'])\n",
    "\n",
    "#parser.add_argument('--map_dataset', type=str, default='/home/kevin/data/michigan_gt/training')\n",
    "parser.add_argument('--enable_ros', type=bool, default=False, help='put data into ros')\n",
    "parser.add_argument('--cuda_device', type=int, default=1, help='cuda device')\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.enable_ros:\n",
    "    rospy.init_node('global_localization_tf_broadcaster_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:12.245190Z",
     "start_time": "2020-08-21T15:40:10.236535Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load norm and std: /notebooks/global_localization/norm_mean_std.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import tf.transformations as tf_tran\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchlib import resnet, vggnet, cnn_auxiliary\n",
    "from torchlib.cnn_auxiliary import normalize, denormalize, denormalize_navie, get_relative_pose, translational_rotational_loss\n",
    "from torchlib.utils import LocalizationDataset, display_loss, data2tensorboard\n",
    "import time\n",
    "\n",
    "path = args.test_dataset[0]\n",
    "x_dense = torch.load(os.path.join(path,'x.pt'))\n",
    "y_dense = torch.load(os.path.join(path,'y.pt'))\n",
    "dataset = TensorDataset(x_dense, y_dense)\n",
    "\n",
    "[args.norm_mean, args.norm_std] = torch.load(*args.norm_tensor)\n",
    "print('Load norm and std:',*args.norm_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, \\\n",
    "                        drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:12.283025Z",
     "start_time": "2020-08-21T15:40:12.246577Z"
    },
    "code_folding": [
     3,
     15,
     36,
     53
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchlib.GPs import Backbone, NN, BaseModule\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_context = vggnet.vggnet(input_channel=2048,opt=\"context\")\n",
    "        self.global_regressor = vggnet.vggnet(opt=\"regressor\")\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        #context_feat = self.global_context(input_data)\n",
    "        context_feat = input_data\n",
    "        output,feature_t, feature_r = self.global_regressor(context_feat)\n",
    "        return output, feature_t, feature_r\n",
    "\n",
    "class GP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, output_dim=3):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([output_dim])\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=output_dim\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([3])),\n",
    "            batch_shape=torch.Size([3]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        inducing_points = torch.zeros(3, 1000, 128)\n",
    "        self.backbone = Backbone()\n",
    "        self.nn = NN()\n",
    "        self.gp = GP(inducing_points)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        #dense_feat = self.backbone(input_data)\n",
    "        dense_feat = input_data\n",
    "        output, feature_t, feature_r = self.nn(dense_feat)\n",
    "        rot_pred = torch.split(output, [3, 4], dim=1)[1] # 4-dimention \n",
    "        trans_pred = self.gp(feature_t)\n",
    "        return trans_pred, rot_pred\n",
    "\n",
    "class PosePredictor(BaseModule):\n",
    "    def __init__(self, norm_mean, norm_std, args,\n",
    "                 is_training=True, mixed_precision=True,\n",
    "                 regressor_context_rate = [0.0,0.0], train_rot = False):\n",
    "        super().__init__(norm_mean, norm_std, args)\n",
    "        self.model = Model().to(self.device)\n",
    "        self.train_rot = train_rot\n",
    "        self.mixed_precision = mixed_precision\n",
    "        if self.mixed_precision:\n",
    "            self.scaler = GradScaler()\n",
    "\n",
    "        self.disable_requires_grad(self.model.backbone)\n",
    "        \n",
    "        if is_training:\n",
    "            self.optimizer = optim.Adam(self._optimize(regressor_context_rate))\n",
    "            self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,\n",
    "                                                             lr_lambda=lambda epoch: args.decay_rate**epoch)\n",
    "            num_data = min(len(dataloader)*args.batch_size,len(dataset))\n",
    "            self.mll = gpytorch.mlls.PredictiveLogLikelihood(self.model.likelihood, \\\n",
    "                                                         self.model.gp, num_data = num_data)\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model)\n",
    "            \n",
    "    def _optimize(self,regressor_context_rate):\n",
    "        # GP\n",
    "        optimizer = [\n",
    "                {'params': self.model.gp.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay},\n",
    "                {'params': self.model.likelihood.parameters(), \\\n",
    "                 'lr': args.learning_rate,'weight_decay':args.weight_decay}]\n",
    "            \n",
    "        # NN\n",
    "        if regressor_context_rate[0]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_regressor.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[0],'weight_decay':args.weight_decay}]\n",
    "            print('Regressor learn rate:',regressor_context_rate[0])\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor)\n",
    "                \n",
    "        if regressor_context_rate[1]!=0:\n",
    "            optimizer += [{'params': self.model.nn.global_context.parameters(), \\\n",
    "                 'lr': args.learning_rate * regressor_context_rate[1],'weight_decay':args.weight_decay}]\n",
    "            print('Context learn rate:',regressor_context_rate[1])\n",
    "            self.train_rot = True\n",
    "        else:\n",
    "            self.disable_requires_grad(self.model.nn.global_context)\n",
    "            \n",
    "        \n",
    "        if not self.train_rot and regressor_context_rate[1]==0.0:\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc1_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc2_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.fc3_rot)\n",
    "            self.disable_requires_grad(self.model.nn.global_regressor.regressor.logits_r)\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    def train(self,x, y):\n",
    "        # Step 0: zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        # Step 1: get data\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        if args.is_normalization:\n",
    "            y = normalize(y,self.norm_mean, self.norm_std)\n",
    "            \n",
    "        # Step 2: training\n",
    "        assert trainer.model.training == True\n",
    "        if self.mixed_precision:\n",
    "            with autocast():\n",
    "                single_loss = self._loss(x, y)\n",
    "            self.scaler.scale(single_loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            single_loss = self._loss(x, y)\n",
    "            single_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        return float(single_loss), batch_time\n",
    "    \n",
    "    def _loss(self,x, y):\n",
    "        # target\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        # predict\n",
    "        trans_pred, rot_pred = self.model(x)\n",
    "        \n",
    "        # trans loss\n",
    "        trans_loss = -1.*self.mll(trans_pred, trans_target)\n",
    "        # rot loss\n",
    "        rot_loss = 1. - torch.mean(torch.square(torch.sum(torch.mul(rot_pred,rot_target),dim=1)))\n",
    "        \n",
    "        total_loss = trans_loss + args.lamda_weights * rot_loss      \n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def eval_forward(self,x,y,num_sample = 100,output_denormalize = True):\n",
    "        # Step 1: get data\n",
    "        x,y = x.to(self.device),y.to(self.device)\n",
    "        \n",
    "        # Step 2: forward\n",
    "        assert trainer.model.training == False\n",
    "        if self.mixed_precision:\n",
    "            with autocast():\n",
    "                trans_prediction, rot_prediction = self.model(x)\n",
    "            self.scaler.scale(trans_prediction)\n",
    "            self.scaler.scale(rot_prediction)\n",
    "        else:\n",
    "            trans_prediction, rot_prediction = self.model(x)\n",
    "            \n",
    "        trans_prediction, trans_mean, trans_var = self._eval_gp(trans_prediction)\n",
    "        \n",
    "        if args.is_normalization and output_denormalize:\n",
    "            trans_prediction = denormalize_navie(trans_prediction, self.norm_mean, self.norm_std)\n",
    "            trans_mean = denormalize_navie(trans_mean, self.norm_mean, self.norm_std)\n",
    "            trans_var = trans_var.mul(self.norm_std)\n",
    "        \n",
    "        samples = self._sample(trans_mean, trans_var, num_sample)\n",
    "            \n",
    "        # Step 3: split output\n",
    "        trans_target, rot_target = torch.split(y, [3, 4], dim=1)\n",
    "        \n",
    "        return trans_prediction, rot_prediction, trans_target, rot_target, samples\n",
    "    \n",
    "    def _sample(self, mean, var, num_sample = 100):\n",
    "        dist = torch.distributions.Normal(mean, var)\n",
    "        samples = dist.sample([num_sample])\n",
    "        return samples\n",
    "    \n",
    "    def _eval_gp(self, trans_pred):\n",
    "        c_mean, c_var = trans_pred.mean, trans_pred.variance\n",
    "        y_mean, y_var = self.model.likelihood(trans_pred).mean, self.model.likelihood(trans_pred).variance\n",
    "        \n",
    "        return y_mean, c_mean, c_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:15.742101Z",
     "start_time": "2020-08-21T15:40:12.284335Z"
    },
    "code_folding": [
     0,
     1
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model to TITAN Xp.\n"
     ]
    }
   ],
   "source": [
    "trainer = PosePredictor(args.norm_mean,args.norm_std,args,mixed_precision=False,is_training = False)\n",
    "trainer.load_model('model-fast-zero-mean.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:40:15.749629Z",
     "start_time": "2020-08-21T15:40:15.743482Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "trans_errors = []\n",
    "rot_errors = []\n",
    "uncertainties = []\n",
    "pose_map = []\n",
    "\n",
    "total_trans_error = 0.\n",
    "total_rot_error = 0.\n",
    "\n",
    "count = 0.\n",
    "\n",
    "is_save_map = False\n",
    "is_read_map = False\n",
    "\n",
    "trans_preds = []\n",
    "trans_gts = []\n",
    "\n",
    "rot_preds = []\n",
    "rot_gts = []\n",
    "\n",
    "pred_uncertainties = []\n",
    "\n",
    "pred_time = []\n",
    "\n",
    "br = tf_ros.TransformBroadcaster()\n",
    "\n",
    "GT_POSE_TOPIC = '/gt_pose'\n",
    "BIRDVIEW_TOPIC_PUB = '/bird_view'\n",
    "MAP_TOPIC_PUB = '/pose_map'\n",
    "PARTICLES_PUB = '/particles'\n",
    "NN_LOCALIZASION_PUB = '/nn_pose'\n",
    "gt_pose_pub = rospy.Publisher(GT_POSE_TOPIC, Odometry, queue_size=1)\n",
    "bird_view_pub = rospy.Publisher(BIRDVIEW_TOPIC_PUB, Image, queue_size=1)\n",
    "map_pub = rospy.Publisher(MAP_TOPIC_PUB, Path, queue_size=1)\n",
    "particles_pub = rospy.Publisher(PARTICLES_PUB, PoseArray, queue_size=1)\n",
    "nn_pose_pub = rospy.Publisher(NN_LOCALIZASION_PUB, Odometry, queue_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:41:09.717194Z",
     "start_time": "2020-08-21T15:41:09.516481Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/183960, translation error = 3.152, rotation error = 2.385, time/batch = 0.097\n",
      "840/183960, translation error = 8.310, rotation error = 3.766, time/batch = 0.046\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "\n",
    "for b, data in enumerate(dataloader, 0):\n",
    "    start = time.time()\n",
    "    x,y = data#.values()\n",
    "    trans_pred, rot_pred, trans_gt, rot_gt, samples = trainer.eval_forward(x,y)\n",
    "    \n",
    "    # transform data\n",
    "    trans_pred = trans_pred.cpu().numpy()\n",
    "    rot_pred = rot_pred.cpu().numpy()\n",
    "    trans_gt = trans_gt.cpu().numpy()\n",
    "    rot_gt = rot_gt.cpu().numpy()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    if args.enable_ros:\n",
    "        particles = PoseArray()\n",
    "        particles.header.stamp = rospy.Time.now()\n",
    "        particles.header.frame_id = 'world'\n",
    "        for s in samples:\n",
    "            pose = Pose()\n",
    "            [pose.position.x, pose.position.y, pose.position.z] = s\n",
    "            [pose.orientation.x, pose.orientation.y, pose.orientation.z, pose.orientation.w] = rot_pred[0]\n",
    "            particles.poses.append(pose)\n",
    "        particles_pub.publish(particles)\n",
    "\n",
    "        [px_pred, py_pred, pz_pred] = trans_pred[0]\n",
    "        [qx_pred, qy_pred, qz_pred, qw_pred] = rot_pred[0]\n",
    "\n",
    "        br.sendTransform((px_pred, py_pred, pz_pred),\n",
    "                         (qx_pred, qy_pred, qz_pred, qw_pred), rospy.Time.now(),\n",
    "                         \"estimation\", \"world\")\n",
    "\n",
    "        [px_gt, py_gt, pz_gt] = trans_gt[0]\n",
    "        [qx_gt, qy_gt, qz_gt, qw_gt] = rot_gt[0]\n",
    "\n",
    "        br.sendTransform((px_gt, py_gt, pz_gt),\n",
    "                         (qx_gt, qy_gt, qz_gt, qw_gt),\n",
    "                         rospy.Time.now(), \"gt\", \"world\")\n",
    "\n",
    "        timestamp = rospy.Time.now()\n",
    "\n",
    "        nn_pose_msg = Odometry()\n",
    "        nn_pose_msg.header.frame_id = 'world'\n",
    "        nn_pose_msg.header.stamp = timestamp\n",
    "        nn_pose_msg.child_frame_id = 'base_link'\n",
    "        nn_pose_msg.pose.pose.position.x = px_pred\n",
    "        nn_pose_msg.pose.pose.position.y = py_pred\n",
    "        nn_pose_msg.pose.pose.position.z = pz_pred\n",
    "        [nn_pose_msg.pose.pose.orientation.x, nn_pose_msg.pose.pose.orientation.y, nn_pose_msg.pose.pose.orientation.z, nn_pose_msg.pose.pose.orientation.w] = [qx_pred, qy_pred, qz_pred, qw_pred]\n",
    "\n",
    "        conv = np.zeros((6,6), dtype=np.float32)\n",
    "        [conv[0][0], conv[1][1], conv[2][2]] = trans_cov[0]\n",
    "        nn_pose_msg.pose.covariance = conv.flatten().tolist()\n",
    "        nn_pose_pub.publish(nn_pose_msg)\n",
    "\n",
    "        bridge = CvBridge()\n",
    "\n",
    "        bird_view_img_msg = bridge.cv2_to_imgmsg(np.asarray(x[0].cpu(), dtype=np.float32), encoding=\"passthrough\")\n",
    "        stamp_now = rospy.Time.now()\n",
    "        bird_view_img_msg.header.stamp = stamp_now\n",
    "\n",
    "        bird_view_pub.publish(bird_view_img_msg)\n",
    "\n",
    "        rospy.sleep(.0)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        count += 1\n",
    "    else:\n",
    "        count += y.shape[0]\n",
    "    \n",
    "    trans_preds += [x for x in trans_pred]\n",
    "    rot_preds += [x for x in rot_pred]\n",
    "    trans_gts += [x for x in trans_gt]\n",
    "    rot_gts += [x for x in rot_gt]\n",
    "\n",
    "    trans_error = np.sqrt(np.sum((trans_pred - trans_gt)**2,axis=1))\n",
    "    rot_error_1 = np.arccos(np.sum(np.multiply(rot_pred,rot_gt),axis=1))/math.pi*180\n",
    "    rot_error_2 = np.arccos(np.sum(np.multiply(rot_pred,-rot_gt),axis=1))/math.pi*180\n",
    "    rot_error = np.minimum(rot_error_1,rot_error_2)\n",
    "\n",
    "    trans_errors += [x for x in trans_error]\n",
    "    rot_errors += [x for x in rot_error]\n",
    "\n",
    "    total_trans_error += np.sum(trans_error)\n",
    "    total_rot_error += np.sum(rot_error)\n",
    "    \n",
    "    display = 1\n",
    "\n",
    "    if b % display == 0:\n",
    "        print(\n",
    "            \"{}/{}, translation error = {:.3f}, rotation error = {:.3f}, time/batch = {:.3f}\"\n",
    "            .format(\n",
    "             (b+1)*args.batch_size,\n",
    "            len(dataloader)*args.batch_size,\n",
    "            total_trans_error / count,\n",
    "            total_rot_error / count,\n",
    "            end - start))\n",
    "    if b == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:45:30.345617Z",
     "start_time": "2020-08-21T15:45:30.338190Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 39.55, 493.35,  -2.04],\n",
       "       [ 38.96, 497.52,  -2.01],\n",
       "       [ 36.72, 501.17,  -1.85],\n",
       "       [ 36.66, 503.8 ,  -1.74],\n",
       "       [ 36.36, 505.65,  -1.73],\n",
       "       [ 37.34, 505.49,  -1.69],\n",
       "       [ 37.88, 505.93,  -1.7 ],\n",
       "       [ 38.34, 507.3 ,  -1.67],\n",
       "       [ 40.29, 508.19,  -1.66],\n",
       "       [ 36.08, 513.36,  -1.51],\n",
       "       [ 35.48, 515.57,  -1.48],\n",
       "       [ 37.14, 515.13,  -1.46],\n",
       "       [ 36.12, 516.33,  -1.45],\n",
       "       [ 35.55, 517.11,  -1.5 ],\n",
       "       [ 35.72, 521.71,  -1.41],\n",
       "       [ 37.85, 518.47,  -1.51],\n",
       "       [ 36.25, 524.23,  -1.5 ],\n",
       "       [ 35.97, 525.13,  -1.47],\n",
       "       [ 36.84, 524.55,  -1.47],\n",
       "       [ 32.92, 519.88,  -1.48],\n",
       "       [ 40.68, 515.2 ,  -1.37],\n",
       "       [ 34.83, 510.02,  -1.36],\n",
       "       [ 34.  , 368.1 ,  -0.86],\n",
       "       [ 34.01, 537.8 ,  -1.73],\n",
       "       [ 38.03, 534.91,  -1.81],\n",
       "       [ 31.06, 543.36,  -1.88],\n",
       "       [ 34.25, 531.37,  -1.85],\n",
       "       [ 36.93, 488.16,  -1.87],\n",
       "       [ 34.02, 536.28,  -1.86],\n",
       "       [ 31.91, 540.43,  -1.89],\n",
       "       [ 34.8 , 539.32,  -1.78],\n",
       "       [ 29.1 , 542.35,  -1.76],\n",
       "       [ 24.56, 537.54,  -2.02],\n",
       "       [ 27.55, 530.86,  -1.67],\n",
       "       [ 25.51, 541.59,  -1.69],\n",
       "       [ 24.2 , 541.51,  -1.7 ],\n",
       "       [ 29.28, 542.44,  -1.8 ],\n",
       "       [ 31.1 , 541.5 ,  -1.86],\n",
       "       [ 29.34, 541.32,  -1.83],\n",
       "       [ 28.02, 543.5 ,  -1.8 ],\n",
       "       [ 21.15, 538.9 ,  -1.71],\n",
       "       [ 21.31, 536.91,  -1.76]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_pred[::10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:45:37.000724Z",
     "start_time": "2020-08-21T15:45:36.992870Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38.15, 494.42,  -2.  ],\n",
       "       [ 38.05, 496.4 ,  -1.96],\n",
       "       [ 37.92, 498.38,  -1.89],\n",
       "       [ 37.86, 500.24,  -1.8 ],\n",
       "       [ 37.89, 502.14,  -1.73],\n",
       "       [ 37.79, 504.09,  -1.66],\n",
       "       [ 37.62, 506.04,  -1.62],\n",
       "       [ 37.55, 508.08,  -1.59],\n",
       "       [ 37.41, 510.11,  -1.52],\n",
       "       [ 37.18, 512.09,  -1.46],\n",
       "       [ 36.81, 514.03,  -1.42],\n",
       "       [ 36.46, 515.86,  -1.4 ],\n",
       "       [ 36.24, 517.81,  -1.41],\n",
       "       [ 36.03, 519.78,  -1.4 ],\n",
       "       [ 35.93, 521.63,  -1.49],\n",
       "       [ 35.88, 523.61,  -1.59],\n",
       "       [ 35.96, 525.45,  -1.62],\n",
       "       [ 36.04, 527.36,  -1.65],\n",
       "       [ 36.32, 529.4 ,  -1.7 ],\n",
       "       [ 36.54, 531.3 ,  -1.74],\n",
       "       [ 36.78, 533.24,  -1.75],\n",
       "       [ 37.15, 534.98,  -1.79],\n",
       "       [ 37.66, 536.25,  -1.99],\n",
       "       [ 37.89, 538.1 ,  -2.22],\n",
       "       [ 37.67, 539.27,  -2.18],\n",
       "       [ 37.58, 539.62,  -2.14],\n",
       "       [ 37.57, 539.69,  -2.13],\n",
       "       [ 37.57, 539.78,  -2.12],\n",
       "       [ 37.56, 539.82,  -2.11],\n",
       "       [ 37.56, 539.81,  -2.11],\n",
       "       [ 37.5 , 540.11,  -2.06],\n",
       "       [ 37.35, 540.67,  -1.93],\n",
       "       [ 36.8 , 541.15,  -1.79],\n",
       "       [ 36.34, 541.31,  -1.8 ],\n",
       "       [ 35.45, 541.45,  -1.83],\n",
       "       [ 34.29, 541.62,  -1.89],\n",
       "       [ 33.31, 541.67,  -1.96],\n",
       "       [ 31.53, 541.84,  -1.94],\n",
       "       [ 29.99, 541.79,  -1.93],\n",
       "       [ 27.69, 541.58,  -1.9 ],\n",
       "       [ 26.26, 541.49,  -1.88],\n",
       "       [ 26.16, 541.5 ,  -1.88]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_gt[::10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:25:14.842180Z",
     "start_time": "2020-08-21T13:24:49.840101Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median translation error = 2.084\n",
      "median rotation error = 3.121\n",
      "mean translation error = 10.984\n",
      "mean rotation error = 6.551\n"
     ]
    }
   ],
   "source": [
    "sio.savemat('results.mat', {'trans_pred': np.array(trans_preds), 'trans_gt': np.array(trans_gts), 'uncertainty': np.array(pred_uncertainties)})\n",
    "\n",
    "if len(pose_map):\n",
    "    np.savetxt(os.path.join(args.map_dataset, 'map.txt'), np.asarray(pose_map, dtype=np.float32))\n",
    "    print(\"map is saved!\")\n",
    "\n",
    "plt.hist(trans_errors, bins='auto')\n",
    "plt.title(\"Translation errors\")\n",
    "plt.xlabel(\"translational error in meters\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('terror.png', bbox_inches='tight')\n",
    "\n",
    "plt.hist(rot_errors, bins='auto')\n",
    "plt.title(\"Rotation errors\")\n",
    "plt.xlabel(\"rotational error in degree\")\n",
    "plt.ylabel(\"number of frames\")\n",
    "plt.savefig('rerror.png', bbox_inches='tight')\n",
    "\n",
    "median_trans_errors = np.median(trans_errors)\n",
    "median_rot_errors = np.median(rot_errors)\n",
    "mean_trans_errors = np.mean(trans_errors)\n",
    "mean_rot_errors = np.mean(rot_errors)\n",
    "\n",
    "print(\"median translation error = {:.3f}\".format(median_trans_errors))\n",
    "print(\"median rotation error = {:.3f}\".format(median_rot_errors))\n",
    "print(\"mean translation error = {:.3f}\".format(mean_trans_errors))\n",
    "print(\"mean rotation error = {:.3f}\".format(mean_rot_errors))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T05:41:39.768672Z",
     "start_time": "2020-08-21T05:41:39.602583Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== median translation error ==================\n",
      "median translation error = 1.588\n",
      "median translation error = 1.707\n",
      "median translation error = 1.979\n",
      "median translation error = 2.043\n",
      "median translation error = 2.072\n",
      "median translation error = 2.164\n",
      "median translation error = 3.589\n",
      "median translation error = 2.820\n",
      "================== median rotation error ==================\n",
      "median rotation error = 2.584\n",
      "median rotation error = 2.794\n",
      "median rotation error = 2.950\n",
      "median rotation error = 2.925\n",
      "median rotation error = 3.105\n",
      "median rotation error = 3.262\n",
      "median rotation error = 4.395\n",
      "median rotation error = 4.155\n",
      "================== mean translation error ==================\n",
      "mean translation error = 4.364\n",
      "mean translation error = 4.497\n",
      "mean translation error = 12.948\n",
      "mean translation error = 12.562\n",
      "mean translation error = 9.571\n",
      "mean translation error = 11.935\n",
      "mean translation error = 20.999\n",
      "mean translation error = 13.741\n",
      "================== mean rotation error ==================\n",
      "mean rotation error = 3.956\n",
      "mean rotation error = 4.262\n",
      "mean rotation error = 6.160\n",
      "mean rotation error = 5.901\n",
      "mean rotation error = 5.634\n",
      "mean rotation error = 6.627\n",
      "mean rotation error = 12.331\n",
      "mean rotation error = 9.281\n"
     ]
    }
   ],
   "source": [
    "def evaluate(trans_errors,rot_errors):\n",
    "    t = dataset.last_indexes\n",
    "    trans_errors_month = list()\n",
    "    trans_errors_month.append(trans_errors[:t[0]])\n",
    "    trans_errors_month.append(trans_errors[t[0]:t[1]])\n",
    "    trans_errors_month.append(trans_errors[t[1]:t[2]])\n",
    "    trans_errors_month.append(trans_errors[t[2]:t[3]])\n",
    "    trans_errors_month.append(trans_errors[t[3]:t[4]])\n",
    "    trans_errors_month.append(trans_errors[t[4]:t[5]])\n",
    "    trans_errors_month.append(trans_errors[t[5]:t[6]])\n",
    "    trans_errors_month.append(trans_errors[t[6]:])\n",
    "\n",
    "    rot_errors_month = list()\n",
    "    rot_errors_month.append(rot_errors[:t[0]])\n",
    "    rot_errors_month.append(rot_errors[t[0]:t[1]])\n",
    "    rot_errors_month.append(rot_errors[t[1]:t[2]])\n",
    "    rot_errors_month.append(rot_errors[t[2]:t[3]])\n",
    "    rot_errors_month.append(rot_errors[t[3]:t[4]])\n",
    "    rot_errors_month.append(rot_errors[t[4]:t[5]])\n",
    "    rot_errors_month.append(rot_errors[t[5]:t[6]])\n",
    "    rot_errors_month.append(rot_errors[t[6]:])\n",
    "    \n",
    "    print('================== median translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"median translation error = {:.3f}\".format(np.median(trans_errors_i)))\n",
    "        \n",
    "    print('================== median rotation error ==================')\n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"median rotation error = {:.3f}\".format(np.median(rot_errors_i)))\n",
    "    \n",
    "    print('================== mean translation error ==================')\n",
    "    for trans_errors_i in trans_errors_month:\n",
    "        print(\"mean translation error = {:.3f}\".format(np.mean(trans_errors_i)))\n",
    "        \n",
    "    print('================== mean rotation error ==================')  \n",
    "    for rot_errors_i in rot_errors_month:\n",
    "        print(\"mean rotation error = {:.3f}\".format(np.mean(rot_errors_i)))\n",
    "        \n",
    "evaluate(trans_errors,rot_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
